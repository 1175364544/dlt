{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing requried libraries 导入模型实现所需要的包\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the Anna Karenina novel text file 读取Anna Karenina 小说的文本\n",
    "with open('Anna_Karenina.txt', 'r') as f:\n",
    "    textlines=f.read()\n",
    "#Building the vocan and encoding the characters as integers 将字符转换为整数，存入数据集中\n",
    "language_vocab = set(textlines)     #文本中所有字符的集合\n",
    "vocab_to_integer = {char: j for j, char in enumerate(language_vocab)}   #字符列表进行字典索引\n",
    "integer_to_vocab = dict(enumerate(language_vocab))\n",
    "encoded_vocab = np.array([vocab_to_integer[char] for char in textlines], dtype=np.int32)   #对文本中的所有字符进行数字编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textlines[:200]  #Anna Karenina文本的前200个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 62, 26, 27, 77,  8, 45,  3, 36, 16, 16, 16, 24, 26, 27, 27, 71,\n",
       "        3,  9, 26, 11, 57,  1, 57,  8, 19,  3, 26, 45,  8,  3, 26,  1,  1,\n",
       "        3, 26,  1, 57, 55,  8, 66,  3,  8, 25,  8, 45, 71,  3, 46, 64, 62,\n",
       "       26, 27, 27, 71,  3,  9, 26, 11, 57,  1, 71,  3, 57, 19,  3, 46, 64,\n",
       "       62, 26, 27, 27, 71,  3, 57, 64,  3, 57, 77, 19,  3, 82,  4, 64, 16,\n",
       "        4, 26, 71, 58, 16, 16, 12, 25,  8, 45, 71, 77, 62, 57, 64,  6,  3,\n",
       "        4, 26, 19,  3, 57, 64,  3, 23, 82, 64,  9, 46, 19, 57, 82, 64,  3,\n",
       "       57, 64,  3, 77, 62,  8,  3, 59, 76,  1, 82, 64, 19, 55, 71, 19,  0,\n",
       "        3, 62, 82, 46, 19,  8, 58,  3, 72, 62,  8,  3,  4, 57,  9,  8,  3,\n",
       "       62, 26, 47, 16, 47, 57, 19, 23, 82, 25,  8, 45,  8, 47,  3, 77, 62,\n",
       "       26, 77,  3, 77, 62,  8,  3, 62, 46, 19, 76, 26, 64, 47,  3,  4, 26,\n",
       "       19,  3, 23, 26, 45, 45, 71, 57, 64,  6,  3, 82, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_vocab[:200]   #对前200个字符进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(language_vocab) #Anna Karenina文本中的所有字符类型，包括83种不同的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据生成小批次进行训练\n",
    "def generate_character_batches(data, num_seq, num_steps):\n",
    "    '''Create a function that returns batches of size\n",
    "       num_seq x num_steps from data.\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches  计算batch的大小，batch的数量\n",
    "    num_char_per_batch = num_seq * num_steps\n",
    "    num_batches = len(data) // num_char_per_batch\n",
    "\n",
    "    # Keep only enough characters to make full batches  只保留完成的batch，也就是说对不能整除的部分舍弃\n",
    "    data = data[:num_batches * num_char_per_batch]\n",
    "\n",
    "    # Reshape the array into n_seqs rows  将这个数组进行重塑，行数为the number of sequences，列数自动生成\n",
    "    data = data.reshape((num_seq, -1))\n",
    "\n",
    "    for i in range(0, data.shape[1], num_steps):\n",
    "        # The input variables  输入变量\n",
    "        input_x = data[:, i:i + num_steps]\n",
    "\n",
    "        # The output variables which are shifted by one  输出变量偏移一个位置\n",
    "        output_y = np.zeros_like(input_x)\n",
    "\n",
    "        output_y[:, :-1], output_y[:, -1] = input_x[:, 1:], input_x[:, 0]\n",
    "        yield input_x, output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      " [[38 62 26 27 77  8 45  3 36 16]\n",
      " [ 3  8  1 57  6 57 76  1  8  3]\n",
      " [82  3  1 57  6 62 77 10 62  8]\n",
      " [ 3 62 26 47  3 76  8  8 64  3]\n",
      " [ 3 82 64  8  3 55 64  8  4 16]\n",
      " [71  8 26 45 19  3  1 26 19 77]\n",
      " [ 3 74  8  3  6 82  3 57 64 77]\n",
      " [82  4  3 77 82  3 11 26 55  8]\n",
      " [76  8  9 82 45  8  3  2  8 64]\n",
      " [19 26  6  8  3  1 57 55  8  3]]\n",
      "\n",
      "target\n",
      " [[62 26 27 77  8 45  3 36 16 16]\n",
      " [ 8  1 57  6 57 76  1  8  3  6]\n",
      " [ 3  1 57  6 62 77 10 62  8 26]\n",
      " [62 26 47  3 76  8  8 64  3 76]\n",
      " [82 64  8  3 55 64  8  4 16  4]\n",
      " [ 8 26 45 19  3  1 26 19 77  3]\n",
      " [74  8  3  6 82  3 57 64 77 82]\n",
      " [ 4  3 77 82  3 11 26 55  8 61]\n",
      " [ 8  9 82 45  8  3  2  8 64 77]\n",
      " [26  6  8  3  1 57 55  8  3 77]]\n"
     ]
    }
   ],
   "source": [
    "#生成一个the number of sequences为15，the number of steps为50的批来演示这个函数\n",
    "generated_batches = generate_character_batches(encoded_vocab, 15, 50)\n",
    "input_x, output_y = next(generated_batches)\n",
    "print('input\\n', input_x[:10, :10])\n",
    "print('\\ntarget\\n', output_y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建输入层\n",
    "def build_model_inputs(batch_size, num_steps):\n",
    "    # Declare placeholders for the input and output variables  定义输入变量和输出变量的占位符\n",
    "    inputs_x = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets_y = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "\n",
    "    # define the keep_probability for the dropout layer  为了防止过拟合，在dropout层定义keep probability这个参数，用来控制dropout的保留结点数\n",
    "    keep_probability = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return inputs_x, targets_y, keep_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建LSTM\n",
    "def build_lstm_cell(size, num_layers, batch_size, keep_probability):\n",
    "    ### Building the LSTM Cell using the tensorflow function 运用TensorFlow中的函数来构建基本的LSTM单元\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(size)\n",
    "\n",
    "    # Adding dropout to the layer to prevent overfitting  为了防止过拟合，在层中添加dropout层\n",
    "    drop_layer = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_probability)\n",
    "\n",
    "    # Add muliple cells together and stack them up to oprovide a level of more understanding  通过堆叠多个LSTM单元，构建多层LSTM\n",
    "    stakced_cell = tf.contrib.rnn.MultiRNNCell([drop_layer] * num_layers)\n",
    "    initial_cell_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return lstm_cell, initial_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建输出层\n",
    "def build_model_output(output, input_size, output_size):\n",
    "    # Reshaping output of the model to become a bunch of rows, where each row correspond for each step in the seq  重塑输出的尺寸\n",
    "    sequence_output = tf.concat(output, axis=1)\n",
    "    reshaped_output = tf.reshape(sequence_output, [-1, input_size])\n",
    "\n",
    "    # Connect the RNN outputs to a softmax layer 将输出连接到softmax层\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((input_size, output_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(output_size))\n",
    "\n",
    "    # the output is a set of rows of LSTM cell outputs, so the logits will be a set \n",
    "    # of rows of logit outputs, one for each step and sequence  计算logits\n",
    "    logits = tf.matmul(reshaped_output, softmax_w) + softmax_b\n",
    "\n",
    "    # Use softmax to get the probabilities for predicted characters  用softmax函数得到预测字符的概率\n",
    "    model_out = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "    return model_out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练损失\n",
    "def model_loss(logits, targets, lstm_size, num_classes):\n",
    "    # convert the targets to one-hot encoded and reshape them to match the logits, one row per batch_size per step\n",
    "    #因为softmax层输出的概率分布，所以对目标值进行one-hot编码\n",
    "    output_y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    output_y_reshaped = tf.reshape(output_y_one_hot, logits.get_shape())\n",
    "\n",
    "    # Use the cross entropy loss  用softmax交叉熵来计算损失\n",
    "    model_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=output_y_reshaped)\n",
    "    model_loss = tf.reduce_mean(model_loss)\n",
    "    return model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化\n",
    "def build_model_optimizer(model_loss, learning_rate, grad_clip):\n",
    "    # define optimizer for training, using gradient clipping to avoid the exploding of the gradients  \n",
    "    #定义一个优化器，为了避免梯度爆炸，引入梯度裁剪，设置一个阈值，当梯度值超过这个特定的阈值时，将它重置为阈值大小。\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    gradients, _ = tf.clip_by_global_norm(tf.gradients(model_loss, trainable_variables), grad_clip)\n",
    "\n",
    "    # Use Adam Optimizer  整个学习过程使用Adam/优化器\n",
    "    train_operation = tf.train.AdamOptimizer(learning_rate)\n",
    "    model_optimizer = train_operation.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return model_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建网络，将所有构建的模块组合起来\n",
    "class CharLSTM:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                 lstm_size=128, num_layers=2, learning_rate=0.001,\n",
    "                 grad_clip=5, sampling=False):\n",
    "\n",
    "        # When we're using this network for generating text by sampling, we'll be providing the network with\n",
    "        # one character at a time, so providing an option for it. 我们使用这个网络来生成文本\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Build the model inputs placeholders of the input and target variables  模型的输入和目标变量\n",
    "        self.inputs, self.targets, self.keep_prob = build_model_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Building the LSTM cell  建立LSTM单元\n",
    "        lstm_cell, self.initial_state = build_lstm_cell(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the LSTM layers 通过LSTM层运行这个数据\n",
    "        # one_hot encode the input  将输入变量进行one-hot编码\n",
    "        input_x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "\n",
    "        # Runing each sequence step through the LSTM architecture and finally collectting the outputs  通过LSTM的结构来运行，并得到输出值\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm_cell, input_x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "\n",
    "        # Get softmax predictions and logits  得到softmax的预测和logits\n",
    "        self.prediction, self.logits = build_model_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "        # Loss and optimizer (with gradient clipping) 训练损失和引入梯度裁剪的优化\n",
    "        self.loss = model_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_model_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training process...\n"
     ]
    }
   ],
   "source": [
    "#设置超参数值\n",
    "batch_size = 100        # Sequences per batch  \n",
    "num_steps = 100         # Number of sequence steps per batch  每个batch的序列步长\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs  \n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate  学习率\n",
    "keep_probability = 0.5         # Dropout keep probability dropout层中保留结点的比例\n",
    "print('Starting the training process...')\n",
    "epochs = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint N iterations每N轮进行一次变量保存\n",
    "save_every_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = CharLSTM(len(language_vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                      lstm_size=lstm_size, num_layers=num_layers,\n",
    "                      learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1/5...  Step: 1...  loss: 4.4197...  7.640 sec/batch\n",
      "Epoch number: 1/5...  Step: 2...  loss: 4.3664...  7.148 sec/batch\n",
      "Epoch number: 1/5...  Step: 3...  loss: 4.2514...  6.937 sec/batch\n",
      "Epoch number: 1/5...  Step: 4...  loss: 3.8124...  6.712 sec/batch\n",
      "Epoch number: 1/5...  Step: 5...  loss: 3.4003...  6.733 sec/batch\n",
      "Epoch number: 1/5...  Step: 6...  loss: 3.3277...  6.739 sec/batch\n",
      "Epoch number: 1/5...  Step: 7...  loss: 3.2580...  6.426 sec/batch\n",
      "Epoch number: 1/5...  Step: 8...  loss: 3.1936...  6.739 sec/batch\n",
      "Epoch number: 1/5...  Step: 9...  loss: 3.1433...  6.618 sec/batch\n",
      "Epoch number: 1/5...  Step: 10...  loss: 3.1419...  6.756 sec/batch\n",
      "Epoch number: 1/5...  Step: 11...  loss: 3.1315...  6.734 sec/batch\n",
      "Epoch number: 1/5...  Step: 12...  loss: 3.1202...  6.393 sec/batch\n",
      "Epoch number: 1/5...  Step: 13...  loss: 3.1152...  6.377 sec/batch\n",
      "Epoch number: 1/5...  Step: 14...  loss: 3.1369...  6.394 sec/batch\n",
      "Epoch number: 1/5...  Step: 15...  loss: 3.1236...  6.381 sec/batch\n",
      "Epoch number: 1/5...  Step: 16...  loss: 3.1144...  6.430 sec/batch\n",
      "Epoch number: 1/5...  Step: 17...  loss: 3.0948...  6.394 sec/batch\n",
      "Epoch number: 1/5...  Step: 18...  loss: 3.1233...  6.379 sec/batch\n",
      "Epoch number: 1/5...  Step: 19...  loss: 3.1022...  6.381 sec/batch\n",
      "Epoch number: 1/5...  Step: 20...  loss: 3.0682...  6.375 sec/batch\n",
      "Epoch number: 1/5...  Step: 21...  loss: 3.0951...  6.460 sec/batch\n",
      "Epoch number: 1/5...  Step: 22...  loss: 3.0915...  6.362 sec/batch\n",
      "Epoch number: 1/5...  Step: 23...  loss: 3.0757...  6.398 sec/batch\n",
      "Epoch number: 1/5...  Step: 24...  loss: 3.0776...  6.381 sec/batch\n",
      "Epoch number: 1/5...  Step: 25...  loss: 3.0678...  6.425 sec/batch\n",
      "Epoch number: 1/5...  Step: 26...  loss: 3.0849...  6.394 sec/batch\n",
      "Epoch number: 1/5...  Step: 27...  loss: 3.0885...  6.499 sec/batch\n",
      "Epoch number: 1/5...  Step: 28...  loss: 3.0580...  6.344 sec/batch\n",
      "Epoch number: 1/5...  Step: 29...  loss: 3.0671...  6.377 sec/batch\n",
      "Epoch number: 1/5...  Step: 30...  loss: 3.0722...  6.390 sec/batch\n",
      "Epoch number: 1/5...  Step: 31...  loss: 3.0916...  6.386 sec/batch\n",
      "Epoch number: 1/5...  Step: 32...  loss: 3.0611...  6.376 sec/batch\n",
      "Epoch number: 1/5...  Step: 33...  loss: 3.0449...  6.343 sec/batch\n",
      "Epoch number: 1/5...  Step: 34...  loss: 3.0679...  6.370 sec/batch\n",
      "Epoch number: 1/5...  Step: 35...  loss: 3.0423...  6.369 sec/batch\n",
      "Epoch number: 1/5...  Step: 36...  loss: 3.0629...  6.416 sec/batch\n",
      "Epoch number: 1/5...  Step: 37...  loss: 3.0293...  6.357 sec/batch\n",
      "Epoch number: 1/5...  Step: 38...  loss: 3.0314...  6.382 sec/batch\n",
      "Epoch number: 1/5...  Step: 39...  loss: 3.0272...  6.371 sec/batch\n",
      "Epoch number: 1/5...  Step: 40...  loss: 3.0295...  6.462 sec/batch\n",
      "Epoch number: 1/5...  Step: 41...  loss: 3.0178...  6.372 sec/batch\n",
      "Epoch number: 1/5...  Step: 42...  loss: 3.0227...  6.369 sec/batch\n",
      "Epoch number: 1/5...  Step: 43...  loss: 3.0154...  6.535 sec/batch\n",
      "Epoch number: 1/5...  Step: 44...  loss: 3.0127...  6.914 sec/batch\n",
      "Epoch number: 1/5...  Step: 45...  loss: 3.0044...  6.632 sec/batch\n",
      "Epoch number: 1/5...  Step: 46...  loss: 3.0171...  6.737 sec/batch\n",
      "Epoch number: 1/5...  Step: 47...  loss: 3.0218...  6.771 sec/batch\n",
      "Epoch number: 1/5...  Step: 48...  loss: 3.0239...  6.915 sec/batch\n",
      "Epoch number: 1/5...  Step: 49...  loss: 3.0155...  9.043 sec/batch\n",
      "Epoch number: 1/5...  Step: 50...  loss: 3.0141...  10.414 sec/batch\n",
      "Epoch number: 1/5...  Step: 51...  loss: 3.0002...  9.221 sec/batch\n",
      "Epoch number: 1/5...  Step: 52...  loss: 2.9929...  8.596 sec/batch\n",
      "Epoch number: 1/5...  Step: 53...  loss: 2.9965...  7.535 sec/batch\n",
      "Epoch number: 1/5...  Step: 54...  loss: 2.9808...  9.713 sec/batch\n",
      "Epoch number: 1/5...  Step: 55...  loss: 2.9876...  12.456 sec/batch\n",
      "Epoch number: 1/5...  Step: 56...  loss: 2.9643...  10.818 sec/batch\n",
      "Epoch number: 1/5...  Step: 57...  loss: 2.9735...  7.962 sec/batch\n",
      "Epoch number: 1/5...  Step: 58...  loss: 2.9682...  7.999 sec/batch\n",
      "Epoch number: 1/5...  Step: 59...  loss: 2.9559...  7.048 sec/batch\n",
      "Epoch number: 1/5...  Step: 60...  loss: 2.9634...  6.980 sec/batch\n",
      "Epoch number: 1/5...  Step: 61...  loss: 2.9603...  7.104 sec/batch\n",
      "Epoch number: 1/5...  Step: 62...  loss: 2.9711...  7.265 sec/batch\n",
      "Epoch number: 1/5...  Step: 63...  loss: 2.9751...  6.628 sec/batch\n",
      "Epoch number: 1/5...  Step: 64...  loss: 2.9226...  6.818 sec/batch\n",
      "Epoch number: 1/5...  Step: 65...  loss: 2.9225...  6.874 sec/batch\n",
      "Epoch number: 1/5...  Step: 66...  loss: 2.9441...  6.993 sec/batch\n",
      "Epoch number: 1/5...  Step: 67...  loss: 2.9385...  7.365 sec/batch\n",
      "Epoch number: 1/5...  Step: 68...  loss: 2.9305...  6.973 sec/batch\n",
      "Epoch number: 1/5...  Step: 69...  loss: 2.9578...  6.681 sec/batch\n",
      "Epoch number: 1/5...  Step: 70...  loss: 2.9352...  6.657 sec/batch\n",
      "Epoch number: 1/5...  Step: 71...  loss: 2.9237...  6.850 sec/batch\n",
      "Epoch number: 1/5...  Step: 72...  loss: 2.9503...  6.947 sec/batch\n",
      "Epoch number: 1/5...  Step: 73...  loss: 2.8934...  7.393 sec/batch\n",
      "Epoch number: 1/5...  Step: 74...  loss: 2.9217...  7.288 sec/batch\n",
      "Epoch number: 1/5...  Step: 75...  loss: 2.9015...  7.022 sec/batch\n",
      "Epoch number: 1/5...  Step: 76...  loss: 2.9066...  6.736 sec/batch\n",
      "Epoch number: 1/5...  Step: 77...  loss: 2.9045...  6.744 sec/batch\n",
      "Epoch number: 1/5...  Step: 78...  loss: 2.8796...  6.750 sec/batch\n",
      "Epoch number: 1/5...  Step: 79...  loss: 2.8637...  6.840 sec/batch\n",
      "Epoch number: 1/5...  Step: 80...  loss: 2.8640...  6.878 sec/batch\n",
      "Epoch number: 1/5...  Step: 81...  loss: 2.8498...  6.960 sec/batch\n",
      "Epoch number: 1/5...  Step: 82...  loss: 2.8592...  6.852 sec/batch\n",
      "Epoch number: 1/5...  Step: 83...  loss: 2.8596...  7.054 sec/batch\n",
      "Epoch number: 1/5...  Step: 84...  loss: 2.8340...  7.873 sec/batch\n",
      "Epoch number: 1/5...  Step: 85...  loss: 2.8044...  7.434 sec/batch\n",
      "Epoch number: 1/5...  Step: 86...  loss: 2.8128...  7.748 sec/batch\n",
      "Epoch number: 1/5...  Step: 87...  loss: 2.8068...  8.430 sec/batch\n",
      "Epoch number: 1/5...  Step: 88...  loss: 2.7959...  7.320 sec/batch\n",
      "Epoch number: 1/5...  Step: 89...  loss: 2.7981...  7.536 sec/batch\n",
      "Epoch number: 1/5...  Step: 90...  loss: 2.7965...  7.359 sec/batch\n",
      "Epoch number: 1/5...  Step: 91...  loss: 2.7880...  7.210 sec/batch\n",
      "Epoch number: 1/5...  Step: 92...  loss: 2.7675...  8.211 sec/batch\n",
      "Epoch number: 1/5...  Step: 93...  loss: 2.7690...  7.811 sec/batch\n",
      "Epoch number: 1/5...  Step: 94...  loss: 2.7485...  8.198 sec/batch\n",
      "Epoch number: 1/5...  Step: 95...  loss: 2.7338...  7.957 sec/batch\n",
      "Epoch number: 1/5...  Step: 96...  loss: 2.7279...  6.757 sec/batch\n",
      "Epoch number: 1/5...  Step: 97...  loss: 2.7355...  6.714 sec/batch\n",
      "Epoch number: 1/5...  Step: 98...  loss: 2.7201...  6.618 sec/batch\n",
      "Epoch number: 1/5...  Step: 99...  loss: 2.7260...  6.598 sec/batch\n",
      "Epoch number: 1/5...  Step: 100...  loss: 2.7018...  6.818 sec/batch\n",
      "Epoch number: 1/5...  Step: 101...  loss: 2.7209...  6.594 sec/batch\n",
      "Epoch number: 1/5...  Step: 102...  loss: 2.6927...  6.625 sec/batch\n",
      "Epoch number: 1/5...  Step: 103...  loss: 2.6803...  6.613 sec/batch\n",
      "Epoch number: 1/5...  Step: 104...  loss: 2.6745...  6.584 sec/batch\n",
      "Epoch number: 1/5...  Step: 105...  loss: 2.6673...  6.667 sec/batch\n",
      "Epoch number: 1/5...  Step: 106...  loss: 2.6673...  6.665 sec/batch\n",
      "Epoch number: 1/5...  Step: 107...  loss: 2.6497...  6.631 sec/batch\n",
      "Epoch number: 1/5...  Step: 108...  loss: 2.6599...  6.564 sec/batch\n",
      "Epoch number: 1/5...  Step: 109...  loss: 2.6512...  6.584 sec/batch\n",
      "Epoch number: 1/5...  Step: 110...  loss: 2.6071...  6.615 sec/batch\n",
      "Epoch number: 1/5...  Step: 111...  loss: 2.6281...  6.566 sec/batch\n",
      "Epoch number: 1/5...  Step: 112...  loss: 2.6403...  6.560 sec/batch\n",
      "Epoch number: 1/5...  Step: 113...  loss: 2.6193...  6.602 sec/batch\n",
      "Epoch number: 1/5...  Step: 114...  loss: 2.5846...  6.722 sec/batch\n",
      "Epoch number: 1/5...  Step: 115...  loss: 2.6049...  7.345 sec/batch\n",
      "Epoch number: 1/5...  Step: 116...  loss: 2.5672...  6.943 sec/batch\n",
      "Epoch number: 1/5...  Step: 117...  loss: 2.5853...  6.932 sec/batch\n",
      "Epoch number: 1/5...  Step: 118...  loss: 2.5848...  7.628 sec/batch\n",
      "Epoch number: 1/5...  Step: 119...  loss: 2.6041...  6.589 sec/batch\n",
      "Epoch number: 1/5...  Step: 120...  loss: 2.5701...  6.607 sec/batch\n",
      "Epoch number: 1/5...  Step: 121...  loss: 2.5996...  6.610 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1/5...  Step: 122...  loss: 2.5608...  6.661 sec/batch\n",
      "Epoch number: 1/5...  Step: 123...  loss: 2.5617...  7.222 sec/batch\n",
      "Epoch number: 1/5...  Step: 124...  loss: 2.5829...  6.807 sec/batch\n",
      "Epoch number: 1/5...  Step: 125...  loss: 2.6361...  6.625 sec/batch\n",
      "Epoch number: 1/5...  Step: 126...  loss: 2.5401...  6.742 sec/batch\n",
      "Epoch number: 1/5...  Step: 127...  loss: 2.5783...  6.622 sec/batch\n",
      "Epoch number: 1/5...  Step: 128...  loss: 2.5695...  6.930 sec/batch\n",
      "Epoch number: 1/5...  Step: 129...  loss: 2.5429...  6.690 sec/batch\n",
      "Epoch number: 1/5...  Step: 130...  loss: 2.5458...  7.829 sec/batch\n",
      "Epoch number: 1/5...  Step: 131...  loss: 2.5373...  7.001 sec/batch\n",
      "Epoch number: 1/5...  Step: 132...  loss: 2.5274...  7.079 sec/batch\n",
      "Epoch number: 1/5...  Step: 133...  loss: 2.5341...  6.808 sec/batch\n",
      "Epoch number: 1/5...  Step: 134...  loss: 2.5284...  6.567 sec/batch\n",
      "Epoch number: 1/5...  Step: 135...  loss: 2.4970...  6.666 sec/batch\n",
      "Epoch number: 1/5...  Step: 136...  loss: 2.5049...  6.625 sec/batch\n",
      "Epoch number: 1/5...  Step: 137...  loss: 2.5047...  6.590 sec/batch\n",
      "Epoch number: 1/5...  Step: 138...  loss: 2.4935...  7.993 sec/batch\n",
      "Epoch number: 1/5...  Step: 139...  loss: 2.5192...  6.722 sec/batch\n",
      "Epoch number: 1/5...  Step: 140...  loss: 2.4839...  6.733 sec/batch\n",
      "Epoch number: 1/5...  Step: 141...  loss: 2.5058...  6.755 sec/batch\n",
      "Epoch number: 1/5...  Step: 142...  loss: 2.4655...  6.856 sec/batch\n",
      "Epoch number: 1/5...  Step: 143...  loss: 2.4812...  7.004 sec/batch\n",
      "Epoch number: 1/5...  Step: 144...  loss: 2.4604...  6.957 sec/batch\n",
      "Epoch number: 1/5...  Step: 145...  loss: 2.4669...  6.810 sec/batch\n",
      "Epoch number: 1/5...  Step: 146...  loss: 2.4900...  6.735 sec/batch\n",
      "Epoch number: 1/5...  Step: 147...  loss: 2.4613...  6.668 sec/batch\n",
      "Epoch number: 1/5...  Step: 148...  loss: 2.4748...  7.189 sec/batch\n",
      "Epoch number: 1/5...  Step: 149...  loss: 2.4444...  7.564 sec/batch\n",
      "Epoch number: 1/5...  Step: 150...  loss: 2.4389...  7.234 sec/batch\n",
      "Epoch number: 1/5...  Step: 151...  loss: 2.4701...  7.120 sec/batch\n",
      "Epoch number: 1/5...  Step: 152...  loss: 2.4780...  6.788 sec/batch\n",
      "Epoch number: 1/5...  Step: 153...  loss: 2.4555...  7.099 sec/batch\n",
      "Epoch number: 1/5...  Step: 154...  loss: 2.4507...  7.332 sec/batch\n",
      "Epoch number: 1/5...  Step: 155...  loss: 2.4223...  8.000 sec/batch\n",
      "Epoch number: 1/5...  Step: 156...  loss: 2.4244...  6.784 sec/batch\n",
      "Epoch number: 1/5...  Step: 157...  loss: 2.4101...  7.221 sec/batch\n",
      "Epoch number: 1/5...  Step: 158...  loss: 2.4097...  7.695 sec/batch\n",
      "Epoch number: 1/5...  Step: 159...  loss: 2.3892...  6.865 sec/batch\n",
      "Epoch number: 1/5...  Step: 160...  loss: 2.4229...  8.438 sec/batch\n",
      "Epoch number: 1/5...  Step: 161...  loss: 2.4150...  6.947 sec/batch\n",
      "Epoch number: 1/5...  Step: 162...  loss: 2.3872...  7.057 sec/batch\n",
      "Epoch number: 1/5...  Step: 163...  loss: 2.3800...  6.552 sec/batch\n",
      "Epoch number: 1/5...  Step: 164...  loss: 2.3895...  6.512 sec/batch\n",
      "Epoch number: 1/5...  Step: 165...  loss: 2.4066...  6.557 sec/batch\n",
      "Epoch number: 1/5...  Step: 166...  loss: 2.3932...  6.555 sec/batch\n",
      "Epoch number: 1/5...  Step: 167...  loss: 2.4149...  7.298 sec/batch\n",
      "Epoch number: 1/5...  Step: 168...  loss: 2.5552...  6.888 sec/batch\n",
      "Epoch number: 1/5...  Step: 169...  loss: 2.4744...  6.757 sec/batch\n",
      "Epoch number: 1/5...  Step: 170...  loss: 2.4122...  6.864 sec/batch\n",
      "Epoch number: 1/5...  Step: 171...  loss: 2.4283...  6.850 sec/batch\n",
      "Epoch number: 1/5...  Step: 172...  loss: 2.4751...  6.865 sec/batch\n",
      "Epoch number: 1/5...  Step: 173...  loss: 2.4501...  7.163 sec/batch\n",
      "Epoch number: 1/5...  Step: 174...  loss: 2.4641...  7.365 sec/batch\n",
      "Epoch number: 1/5...  Step: 175...  loss: 2.4521...  6.853 sec/batch\n",
      "Epoch number: 1/5...  Step: 176...  loss: 2.4072...  7.110 sec/batch\n",
      "Epoch number: 1/5...  Step: 177...  loss: 2.4011...  7.786 sec/batch\n",
      "Epoch number: 1/5...  Step: 178...  loss: 2.3870...  7.418 sec/batch\n",
      "Epoch number: 1/5...  Step: 179...  loss: 2.3728...  7.186 sec/batch\n",
      "Epoch number: 1/5...  Step: 180...  loss: 2.3656...  7.248 sec/batch\n",
      "Epoch number: 1/5...  Step: 181...  loss: 2.3846...  7.791 sec/batch\n",
      "Epoch number: 1/5...  Step: 182...  loss: 2.3861...  6.742 sec/batch\n",
      "Epoch number: 1/5...  Step: 183...  loss: 2.3629...  7.971 sec/batch\n",
      "Epoch number: 1/5...  Step: 184...  loss: 2.4004...  7.341 sec/batch\n",
      "Epoch number: 1/5...  Step: 185...  loss: 2.4158...  7.103 sec/batch\n",
      "Epoch number: 1/5...  Step: 186...  loss: 2.3768...  7.067 sec/batch\n",
      "Epoch number: 1/5...  Step: 187...  loss: 2.3448...  7.075 sec/batch\n",
      "Epoch number: 1/5...  Step: 188...  loss: 2.3468...  6.990 sec/batch\n",
      "Epoch number: 1/5...  Step: 189...  loss: 2.3516...  7.065 sec/batch\n",
      "Epoch number: 1/5...  Step: 190...  loss: 2.3504...  7.068 sec/batch\n",
      "Epoch number: 1/5...  Step: 191...  loss: 2.3653...  7.155 sec/batch\n",
      "Epoch number: 1/5...  Step: 192...  loss: 2.3217...  6.996 sec/batch\n",
      "Epoch number: 1/5...  Step: 193...  loss: 2.3466...  6.995 sec/batch\n",
      "Epoch number: 1/5...  Step: 194...  loss: 2.3471...  7.151 sec/batch\n",
      "Epoch number: 1/5...  Step: 195...  loss: 2.3268...  7.097 sec/batch\n",
      "Epoch number: 1/5...  Step: 196...  loss: 2.3341...  7.050 sec/batch\n",
      "Epoch number: 1/5...  Step: 197...  loss: 2.3187...  7.039 sec/batch\n",
      "Epoch number: 1/5...  Step: 198...  loss: 2.3128...  6.975 sec/batch\n",
      "Epoch number: 2/5...  Step: 199...  loss: 2.4384...  7.066 sec/batch\n",
      "Epoch number: 2/5...  Step: 200...  loss: 2.2978...  7.037 sec/batch\n",
      "Epoch number: 2/5...  Step: 201...  loss: 2.3062...  7.052 sec/batch\n",
      "Epoch number: 2/5...  Step: 202...  loss: 2.3174...  6.678 sec/batch\n",
      "Epoch number: 2/5...  Step: 203...  loss: 2.3105...  6.652 sec/batch\n",
      "Epoch number: 2/5...  Step: 204...  loss: 2.3176...  6.588 sec/batch\n",
      "Epoch number: 2/5...  Step: 205...  loss: 2.3196...  6.550 sec/batch\n",
      "Epoch number: 2/5...  Step: 206...  loss: 2.3342...  6.580 sec/batch\n",
      "Epoch number: 2/5...  Step: 207...  loss: 2.3712...  6.550 sec/batch\n",
      "Epoch number: 2/5...  Step: 208...  loss: 2.3189...  6.628 sec/batch\n",
      "Epoch number: 2/5...  Step: 209...  loss: 2.3131...  6.617 sec/batch\n",
      "Epoch number: 2/5...  Step: 210...  loss: 2.3222...  6.613 sec/batch\n",
      "Epoch number: 2/5...  Step: 211...  loss: 2.3143...  6.743 sec/batch\n",
      "Epoch number: 2/5...  Step: 212...  loss: 2.3591...  6.567 sec/batch\n",
      "Epoch number: 2/5...  Step: 213...  loss: 2.3179...  6.585 sec/batch\n",
      "Epoch number: 2/5...  Step: 214...  loss: 2.3045...  6.564 sec/batch\n",
      "Epoch number: 2/5...  Step: 215...  loss: 2.3214...  6.557 sec/batch\n",
      "Epoch number: 2/5...  Step: 216...  loss: 2.3424...  6.612 sec/batch\n",
      "Epoch number: 2/5...  Step: 217...  loss: 2.3198...  6.574 sec/batch\n",
      "Epoch number: 2/5...  Step: 218...  loss: 2.2847...  6.587 sec/batch\n",
      "Epoch number: 2/5...  Step: 219...  loss: 2.3008...  6.621 sec/batch\n",
      "Epoch number: 2/5...  Step: 220...  loss: 2.3284...  6.579 sec/batch\n",
      "Epoch number: 2/5...  Step: 221...  loss: 2.3026...  6.593 sec/batch\n",
      "Epoch number: 2/5...  Step: 222...  loss: 2.2795...  6.565 sec/batch\n",
      "Epoch number: 2/5...  Step: 223...  loss: 2.2870...  6.641 sec/batch\n",
      "Epoch number: 2/5...  Step: 224...  loss: 2.2845...  6.567 sec/batch\n",
      "Epoch number: 2/5...  Step: 225...  loss: 2.2802...  6.989 sec/batch\n",
      "Epoch number: 2/5...  Step: 226...  loss: 2.2834...  6.753 sec/batch\n",
      "Epoch number: 2/5...  Step: 227...  loss: 2.3024...  7.036 sec/batch\n",
      "Epoch number: 2/5...  Step: 228...  loss: 2.2959...  6.604 sec/batch\n",
      "Epoch number: 2/5...  Step: 229...  loss: 2.2945...  6.705 sec/batch\n",
      "Epoch number: 2/5...  Step: 230...  loss: 2.2646...  9.018 sec/batch\n",
      "Epoch number: 2/5...  Step: 231...  loss: 2.2638...  7.238 sec/batch\n",
      "Epoch number: 2/5...  Step: 232...  loss: 2.2916...  8.323 sec/batch\n",
      "Epoch number: 2/5...  Step: 233...  loss: 2.2594...  7.937 sec/batch\n",
      "Epoch number: 2/5...  Step: 234...  loss: 2.2795...  7.964 sec/batch\n",
      "Epoch number: 2/5...  Step: 235...  loss: 2.2683...  7.777 sec/batch\n",
      "Epoch number: 2/5...  Step: 236...  loss: 2.2325...  7.706 sec/batch\n",
      "Epoch number: 2/5...  Step: 237...  loss: 2.2490...  7.365 sec/batch\n",
      "Epoch number: 2/5...  Step: 238...  loss: 2.2408...  7.424 sec/batch\n",
      "Epoch number: 2/5...  Step: 239...  loss: 2.2365...  7.761 sec/batch\n",
      "Epoch number: 2/5...  Step: 240...  loss: 2.2480...  7.514 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 2/5...  Step: 241...  loss: 2.2363...  7.566 sec/batch\n",
      "Epoch number: 2/5...  Step: 242...  loss: 2.2335...  7.270 sec/batch\n",
      "Epoch number: 2/5...  Step: 243...  loss: 2.2369...  6.848 sec/batch\n",
      "Epoch number: 2/5...  Step: 244...  loss: 2.1976...  6.738 sec/batch\n",
      "Epoch number: 2/5...  Step: 245...  loss: 2.2597...  6.801 sec/batch\n",
      "Epoch number: 2/5...  Step: 246...  loss: 2.2400...  7.153 sec/batch\n",
      "Epoch number: 2/5...  Step: 247...  loss: 2.2401...  7.105 sec/batch\n",
      "Epoch number: 2/5...  Step: 248...  loss: 2.2894...  6.883 sec/batch\n",
      "Epoch number: 2/5...  Step: 249...  loss: 2.2330...  7.411 sec/batch\n",
      "Epoch number: 2/5...  Step: 250...  loss: 2.2809...  6.810 sec/batch\n",
      "Epoch number: 2/5...  Step: 251...  loss: 2.2288...  6.747 sec/batch\n",
      "Epoch number: 2/5...  Step: 252...  loss: 2.2369...  6.822 sec/batch\n",
      "Epoch number: 2/5...  Step: 253...  loss: 2.2326...  6.890 sec/batch\n",
      "Epoch number: 2/5...  Step: 254...  loss: 2.2507...  7.049 sec/batch\n",
      "Epoch number: 2/5...  Step: 255...  loss: 2.2374...  6.873 sec/batch\n",
      "Epoch number: 2/5...  Step: 256...  loss: 2.2155...  6.888 sec/batch\n",
      "Epoch number: 2/5...  Step: 257...  loss: 2.2263...  7.140 sec/batch\n",
      "Epoch number: 2/5...  Step: 258...  loss: 2.2519...  6.751 sec/batch\n",
      "Epoch number: 2/5...  Step: 259...  loss: 2.2206...  6.885 sec/batch\n",
      "Epoch number: 2/5...  Step: 260...  loss: 2.2411...  6.964 sec/batch\n",
      "Epoch number: 2/5...  Step: 261...  loss: 2.2494...  6.933 sec/batch\n",
      "Epoch number: 2/5...  Step: 262...  loss: 2.2227...  7.591 sec/batch\n",
      "Epoch number: 2/5...  Step: 263...  loss: 2.2137...  6.950 sec/batch\n",
      "Epoch number: 2/5...  Step: 264...  loss: 2.2385...  6.631 sec/batch\n",
      "Epoch number: 2/5...  Step: 265...  loss: 2.2251...  7.340 sec/batch\n",
      "Epoch number: 2/5...  Step: 266...  loss: 2.1952...  6.971 sec/batch\n",
      "Epoch number: 2/5...  Step: 267...  loss: 2.2021...  8.037 sec/batch\n",
      "Epoch number: 2/5...  Step: 268...  loss: 2.2182...  6.741 sec/batch\n",
      "Epoch number: 2/5...  Step: 269...  loss: 2.2251...  6.929 sec/batch\n",
      "Epoch number: 2/5...  Step: 270...  loss: 2.2258...  6.732 sec/batch\n",
      "Epoch number: 2/5...  Step: 271...  loss: 2.2204...  6.659 sec/batch\n",
      "Epoch number: 2/5...  Step: 272...  loss: 2.1988...  7.046 sec/batch\n",
      "Epoch number: 2/5...  Step: 273...  loss: 2.1990...  7.106 sec/batch\n",
      "Epoch number: 2/5...  Step: 274...  loss: 2.2532...  6.964 sec/batch\n",
      "Epoch number: 2/5...  Step: 275...  loss: 2.2003...  6.707 sec/batch\n",
      "Epoch number: 2/5...  Step: 276...  loss: 2.2242...  8.074 sec/batch\n",
      "Epoch number: 2/5...  Step: 277...  loss: 2.1823...  7.742 sec/batch\n",
      "Epoch number: 2/5...  Step: 278...  loss: 2.1982...  6.656 sec/batch\n",
      "Epoch number: 2/5...  Step: 279...  loss: 2.1894...  6.579 sec/batch\n",
      "Epoch number: 2/5...  Step: 280...  loss: 2.2436...  6.612 sec/batch\n",
      "Epoch number: 2/5...  Step: 281...  loss: 2.2063...  6.935 sec/batch\n",
      "Epoch number: 2/5...  Step: 282...  loss: 2.1821...  6.656 sec/batch\n",
      "Epoch number: 2/5...  Step: 283...  loss: 2.1546...  6.747 sec/batch\n",
      "Epoch number: 2/5...  Step: 284...  loss: 2.1871...  6.628 sec/batch\n",
      "Epoch number: 2/5...  Step: 285...  loss: 2.1990...  6.572 sec/batch\n",
      "Epoch number: 2/5...  Step: 286...  loss: 2.1886...  6.558 sec/batch\n",
      "Epoch number: 2/5...  Step: 287...  loss: 2.1781...  6.559 sec/batch\n",
      "Epoch number: 2/5...  Step: 288...  loss: 2.2053...  6.738 sec/batch\n",
      "Epoch number: 2/5...  Step: 289...  loss: 2.1775...  8.105 sec/batch\n",
      "Epoch number: 2/5...  Step: 290...  loss: 2.1889...  7.833 sec/batch\n",
      "Epoch number: 2/5...  Step: 291...  loss: 2.1717...  7.670 sec/batch\n",
      "Epoch number: 2/5...  Step: 292...  loss: 2.1686...  7.247 sec/batch\n",
      "Epoch number: 2/5...  Step: 293...  loss: 2.1723...  7.076 sec/batch\n",
      "Epoch number: 2/5...  Step: 294...  loss: 2.1694...  7.828 sec/batch\n",
      "Epoch number: 2/5...  Step: 295...  loss: 2.1727...  7.100 sec/batch\n",
      "Epoch number: 2/5...  Step: 296...  loss: 2.1677...  7.075 sec/batch\n",
      "Epoch number: 2/5...  Step: 297...  loss: 2.1643...  6.900 sec/batch\n",
      "Epoch number: 2/5...  Step: 298...  loss: 2.1493...  6.799 sec/batch\n",
      "Epoch number: 2/5...  Step: 299...  loss: 2.1880...  6.727 sec/batch\n",
      "Epoch number: 2/5...  Step: 300...  loss: 2.1754...  7.247 sec/batch\n",
      "Epoch number: 2/5...  Step: 301...  loss: 2.1558...  6.817 sec/batch\n",
      "Epoch number: 2/5...  Step: 302...  loss: 2.1667...  7.047 sec/batch\n",
      "Epoch number: 2/5...  Step: 303...  loss: 2.1473...  6.934 sec/batch\n",
      "Epoch number: 2/5...  Step: 304...  loss: 2.1671...  6.858 sec/batch\n",
      "Epoch number: 2/5...  Step: 305...  loss: 2.1623...  6.833 sec/batch\n",
      "Epoch number: 2/5...  Step: 306...  loss: 2.1941...  6.766 sec/batch\n",
      "Epoch number: 2/5...  Step: 307...  loss: 2.1694...  6.819 sec/batch\n",
      "Epoch number: 2/5...  Step: 308...  loss: 2.1485...  6.814 sec/batch\n",
      "Epoch number: 2/5...  Step: 309...  loss: 2.1601...  6.982 sec/batch\n",
      "Epoch number: 2/5...  Step: 310...  loss: 2.1750...  6.908 sec/batch\n",
      "Epoch number: 2/5...  Step: 311...  loss: 2.1629...  7.018 sec/batch\n",
      "Epoch number: 2/5...  Step: 312...  loss: 2.1471...  6.889 sec/batch\n",
      "Epoch number: 2/5...  Step: 313...  loss: 2.1505...  6.723 sec/batch\n",
      "Epoch number: 2/5...  Step: 314...  loss: 2.1195...  6.839 sec/batch\n",
      "Epoch number: 2/5...  Step: 315...  loss: 2.1556...  6.829 sec/batch\n",
      "Epoch number: 2/5...  Step: 316...  loss: 2.1520...  6.991 sec/batch\n",
      "Epoch number: 2/5...  Step: 317...  loss: 2.1744...  6.898 sec/batch\n",
      "Epoch number: 2/5...  Step: 318...  loss: 2.1607...  6.758 sec/batch\n",
      "Epoch number: 2/5...  Step: 319...  loss: 2.1751...  6.826 sec/batch\n",
      "Epoch number: 2/5...  Step: 320...  loss: 2.1382...  6.776 sec/batch\n",
      "Epoch number: 2/5...  Step: 321...  loss: 2.1431...  6.850 sec/batch\n",
      "Epoch number: 2/5...  Step: 322...  loss: 2.1765...  6.871 sec/batch\n",
      "Epoch number: 2/5...  Step: 323...  loss: 2.1496...  7.007 sec/batch\n",
      "Epoch number: 2/5...  Step: 324...  loss: 2.1382...  6.883 sec/batch\n",
      "Epoch number: 2/5...  Step: 325...  loss: 2.1568...  7.119 sec/batch\n",
      "Epoch number: 2/5...  Step: 326...  loss: 2.1701...  6.905 sec/batch\n",
      "Epoch number: 2/5...  Step: 327...  loss: 2.1483...  6.684 sec/batch\n",
      "Epoch number: 2/5...  Step: 328...  loss: 2.1577...  7.000 sec/batch\n",
      "Epoch number: 2/5...  Step: 329...  loss: 2.1359...  7.297 sec/batch\n",
      "Epoch number: 2/5...  Step: 330...  loss: 2.1184...  6.972 sec/batch\n",
      "Epoch number: 2/5...  Step: 331...  loss: 2.1506...  6.850 sec/batch\n",
      "Epoch number: 2/5...  Step: 332...  loss: 2.1555...  7.074 sec/batch\n",
      "Epoch number: 2/5...  Step: 333...  loss: 2.1433...  8.102 sec/batch\n",
      "Epoch number: 2/5...  Step: 334...  loss: 2.1508...  6.945 sec/batch\n",
      "Epoch number: 2/5...  Step: 335...  loss: 2.1440...  6.845 sec/batch\n",
      "Epoch number: 2/5...  Step: 336...  loss: 2.1390...  6.897 sec/batch\n",
      "Epoch number: 2/5...  Step: 337...  loss: 2.1770...  6.941 sec/batch\n",
      "Epoch number: 2/5...  Step: 338...  loss: 2.1311...  6.974 sec/batch\n",
      "Epoch number: 2/5...  Step: 339...  loss: 2.1617...  6.996 sec/batch\n",
      "Epoch number: 2/5...  Step: 340...  loss: 2.1312...  6.802 sec/batch\n",
      "Epoch number: 2/5...  Step: 341...  loss: 2.1344...  6.787 sec/batch\n",
      "Epoch number: 2/5...  Step: 342...  loss: 2.1320...  6.851 sec/batch\n",
      "Epoch number: 2/5...  Step: 343...  loss: 2.1158...  6.846 sec/batch\n",
      "Epoch number: 2/5...  Step: 344...  loss: 2.1643...  7.006 sec/batch\n",
      "Epoch number: 2/5...  Step: 345...  loss: 2.1366...  7.081 sec/batch\n",
      "Epoch number: 2/5...  Step: 346...  loss: 2.1462...  6.856 sec/batch\n",
      "Epoch number: 2/5...  Step: 347...  loss: 2.1277...  6.763 sec/batch\n",
      "Epoch number: 2/5...  Step: 348...  loss: 2.1110...  6.765 sec/batch\n",
      "Epoch number: 2/5...  Step: 349...  loss: 2.1323...  6.902 sec/batch\n",
      "Epoch number: 2/5...  Step: 350...  loss: 2.1604...  6.840 sec/batch\n",
      "Epoch number: 2/5...  Step: 351...  loss: 2.1447...  6.948 sec/batch\n",
      "Epoch number: 2/5...  Step: 352...  loss: 2.1439...  6.947 sec/batch\n",
      "Epoch number: 2/5...  Step: 353...  loss: 2.1187...  6.795 sec/batch\n",
      "Epoch number: 2/5...  Step: 354...  loss: 2.1155...  6.744 sec/batch\n",
      "Epoch number: 2/5...  Step: 355...  loss: 2.1086...  6.811 sec/batch\n",
      "Epoch number: 2/5...  Step: 356...  loss: 2.1050...  6.832 sec/batch\n",
      "Epoch number: 2/5...  Step: 357...  loss: 2.0808...  6.864 sec/batch\n",
      "Epoch number: 2/5...  Step: 358...  loss: 2.1588...  7.077 sec/batch\n",
      "Epoch number: 2/5...  Step: 359...  loss: 2.1227...  6.924 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 2/5...  Step: 360...  loss: 2.1109...  6.793 sec/batch\n",
      "Epoch number: 2/5...  Step: 361...  loss: 2.1157...  6.765 sec/batch\n",
      "Epoch number: 2/5...  Step: 362...  loss: 2.1097...  6.777 sec/batch\n",
      "Epoch number: 2/5...  Step: 363...  loss: 2.1243...  6.919 sec/batch\n",
      "Epoch number: 2/5...  Step: 364...  loss: 2.1183...  6.876 sec/batch\n",
      "Epoch number: 2/5...  Step: 365...  loss: 2.1295...  6.977 sec/batch\n",
      "Epoch number: 2/5...  Step: 366...  loss: 2.1319...  6.911 sec/batch\n",
      "Epoch number: 2/5...  Step: 367...  loss: 2.1114...  6.816 sec/batch\n",
      "Epoch number: 2/5...  Step: 368...  loss: 2.0973...  6.828 sec/batch\n",
      "Epoch number: 2/5...  Step: 369...  loss: 2.1165...  6.725 sec/batch\n",
      "Epoch number: 2/5...  Step: 370...  loss: 2.1276...  6.805 sec/batch\n",
      "Epoch number: 2/5...  Step: 371...  loss: 2.1457...  6.898 sec/batch\n",
      "Epoch number: 2/5...  Step: 372...  loss: 2.1558...  6.994 sec/batch\n",
      "Epoch number: 2/5...  Step: 373...  loss: 2.1422...  6.919 sec/batch\n",
      "Epoch number: 2/5...  Step: 374...  loss: 2.1975...  6.821 sec/batch\n",
      "Epoch number: 2/5...  Step: 375...  loss: 2.2061...  6.752 sec/batch\n",
      "Epoch number: 2/5...  Step: 376...  loss: 2.1893...  6.795 sec/batch\n",
      "Epoch number: 2/5...  Step: 377...  loss: 2.1705...  6.869 sec/batch\n",
      "Epoch number: 2/5...  Step: 378...  loss: 2.1252...  6.913 sec/batch\n",
      "Epoch number: 2/5...  Step: 379...  loss: 2.1289...  6.962 sec/batch\n",
      "Epoch number: 2/5...  Step: 380...  loss: 2.1608...  7.143 sec/batch\n",
      "Epoch number: 2/5...  Step: 381...  loss: 2.1490...  6.866 sec/batch\n",
      "Epoch number: 2/5...  Step: 382...  loss: 2.1643...  6.770 sec/batch\n",
      "Epoch number: 2/5...  Step: 383...  loss: 2.1877...  6.895 sec/batch\n",
      "Epoch number: 2/5...  Step: 384...  loss: 2.1334...  6.800 sec/batch\n",
      "Epoch number: 2/5...  Step: 385...  loss: 2.1302...  7.069 sec/batch\n",
      "Epoch number: 2/5...  Step: 386...  loss: 2.0999...  7.378 sec/batch\n",
      "Epoch number: 2/5...  Step: 387...  loss: 2.1139...  7.375 sec/batch\n",
      "Epoch number: 2/5...  Step: 388...  loss: 2.1180...  7.472 sec/batch\n",
      "Epoch number: 2/5...  Step: 389...  loss: 2.1303...  7.279 sec/batch\n",
      "Epoch number: 2/5...  Step: 390...  loss: 2.0979...  7.137 sec/batch\n",
      "Epoch number: 2/5...  Step: 391...  loss: 2.1111...  7.477 sec/batch\n",
      "Epoch number: 2/5...  Step: 392...  loss: 2.1098...  7.646 sec/batch\n",
      "Epoch number: 2/5...  Step: 393...  loss: 2.0864...  7.434 sec/batch\n",
      "Epoch number: 2/5...  Step: 394...  loss: 2.1134...  6.950 sec/batch\n",
      "Epoch number: 2/5...  Step: 395...  loss: 2.0927...  6.907 sec/batch\n",
      "Epoch number: 2/5...  Step: 396...  loss: 2.0834...  8.505 sec/batch\n",
      "Epoch number: 3/5...  Step: 397...  loss: 2.2071...  7.006 sec/batch\n",
      "Epoch number: 3/5...  Step: 398...  loss: 2.0719...  6.785 sec/batch\n",
      "Epoch number: 3/5...  Step: 399...  loss: 2.0808...  7.237 sec/batch\n",
      "Epoch number: 3/5...  Step: 400...  loss: 2.0866...  6.861 sec/batch\n",
      "Epoch number: 3/5...  Step: 401...  loss: 2.0791...  7.095 sec/batch\n",
      "Epoch number: 3/5...  Step: 402...  loss: 2.0748...  7.316 sec/batch\n",
      "Epoch number: 3/5...  Step: 403...  loss: 2.0889...  7.344 sec/batch\n",
      "Epoch number: 3/5...  Step: 404...  loss: 2.0900...  7.400 sec/batch\n",
      "Epoch number: 3/5...  Step: 405...  loss: 2.1067...  7.412 sec/batch\n",
      "Epoch number: 3/5...  Step: 406...  loss: 2.0828...  7.415 sec/batch\n",
      "Epoch number: 3/5...  Step: 407...  loss: 2.0701...  7.306 sec/batch\n",
      "Epoch number: 3/5...  Step: 408...  loss: 2.0642...  7.767 sec/batch\n",
      "Epoch number: 3/5...  Step: 409...  loss: 2.0850...  7.862 sec/batch\n",
      "Epoch number: 3/5...  Step: 410...  loss: 2.1116...  7.231 sec/batch\n",
      "Epoch number: 3/5...  Step: 411...  loss: 2.0750...  8.124 sec/batch\n",
      "Epoch number: 3/5...  Step: 412...  loss: 2.0565...  7.541 sec/batch\n",
      "Epoch number: 3/5...  Step: 413...  loss: 2.0759...  7.940 sec/batch\n",
      "Epoch number: 3/5...  Step: 414...  loss: 2.1164...  7.336 sec/batch\n",
      "Epoch number: 3/5...  Step: 415...  loss: 2.0817...  7.244 sec/batch\n",
      "Epoch number: 3/5...  Step: 416...  loss: 2.0718...  7.636 sec/batch\n",
      "Epoch number: 3/5...  Step: 417...  loss: 2.0674...  8.453 sec/batch\n",
      "Epoch number: 3/5...  Step: 418...  loss: 2.1352...  8.723 sec/batch\n",
      "Epoch number: 3/5...  Step: 419...  loss: 2.0656...  7.570 sec/batch\n",
      "Epoch number: 3/5...  Step: 420...  loss: 2.0618...  7.241 sec/batch\n",
      "Epoch number: 3/5...  Step: 421...  loss: 2.0689...  7.126 sec/batch\n",
      "Epoch number: 3/5...  Step: 422...  loss: 2.0537...  7.313 sec/batch\n",
      "Epoch number: 3/5...  Step: 423...  loss: 2.0461...  7.205 sec/batch\n",
      "Epoch number: 3/5...  Step: 424...  loss: 2.0696...  7.068 sec/batch\n",
      "Epoch number: 3/5...  Step: 425...  loss: 2.0953...  7.540 sec/batch\n",
      "Epoch number: 3/5...  Step: 426...  loss: 2.0750...  6.905 sec/batch\n",
      "Epoch number: 3/5...  Step: 427...  loss: 2.0628...  6.769 sec/batch\n",
      "Epoch number: 3/5...  Step: 428...  loss: 2.0472...  7.953 sec/batch\n",
      "Epoch number: 3/5...  Step: 429...  loss: 2.0552...  7.514 sec/batch\n",
      "Epoch number: 3/5...  Step: 430...  loss: 2.0861...  7.043 sec/batch\n",
      "Epoch number: 3/5...  Step: 431...  loss: 2.0464...  6.991 sec/batch\n",
      "Epoch number: 3/5...  Step: 432...  loss: 2.0585...  8.080 sec/batch\n",
      "Epoch number: 3/5...  Step: 433...  loss: 2.0522...  7.659 sec/batch\n",
      "Epoch number: 3/5...  Step: 434...  loss: 2.0165...  9.238 sec/batch\n",
      "Epoch number: 3/5...  Step: 435...  loss: 2.0310...  8.851 sec/batch\n",
      "Epoch number: 3/5...  Step: 436...  loss: 2.0266...  8.050 sec/batch\n",
      "Epoch number: 3/5...  Step: 437...  loss: 2.0344...  7.452 sec/batch\n",
      "Epoch number: 3/5...  Step: 438...  loss: 2.0523...  8.138 sec/batch\n",
      "Epoch number: 3/5...  Step: 439...  loss: 2.0345...  7.398 sec/batch\n",
      "Epoch number: 3/5...  Step: 440...  loss: 2.0310...  8.415 sec/batch\n",
      "Epoch number: 3/5...  Step: 441...  loss: 2.0468...  9.433 sec/batch\n",
      "Epoch number: 3/5...  Step: 442...  loss: 1.9892...  7.445 sec/batch\n",
      "Epoch number: 3/5...  Step: 443...  loss: 2.0505...  8.697 sec/batch\n",
      "Epoch number: 3/5...  Step: 444...  loss: 2.0298...  9.975 sec/batch\n",
      "Epoch number: 3/5...  Step: 445...  loss: 2.0324...  8.680 sec/batch\n",
      "Epoch number: 3/5...  Step: 446...  loss: 2.0749...  7.411 sec/batch\n",
      "Epoch number: 3/5...  Step: 447...  loss: 2.0118...  8.400 sec/batch\n",
      "Epoch number: 3/5...  Step: 448...  loss: 2.0773...  7.217 sec/batch\n",
      "Epoch number: 3/5...  Step: 449...  loss: 2.0279...  7.600 sec/batch\n",
      "Epoch number: 3/5...  Step: 450...  loss: 2.0233...  7.954 sec/batch\n",
      "Epoch number: 3/5...  Step: 451...  loss: 2.0272...  7.453 sec/batch\n",
      "Epoch number: 3/5...  Step: 452...  loss: 2.0522...  7.553 sec/batch\n",
      "Epoch number: 3/5...  Step: 453...  loss: 2.0359...  7.644 sec/batch\n",
      "Epoch number: 3/5...  Step: 454...  loss: 2.0204...  8.107 sec/batch\n",
      "Epoch number: 3/5...  Step: 455...  loss: 2.0176...  8.760 sec/batch\n",
      "Epoch number: 3/5...  Step: 456...  loss: 2.0784...  9.094 sec/batch\n",
      "Epoch number: 3/5...  Step: 457...  loss: 2.0255...  9.405 sec/batch\n",
      "Epoch number: 3/5...  Step: 458...  loss: 2.0589...  9.061 sec/batch\n",
      "Epoch number: 3/5...  Step: 459...  loss: 2.0591...  7.334 sec/batch\n",
      "Epoch number: 3/5...  Step: 460...  loss: 2.0369...  7.085 sec/batch\n",
      "Epoch number: 3/5...  Step: 461...  loss: 2.0172...  7.112 sec/batch\n",
      "Epoch number: 3/5...  Step: 462...  loss: 2.0512...  7.041 sec/batch\n",
      "Epoch number: 3/5...  Step: 463...  loss: 2.0313...  7.030 sec/batch\n",
      "Epoch number: 3/5...  Step: 464...  loss: 1.9974...  7.143 sec/batch\n",
      "Epoch number: 3/5...  Step: 465...  loss: 2.0107...  7.030 sec/batch\n",
      "Epoch number: 3/5...  Step: 466...  loss: 2.0263...  7.082 sec/batch\n",
      "Epoch number: 3/5...  Step: 467...  loss: 2.0509...  8.135 sec/batch\n",
      "Epoch number: 3/5...  Step: 468...  loss: 2.0324...  7.306 sec/batch\n",
      "Epoch number: 3/5...  Step: 469...  loss: 2.0409...  6.991 sec/batch\n",
      "Epoch number: 3/5...  Step: 470...  loss: 1.9977...  6.999 sec/batch\n",
      "Epoch number: 3/5...  Step: 471...  loss: 2.0124...  6.775 sec/batch\n",
      "Epoch number: 3/5...  Step: 472...  loss: 2.0595...  6.843 sec/batch\n",
      "Epoch number: 3/5...  Step: 473...  loss: 2.0160...  7.010 sec/batch\n",
      "Epoch number: 3/5...  Step: 474...  loss: 2.0273...  7.703 sec/batch\n",
      "Epoch number: 3/5...  Step: 475...  loss: 1.9859...  7.220 sec/batch\n",
      "Epoch number: 3/5...  Step: 476...  loss: 2.0048...  7.015 sec/batch\n",
      "Epoch number: 3/5...  Step: 477...  loss: 1.9789...  6.898 sec/batch\n",
      "Epoch number: 3/5...  Step: 478...  loss: 2.0235...  6.842 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 3/5...  Step: 479...  loss: 1.9809...  6.819 sec/batch\n",
      "Epoch number: 3/5...  Step: 480...  loss: 2.0021...  7.004 sec/batch\n",
      "Epoch number: 3/5...  Step: 481...  loss: 1.9675...  6.989 sec/batch\n",
      "Epoch number: 3/5...  Step: 482...  loss: 1.9913...  7.248 sec/batch\n",
      "Epoch number: 3/5...  Step: 483...  loss: 2.0026...  6.977 sec/batch\n",
      "Epoch number: 3/5...  Step: 484...  loss: 1.9892...  6.838 sec/batch\n",
      "Epoch number: 3/5...  Step: 485...  loss: 1.9781...  6.834 sec/batch\n",
      "Epoch number: 3/5...  Step: 486...  loss: 2.0174...  6.852 sec/batch\n",
      "Epoch number: 3/5...  Step: 487...  loss: 1.9833...  7.295 sec/batch\n",
      "Epoch number: 3/5...  Step: 488...  loss: 1.9994...  7.139 sec/batch\n",
      "Epoch number: 3/5...  Step: 489...  loss: 1.9716...  6.989 sec/batch\n",
      "Epoch number: 3/5...  Step: 490...  loss: 1.9807...  6.787 sec/batch\n",
      "Epoch number: 3/5...  Step: 491...  loss: 1.9836...  7.035 sec/batch\n",
      "Epoch number: 3/5...  Step: 492...  loss: 1.9905...  7.128 sec/batch\n",
      "Epoch number: 3/5...  Step: 493...  loss: 1.9846...  6.904 sec/batch\n",
      "Epoch number: 3/5...  Step: 494...  loss: 1.9802...  6.807 sec/batch\n",
      "Epoch number: 3/5...  Step: 495...  loss: 1.9789...  6.863 sec/batch\n",
      "Epoch number: 3/5...  Step: 496...  loss: 1.9551...  6.933 sec/batch\n",
      "Epoch number: 3/5...  Step: 497...  loss: 2.0085...  6.804 sec/batch\n",
      "Epoch number: 3/5...  Step: 498...  loss: 2.0113...  6.728 sec/batch\n",
      "Epoch number: 3/5...  Step: 499...  loss: 1.9849...  7.159 sec/batch\n",
      "Epoch number: 3/5...  Step: 500...  loss: 1.9781...  6.836 sec/batch\n",
      "Epoch number: 3/5...  Step: 501...  loss: 1.9705...  6.913 sec/batch\n",
      "Epoch number: 3/5...  Step: 502...  loss: 2.0023...  6.878 sec/batch\n",
      "Epoch number: 3/5...  Step: 503...  loss: 1.9848...  7.175 sec/batch\n",
      "Epoch number: 3/5...  Step: 504...  loss: 2.0054...  6.929 sec/batch\n",
      "Epoch number: 3/5...  Step: 505...  loss: 1.9936...  6.980 sec/batch\n",
      "Epoch number: 3/5...  Step: 506...  loss: 1.9855...  6.757 sec/batch\n",
      "Epoch number: 3/5...  Step: 507...  loss: 1.9826...  6.913 sec/batch\n",
      "Epoch number: 3/5...  Step: 508...  loss: 1.9848...  6.892 sec/batch\n",
      "Epoch number: 3/5...  Step: 509...  loss: 1.9899...  6.931 sec/batch\n",
      "Epoch number: 3/5...  Step: 510...  loss: 1.9760...  7.258 sec/batch\n",
      "Epoch number: 3/5...  Step: 511...  loss: 1.9728...  7.132 sec/batch\n",
      "Epoch number: 3/5...  Step: 512...  loss: 1.9512...  7.053 sec/batch\n",
      "Epoch number: 3/5...  Step: 513...  loss: 1.9860...  6.728 sec/batch\n",
      "Epoch number: 3/5...  Step: 514...  loss: 1.9801...  7.083 sec/batch\n",
      "Epoch number: 3/5...  Step: 515...  loss: 1.9998...  6.872 sec/batch\n",
      "Epoch number: 3/5...  Step: 516...  loss: 1.9855...  6.850 sec/batch\n",
      "Epoch number: 3/5...  Step: 517...  loss: 2.0034...  6.884 sec/batch\n",
      "Epoch number: 3/5...  Step: 518...  loss: 1.9581...  6.726 sec/batch\n",
      "Epoch number: 3/5...  Step: 519...  loss: 1.9584...  6.745 sec/batch\n",
      "Epoch number: 3/5...  Step: 520...  loss: 2.0035...  6.958 sec/batch\n",
      "Epoch number: 3/5...  Step: 521...  loss: 1.9721...  8.124 sec/batch\n",
      "Epoch number: 3/5...  Step: 522...  loss: 1.9496...  7.009 sec/batch\n",
      "Epoch number: 3/5...  Step: 523...  loss: 1.9900...  6.789 sec/batch\n",
      "Epoch number: 3/5...  Step: 524...  loss: 1.9946...  6.867 sec/batch\n",
      "Epoch number: 3/5...  Step: 525...  loss: 1.9771...  6.970 sec/batch\n",
      "Epoch number: 3/5...  Step: 526...  loss: 1.9876...  6.987 sec/batch\n",
      "Epoch number: 3/5...  Step: 527...  loss: 1.9535...  6.934 sec/batch\n",
      "Epoch number: 3/5...  Step: 528...  loss: 1.9388...  6.780 sec/batch\n",
      "Epoch number: 3/5...  Step: 529...  loss: 1.9844...  7.205 sec/batch\n",
      "Epoch number: 3/5...  Step: 530...  loss: 1.9835...  8.174 sec/batch\n",
      "Epoch number: 3/5...  Step: 531...  loss: 1.9828...  6.723 sec/batch\n",
      "Epoch number: 3/5...  Step: 532...  loss: 1.9752...  6.794 sec/batch\n",
      "Epoch number: 3/5...  Step: 533...  loss: 1.9827...  6.899 sec/batch\n",
      "Epoch number: 3/5...  Step: 534...  loss: 1.9734...  7.004 sec/batch\n",
      "Epoch number: 3/5...  Step: 535...  loss: 2.0106...  6.792 sec/batch\n",
      "Epoch number: 3/5...  Step: 536...  loss: 1.9603...  6.822 sec/batch\n",
      "Epoch number: 3/5...  Step: 537...  loss: 1.9987...  6.731 sec/batch\n",
      "Epoch number: 3/5...  Step: 538...  loss: 1.9674...  6.792 sec/batch\n",
      "Epoch number: 3/5...  Step: 539...  loss: 1.9733...  6.816 sec/batch\n",
      "Epoch number: 3/5...  Step: 540...  loss: 1.9643...  6.722 sec/batch\n",
      "Epoch number: 3/5...  Step: 541...  loss: 1.9570...  6.732 sec/batch\n",
      "Epoch number: 3/5...  Step: 542...  loss: 1.9944...  6.847 sec/batch\n",
      "Epoch number: 3/5...  Step: 543...  loss: 1.9823...  6.806 sec/batch\n",
      "Epoch number: 3/5...  Step: 544...  loss: 1.9890...  6.968 sec/batch\n",
      "Epoch number: 3/5...  Step: 545...  loss: 1.9766...  6.793 sec/batch\n",
      "Epoch number: 3/5...  Step: 546...  loss: 1.9551...  6.725 sec/batch\n",
      "Epoch number: 3/5...  Step: 547...  loss: 1.9563...  6.828 sec/batch\n",
      "Epoch number: 3/5...  Step: 548...  loss: 1.9979...  6.739 sec/batch\n",
      "Epoch number: 3/5...  Step: 549...  loss: 1.9823...  6.795 sec/batch\n",
      "Epoch number: 3/5...  Step: 550...  loss: 1.9736...  6.795 sec/batch\n",
      "Epoch number: 3/5...  Step: 551...  loss: 1.9641...  6.799 sec/batch\n",
      "Epoch number: 3/5...  Step: 552...  loss: 1.9617...  6.799 sec/batch\n",
      "Epoch number: 3/5...  Step: 553...  loss: 1.9566...  6.704 sec/batch\n",
      "Epoch number: 3/5...  Step: 554...  loss: 1.9513...  6.754 sec/batch\n",
      "Epoch number: 3/5...  Step: 555...  loss: 1.9300...  6.743 sec/batch\n",
      "Epoch number: 3/5...  Step: 556...  loss: 2.0064...  6.765 sec/batch\n",
      "Epoch number: 3/5...  Step: 557...  loss: 1.9790...  6.755 sec/batch\n",
      "Epoch number: 3/5...  Step: 558...  loss: 1.9557...  7.002 sec/batch\n",
      "Epoch number: 3/5...  Step: 559...  loss: 1.9705...  6.729 sec/batch\n",
      "Epoch number: 3/5...  Step: 560...  loss: 1.9648...  6.727 sec/batch\n",
      "Epoch number: 3/5...  Step: 561...  loss: 1.9580...  6.783 sec/batch\n",
      "Epoch number: 3/5...  Step: 562...  loss: 1.9524...  6.915 sec/batch\n",
      "Epoch number: 3/5...  Step: 563...  loss: 1.9767...  6.783 sec/batch\n",
      "Epoch number: 3/5...  Step: 564...  loss: 1.9915...  6.946 sec/batch\n",
      "Epoch number: 3/5...  Step: 565...  loss: 1.9495...  6.860 sec/batch\n",
      "Epoch number: 3/5...  Step: 566...  loss: 1.9444...  6.752 sec/batch\n",
      "Epoch number: 3/5...  Step: 567...  loss: 1.9638...  6.719 sec/batch\n",
      "Epoch number: 3/5...  Step: 568...  loss: 1.9823...  6.652 sec/batch\n",
      "Epoch number: 3/5...  Step: 569...  loss: 2.0032...  6.773 sec/batch\n",
      "Epoch number: 3/5...  Step: 570...  loss: 1.9957...  6.918 sec/batch\n",
      "Epoch number: 3/5...  Step: 571...  loss: 1.9966...  7.973 sec/batch\n",
      "Epoch number: 3/5...  Step: 572...  loss: 1.9513...  6.806 sec/batch\n",
      "Epoch number: 3/5...  Step: 573...  loss: 1.9291...  6.882 sec/batch\n",
      "Epoch number: 3/5...  Step: 574...  loss: 1.9594...  6.872 sec/batch\n",
      "Epoch number: 3/5...  Step: 575...  loss: 1.9323...  6.912 sec/batch\n",
      "Epoch number: 3/5...  Step: 576...  loss: 1.9123...  6.588 sec/batch\n",
      "Epoch number: 3/5...  Step: 577...  loss: 1.9231...  6.499 sec/batch\n",
      "Epoch number: 3/5...  Step: 578...  loss: 1.9460...  6.481 sec/batch\n",
      "Epoch number: 3/5...  Step: 579...  loss: 1.9497...  6.615 sec/batch\n",
      "Epoch number: 3/5...  Step: 580...  loss: 1.9718...  6.609 sec/batch\n",
      "Epoch number: 3/5...  Step: 581...  loss: 1.9814...  6.581 sec/batch\n",
      "Epoch number: 3/5...  Step: 582...  loss: 1.9426...  6.492 sec/batch\n",
      "Epoch number: 3/5...  Step: 583...  loss: 1.9464...  6.699 sec/batch\n",
      "Epoch number: 3/5...  Step: 584...  loss: 1.9202...  7.342 sec/batch\n",
      "Epoch number: 3/5...  Step: 585...  loss: 1.9217...  6.976 sec/batch\n",
      "Epoch number: 3/5...  Step: 586...  loss: 1.9340...  6.601 sec/batch\n",
      "Epoch number: 3/5...  Step: 587...  loss: 1.9435...  6.497 sec/batch\n",
      "Epoch number: 3/5...  Step: 588...  loss: 1.9140...  6.503 sec/batch\n",
      "Epoch number: 3/5...  Step: 589...  loss: 1.9385...  6.517 sec/batch\n",
      "Epoch number: 3/5...  Step: 590...  loss: 1.9304...  6.529 sec/batch\n",
      "Epoch number: 3/5...  Step: 591...  loss: 1.9155...  6.530 sec/batch\n",
      "Epoch number: 3/5...  Step: 592...  loss: 1.9338...  6.516 sec/batch\n",
      "Epoch number: 3/5...  Step: 593...  loss: 1.9343...  6.506 sec/batch\n",
      "Epoch number: 3/5...  Step: 594...  loss: 1.9132...  6.533 sec/batch\n",
      "Epoch number: 4/5...  Step: 595...  loss: 2.0449...  6.506 sec/batch\n",
      "Epoch number: 4/5...  Step: 596...  loss: 1.9209...  6.498 sec/batch\n",
      "Epoch number: 4/5...  Step: 597...  loss: 1.9254...  6.774 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 4/5...  Step: 598...  loss: 1.9204...  7.566 sec/batch\n",
      "Epoch number: 4/5...  Step: 599...  loss: 1.9190...  7.594 sec/batch\n",
      "Epoch number: 4/5...  Step: 600...  loss: 1.8985...  8.960 sec/batch\n",
      "Epoch number: 4/5...  Step: 601...  loss: 1.9297...  7.311 sec/batch\n",
      "Epoch number: 4/5...  Step: 602...  loss: 1.9211...  6.695 sec/batch\n",
      "Epoch number: 4/5...  Step: 603...  loss: 1.9541...  6.872 sec/batch\n",
      "Epoch number: 4/5...  Step: 604...  loss: 1.9198...  7.483 sec/batch\n",
      "Epoch number: 4/5...  Step: 605...  loss: 1.9011...  7.677 sec/batch\n",
      "Epoch number: 4/5...  Step: 606...  loss: 1.8946...  6.650 sec/batch\n",
      "Epoch number: 4/5...  Step: 607...  loss: 1.9267...  6.564 sec/batch\n",
      "Epoch number: 4/5...  Step: 608...  loss: 1.9472...  6.625 sec/batch\n",
      "Epoch number: 4/5...  Step: 609...  loss: 1.9135...  6.663 sec/batch\n",
      "Epoch number: 4/5...  Step: 610...  loss: 1.8966...  6.582 sec/batch\n",
      "Epoch number: 4/5...  Step: 611...  loss: 1.9169...  6.585 sec/batch\n",
      "Epoch number: 4/5...  Step: 612...  loss: 1.9544...  7.090 sec/batch\n",
      "Epoch number: 4/5...  Step: 613...  loss: 1.9176...  6.707 sec/batch\n",
      "Epoch number: 4/5...  Step: 614...  loss: 1.9257...  6.748 sec/batch\n",
      "Epoch number: 4/5...  Step: 615...  loss: 1.9082...  6.786 sec/batch\n",
      "Epoch number: 4/5...  Step: 616...  loss: 1.9564...  6.797 sec/batch\n",
      "Epoch number: 4/5...  Step: 617...  loss: 1.9123...  6.723 sec/batch\n",
      "Epoch number: 4/5...  Step: 618...  loss: 1.9099...  6.732 sec/batch\n",
      "Epoch number: 4/5...  Step: 619...  loss: 1.9163...  6.861 sec/batch\n",
      "Epoch number: 4/5...  Step: 620...  loss: 1.8909...  6.763 sec/batch\n",
      "Epoch number: 4/5...  Step: 621...  loss: 1.8835...  6.696 sec/batch\n",
      "Epoch number: 4/5...  Step: 622...  loss: 1.9198...  6.669 sec/batch\n",
      "Epoch number: 4/5...  Step: 623...  loss: 1.9430...  6.620 sec/batch\n",
      "Epoch number: 4/5...  Step: 624...  loss: 1.9211...  6.719 sec/batch\n",
      "Epoch number: 4/5...  Step: 625...  loss: 1.9111...  6.661 sec/batch\n",
      "Epoch number: 4/5...  Step: 626...  loss: 1.8873...  6.679 sec/batch\n",
      "Epoch number: 4/5...  Step: 627...  loss: 1.9087...  6.714 sec/batch\n",
      "Epoch number: 4/5...  Step: 628...  loss: 1.9382...  6.764 sec/batch\n",
      "Epoch number: 4/5...  Step: 629...  loss: 1.8961...  6.688 sec/batch\n",
      "Epoch number: 4/5...  Step: 630...  loss: 1.9072...  6.652 sec/batch\n",
      "Epoch number: 4/5...  Step: 631...  loss: 1.8979...  6.652 sec/batch\n",
      "Epoch number: 4/5...  Step: 632...  loss: 1.8667...  6.700 sec/batch\n",
      "Epoch number: 4/5...  Step: 633...  loss: 1.8730...  6.692 sec/batch\n",
      "Epoch number: 4/5...  Step: 634...  loss: 1.8740...  6.679 sec/batch\n",
      "Epoch number: 4/5...  Step: 635...  loss: 1.8779...  6.767 sec/batch\n",
      "Epoch number: 4/5...  Step: 636...  loss: 1.9101...  7.196 sec/batch\n",
      "Epoch number: 4/5...  Step: 637...  loss: 1.8827...  7.364 sec/batch\n",
      "Epoch number: 4/5...  Step: 638...  loss: 1.8777...  7.311 sec/batch\n",
      "Epoch number: 4/5...  Step: 639...  loss: 1.9019...  6.641 sec/batch\n",
      "Epoch number: 4/5...  Step: 640...  loss: 1.8436...  6.586 sec/batch\n",
      "Epoch number: 4/5...  Step: 641...  loss: 1.8973...  6.537 sec/batch\n",
      "Epoch number: 4/5...  Step: 642...  loss: 1.8812...  6.578 sec/batch\n",
      "Epoch number: 4/5...  Step: 643...  loss: 1.8845...  6.645 sec/batch\n",
      "Epoch number: 4/5...  Step: 644...  loss: 1.9366...  6.519 sec/batch\n",
      "Epoch number: 4/5...  Step: 645...  loss: 1.8618...  6.527 sec/batch\n",
      "Epoch number: 4/5...  Step: 646...  loss: 1.9387...  6.522 sec/batch\n",
      "Epoch number: 4/5...  Step: 647...  loss: 1.8936...  6.531 sec/batch\n",
      "Epoch number: 4/5...  Step: 648...  loss: 1.8821...  6.615 sec/batch\n",
      "Epoch number: 4/5...  Step: 649...  loss: 1.8815...  6.558 sec/batch\n",
      "Epoch number: 4/5...  Step: 650...  loss: 1.9051...  6.557 sec/batch\n",
      "Epoch number: 4/5...  Step: 651...  loss: 1.8990...  6.638 sec/batch\n",
      "Epoch number: 4/5...  Step: 652...  loss: 1.8763...  6.569 sec/batch\n",
      "Epoch number: 4/5...  Step: 653...  loss: 1.8698...  6.592 sec/batch\n",
      "Epoch number: 4/5...  Step: 654...  loss: 1.9362...  6.558 sec/batch\n",
      "Epoch number: 4/5...  Step: 655...  loss: 1.8848...  6.550 sec/batch\n",
      "Epoch number: 4/5...  Step: 656...  loss: 1.9253...  6.575 sec/batch\n",
      "Epoch number: 4/5...  Step: 657...  loss: 1.9164...  6.598 sec/batch\n",
      "Epoch number: 4/5...  Step: 658...  loss: 1.8921...  6.948 sec/batch\n",
      "Epoch number: 4/5...  Step: 659...  loss: 1.8778...  6.750 sec/batch\n",
      "Epoch number: 4/5...  Step: 660...  loss: 1.9079...  6.692 sec/batch\n",
      "Epoch number: 4/5...  Step: 661...  loss: 1.8982...  6.600 sec/batch\n",
      "Epoch number: 4/5...  Step: 662...  loss: 1.8598...  6.576 sec/batch\n",
      "Epoch number: 4/5...  Step: 663...  loss: 1.8726...  6.614 sec/batch\n",
      "Epoch number: 4/5...  Step: 664...  loss: 1.8854...  6.552 sec/batch\n",
      "Epoch number: 4/5...  Step: 665...  loss: 1.9203...  6.570 sec/batch\n",
      "Epoch number: 4/5...  Step: 666...  loss: 1.8905...  6.730 sec/batch\n",
      "Epoch number: 4/5...  Step: 667...  loss: 1.9101...  6.947 sec/batch\n",
      "Epoch number: 4/5...  Step: 668...  loss: 1.8499...  7.359 sec/batch\n",
      "Epoch number: 4/5...  Step: 669...  loss: 1.8711...  6.848 sec/batch\n",
      "Epoch number: 4/5...  Step: 670...  loss: 1.9147...  6.678 sec/batch\n",
      "Epoch number: 4/5...  Step: 671...  loss: 1.8763...  6.655 sec/batch\n",
      "Epoch number: 4/5...  Step: 672...  loss: 1.8820...  6.665 sec/batch\n",
      "Epoch number: 4/5...  Step: 673...  loss: 1.8453...  7.742 sec/batch\n",
      "Epoch number: 4/5...  Step: 674...  loss: 1.8717...  6.690 sec/batch\n",
      "Epoch number: 4/5...  Step: 675...  loss: 1.8328...  6.674 sec/batch\n",
      "Epoch number: 4/5...  Step: 676...  loss: 1.8882...  6.714 sec/batch\n",
      "Epoch number: 4/5...  Step: 677...  loss: 1.8399...  6.725 sec/batch\n",
      "Epoch number: 4/5...  Step: 678...  loss: 1.8722...  6.693 sec/batch\n",
      "Epoch number: 4/5...  Step: 679...  loss: 1.8311...  6.686 sec/batch\n",
      "Epoch number: 4/5...  Step: 680...  loss: 1.8505...  6.661 sec/batch\n",
      "Epoch number: 4/5...  Step: 681...  loss: 1.8639...  6.675 sec/batch\n",
      "Epoch number: 4/5...  Step: 682...  loss: 1.8478...  6.670 sec/batch\n",
      "Epoch number: 4/5...  Step: 683...  loss: 1.8317...  6.731 sec/batch\n",
      "Epoch number: 4/5...  Step: 684...  loss: 1.8840...  6.659 sec/batch\n",
      "Epoch number: 4/5...  Step: 685...  loss: 1.8473...  6.746 sec/batch\n",
      "Epoch number: 4/5...  Step: 686...  loss: 1.8621...  6.701 sec/batch\n",
      "Epoch number: 4/5...  Step: 687...  loss: 1.8356...  6.651 sec/batch\n",
      "Epoch number: 4/5...  Step: 688...  loss: 1.8497...  6.767 sec/batch\n",
      "Epoch number: 4/5...  Step: 689...  loss: 1.8425...  6.688 sec/batch\n",
      "Epoch number: 4/5...  Step: 690...  loss: 1.8593...  6.702 sec/batch\n",
      "Epoch number: 4/5...  Step: 691...  loss: 1.8484...  6.759 sec/batch\n",
      "Epoch number: 4/5...  Step: 692...  loss: 1.8388...  6.697 sec/batch\n",
      "Epoch number: 4/5...  Step: 693...  loss: 1.8406...  6.795 sec/batch\n",
      "Epoch number: 4/5...  Step: 694...  loss: 1.8163...  6.654 sec/batch\n",
      "Epoch number: 4/5...  Step: 695...  loss: 1.8701...  6.639 sec/batch\n",
      "Epoch number: 4/5...  Step: 696...  loss: 1.8628...  6.972 sec/batch\n",
      "Epoch number: 4/5...  Step: 697...  loss: 1.8462...  6.890 sec/batch\n",
      "Epoch number: 4/5...  Step: 698...  loss: 1.8447...  6.726 sec/batch\n",
      "Epoch number: 4/5...  Step: 699...  loss: 1.8361...  6.715 sec/batch\n",
      "Epoch number: 4/5...  Step: 700...  loss: 1.8609...  6.699 sec/batch\n",
      "Epoch number: 4/5...  Step: 701...  loss: 1.8500...  6.710 sec/batch\n",
      "Epoch number: 4/5...  Step: 702...  loss: 1.8589...  6.776 sec/batch\n",
      "Epoch number: 4/5...  Step: 703...  loss: 1.8590...  6.735 sec/batch\n",
      "Epoch number: 4/5...  Step: 704...  loss: 1.8608...  6.689 sec/batch\n",
      "Epoch number: 4/5...  Step: 705...  loss: 1.8460...  6.758 sec/batch\n",
      "Epoch number: 4/5...  Step: 706...  loss: 1.8477...  6.744 sec/batch\n",
      "Epoch number: 4/5...  Step: 707...  loss: 1.8534...  6.694 sec/batch\n",
      "Epoch number: 4/5...  Step: 708...  loss: 1.8455...  6.684 sec/batch\n",
      "Epoch number: 4/5...  Step: 709...  loss: 1.8317...  6.676 sec/batch\n",
      "Epoch number: 4/5...  Step: 710...  loss: 1.8163...  6.645 sec/batch\n",
      "Epoch number: 4/5...  Step: 711...  loss: 1.8491...  6.733 sec/batch\n",
      "Epoch number: 4/5...  Step: 712...  loss: 1.8504...  6.743 sec/batch\n",
      "Epoch number: 4/5...  Step: 713...  loss: 1.8575...  6.705 sec/batch\n",
      "Epoch number: 4/5...  Step: 714...  loss: 1.8518...  6.771 sec/batch\n",
      "Epoch number: 4/5...  Step: 715...  loss: 1.8655...  6.693 sec/batch\n",
      "Epoch number: 4/5...  Step: 716...  loss: 1.8203...  6.680 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 4/5...  Step: 717...  loss: 1.8272...  6.600 sec/batch\n",
      "Epoch number: 4/5...  Step: 718...  loss: 1.8702...  6.720 sec/batch\n",
      "Epoch number: 4/5...  Step: 719...  loss: 1.8466...  6.690 sec/batch\n",
      "Epoch number: 4/5...  Step: 720...  loss: 1.8102...  6.693 sec/batch\n",
      "Epoch number: 4/5...  Step: 721...  loss: 1.8639...  6.737 sec/batch\n",
      "Epoch number: 4/5...  Step: 722...  loss: 1.8614...  6.906 sec/batch\n",
      "Epoch number: 4/5...  Step: 723...  loss: 1.8475...  6.699 sec/batch\n",
      "Epoch number: 4/5...  Step: 724...  loss: 1.8538...  6.659 sec/batch\n",
      "Epoch number: 4/5...  Step: 725...  loss: 1.8153...  7.428 sec/batch\n",
      "Epoch number: 4/5...  Step: 726...  loss: 1.8102...  6.658 sec/batch\n",
      "Epoch number: 4/5...  Step: 727...  loss: 1.8596...  6.608 sec/batch\n",
      "Epoch number: 4/5...  Step: 728...  loss: 1.8576...  6.602 sec/batch\n",
      "Epoch number: 4/5...  Step: 729...  loss: 1.8521...  6.563 sec/batch\n",
      "Epoch number: 4/5...  Step: 730...  loss: 1.8505...  7.383 sec/batch\n",
      "Epoch number: 4/5...  Step: 731...  loss: 1.8555...  7.879 sec/batch\n",
      "Epoch number: 4/5...  Step: 732...  loss: 1.8483...  8.704 sec/batch\n",
      "Epoch number: 4/5...  Step: 733...  loss: 1.8788...  6.952 sec/batch\n",
      "Epoch number: 4/5...  Step: 734...  loss: 1.8284...  6.793 sec/batch\n",
      "Epoch number: 4/5...  Step: 735...  loss: 1.8768...  6.810 sec/batch\n",
      "Epoch number: 4/5...  Step: 736...  loss: 1.8388...  7.078 sec/batch\n",
      "Epoch number: 4/5...  Step: 737...  loss: 1.8415...  7.190 sec/batch\n",
      "Epoch number: 4/5...  Step: 738...  loss: 1.8430...  7.000 sec/batch\n",
      "Epoch number: 4/5...  Step: 739...  loss: 1.8204...  6.879 sec/batch\n",
      "Epoch number: 4/5...  Step: 740...  loss: 1.8640...  6.822 sec/batch\n",
      "Epoch number: 4/5...  Step: 741...  loss: 1.8547...  6.824 sec/batch\n",
      "Epoch number: 4/5...  Step: 742...  loss: 1.8739...  6.817 sec/batch\n",
      "Epoch number: 4/5...  Step: 743...  loss: 1.8544...  6.964 sec/batch\n",
      "Epoch number: 4/5...  Step: 744...  loss: 1.8331...  6.837 sec/batch\n",
      "Epoch number: 4/5...  Step: 745...  loss: 1.8128...  6.834 sec/batch\n",
      "Epoch number: 4/5...  Step: 746...  loss: 1.8595...  7.371 sec/batch\n",
      "Epoch number: 4/5...  Step: 747...  loss: 1.8468...  6.767 sec/batch\n",
      "Epoch number: 4/5...  Step: 748...  loss: 1.8501...  6.783 sec/batch\n",
      "Epoch number: 4/5...  Step: 749...  loss: 1.8359...  6.880 sec/batch\n",
      "Epoch number: 4/5...  Step: 750...  loss: 1.8386...  6.701 sec/batch\n",
      "Epoch number: 4/5...  Step: 751...  loss: 1.8376...  6.698 sec/batch\n",
      "Epoch number: 4/5...  Step: 752...  loss: 1.8333...  6.682 sec/batch\n",
      "Epoch number: 4/5...  Step: 753...  loss: 1.7994...  7.459 sec/batch\n",
      "Epoch number: 4/5...  Step: 754...  loss: 1.8753...  6.897 sec/batch\n",
      "Epoch number: 4/5...  Step: 755...  loss: 1.8529...  6.784 sec/batch\n",
      "Epoch number: 4/5...  Step: 756...  loss: 1.8300...  6.917 sec/batch\n",
      "Epoch number: 4/5...  Step: 757...  loss: 1.8449...  7.261 sec/batch\n",
      "Epoch number: 4/5...  Step: 758...  loss: 1.8377...  6.763 sec/batch\n",
      "Epoch number: 4/5...  Step: 759...  loss: 1.8342...  6.736 sec/batch\n",
      "Epoch number: 4/5...  Step: 760...  loss: 1.8226...  6.803 sec/batch\n",
      "Epoch number: 4/5...  Step: 761...  loss: 1.8482...  6.717 sec/batch\n",
      "Epoch number: 4/5...  Step: 762...  loss: 1.8752...  6.662 sec/batch\n",
      "Epoch number: 4/5...  Step: 763...  loss: 1.8245...  6.702 sec/batch\n",
      "Epoch number: 4/5...  Step: 764...  loss: 1.8256...  6.780 sec/batch\n",
      "Epoch number: 4/5...  Step: 765...  loss: 1.8347...  6.787 sec/batch\n",
      "Epoch number: 4/5...  Step: 766...  loss: 1.8438...  6.845 sec/batch\n",
      "Epoch number: 4/5...  Step: 767...  loss: 1.8733...  6.882 sec/batch\n",
      "Epoch number: 4/5...  Step: 768...  loss: 1.8523...  6.758 sec/batch\n",
      "Epoch number: 4/5...  Step: 769...  loss: 1.8563...  6.768 sec/batch\n",
      "Epoch number: 4/5...  Step: 770...  loss: 1.8311...  6.757 sec/batch\n",
      "Epoch number: 4/5...  Step: 771...  loss: 1.8070...  6.750 sec/batch\n",
      "Epoch number: 4/5...  Step: 772...  loss: 1.8477...  6.793 sec/batch\n",
      "Epoch number: 4/5...  Step: 773...  loss: 1.8167...  6.839 sec/batch\n",
      "Epoch number: 4/5...  Step: 774...  loss: 1.7985...  6.718 sec/batch\n",
      "Epoch number: 4/5...  Step: 775...  loss: 1.7992...  7.303 sec/batch\n",
      "Epoch number: 4/5...  Step: 776...  loss: 1.8254...  8.104 sec/batch\n",
      "Epoch number: 4/5...  Step: 777...  loss: 1.8276...  7.296 sec/batch\n",
      "Epoch number: 4/5...  Step: 778...  loss: 1.8454...  7.933 sec/batch\n",
      "Epoch number: 4/5...  Step: 779...  loss: 1.8327...  8.028 sec/batch\n",
      "Epoch number: 4/5...  Step: 780...  loss: 1.8109...  8.933 sec/batch\n",
      "Epoch number: 4/5...  Step: 781...  loss: 1.8313...  7.888 sec/batch\n",
      "Epoch number: 4/5...  Step: 782...  loss: 1.8000...  7.208 sec/batch\n",
      "Epoch number: 4/5...  Step: 783...  loss: 1.8063...  6.973 sec/batch\n",
      "Epoch number: 4/5...  Step: 784...  loss: 1.8230...  8.935 sec/batch\n",
      "Epoch number: 4/5...  Step: 785...  loss: 1.8232...  10.007 sec/batch\n",
      "Epoch number: 4/5...  Step: 786...  loss: 1.8023...  7.324 sec/batch\n",
      "Epoch number: 4/5...  Step: 787...  loss: 1.8310...  6.863 sec/batch\n",
      "Epoch number: 4/5...  Step: 788...  loss: 1.8178...  6.927 sec/batch\n",
      "Epoch number: 4/5...  Step: 789...  loss: 1.8059...  7.179 sec/batch\n",
      "Epoch number: 4/5...  Step: 790...  loss: 1.8193...  6.599 sec/batch\n",
      "Epoch number: 4/5...  Step: 791...  loss: 1.8127...  6.644 sec/batch\n",
      "Epoch number: 4/5...  Step: 792...  loss: 1.8065...  6.744 sec/batch\n",
      "Epoch number: 5/5...  Step: 793...  loss: 1.9270...  6.544 sec/batch\n",
      "Epoch number: 5/5...  Step: 794...  loss: 1.8137...  6.702 sec/batch\n",
      "Epoch number: 5/5...  Step: 795...  loss: 1.8041...  6.681 sec/batch\n",
      "Epoch number: 5/5...  Step: 796...  loss: 1.8101...  6.583 sec/batch\n",
      "Epoch number: 5/5...  Step: 797...  loss: 1.8022...  7.467 sec/batch\n",
      "Epoch number: 5/5...  Step: 798...  loss: 1.7751...  7.142 sec/batch\n",
      "Epoch number: 5/5...  Step: 799...  loss: 1.8146...  6.714 sec/batch\n",
      "Epoch number: 5/5...  Step: 800...  loss: 1.8001...  7.041 sec/batch\n",
      "Epoch number: 5/5...  Step: 801...  loss: 1.8397...  7.012 sec/batch\n",
      "Epoch number: 5/5...  Step: 802...  loss: 1.8052...  6.818 sec/batch\n",
      "Epoch number: 5/5...  Step: 803...  loss: 1.7869...  6.711 sec/batch\n",
      "Epoch number: 5/5...  Step: 804...  loss: 1.7735...  6.697 sec/batch\n",
      "Epoch number: 5/5...  Step: 805...  loss: 1.8083...  6.651 sec/batch\n",
      "Epoch number: 5/5...  Step: 806...  loss: 1.8353...  6.696 sec/batch\n",
      "Epoch number: 5/5...  Step: 807...  loss: 1.7906...  6.857 sec/batch\n",
      "Epoch number: 5/5...  Step: 808...  loss: 1.7766...  6.762 sec/batch\n",
      "Epoch number: 5/5...  Step: 809...  loss: 1.8067...  6.720 sec/batch\n",
      "Epoch number: 5/5...  Step: 810...  loss: 1.8363...  6.831 sec/batch\n",
      "Epoch number: 5/5...  Step: 811...  loss: 1.8004...  6.726 sec/batch\n",
      "Epoch number: 5/5...  Step: 812...  loss: 1.8140...  6.658 sec/batch\n",
      "Epoch number: 5/5...  Step: 813...  loss: 1.7909...  6.665 sec/batch\n",
      "Epoch number: 5/5...  Step: 814...  loss: 1.8278...  6.693 sec/batch\n",
      "Epoch number: 5/5...  Step: 815...  loss: 1.7966...  6.734 sec/batch\n",
      "Epoch number: 5/5...  Step: 816...  loss: 1.7953...  6.702 sec/batch\n",
      "Epoch number: 5/5...  Step: 817...  loss: 1.8000...  6.709 sec/batch\n",
      "Epoch number: 5/5...  Step: 818...  loss: 1.7685...  6.840 sec/batch\n",
      "Epoch number: 5/5...  Step: 819...  loss: 1.7645...  6.909 sec/batch\n",
      "Epoch number: 5/5...  Step: 820...  loss: 1.8017...  7.254 sec/batch\n",
      "Epoch number: 5/5...  Step: 821...  loss: 1.8288...  7.104 sec/batch\n",
      "Epoch number: 5/5...  Step: 822...  loss: 1.8092...  6.926 sec/batch\n",
      "Epoch number: 5/5...  Step: 823...  loss: 1.7959...  6.937 sec/batch\n",
      "Epoch number: 5/5...  Step: 824...  loss: 1.7669...  6.587 sec/batch\n",
      "Epoch number: 5/5...  Step: 825...  loss: 1.8017...  6.637 sec/batch\n",
      "Epoch number: 5/5...  Step: 826...  loss: 1.8241...  6.540 sec/batch\n",
      "Epoch number: 5/5...  Step: 827...  loss: 1.7779...  7.010 sec/batch\n",
      "Epoch number: 5/5...  Step: 828...  loss: 1.7915...  7.399 sec/batch\n",
      "Epoch number: 5/5...  Step: 829...  loss: 1.7808...  6.716 sec/batch\n",
      "Epoch number: 5/5...  Step: 830...  loss: 1.7499...  6.743 sec/batch\n",
      "Epoch number: 5/5...  Step: 831...  loss: 1.7539...  6.707 sec/batch\n",
      "Epoch number: 5/5...  Step: 832...  loss: 1.7546...  6.754 sec/batch\n",
      "Epoch number: 5/5...  Step: 833...  loss: 1.7608...  6.684 sec/batch\n",
      "Epoch number: 5/5...  Step: 834...  loss: 1.7993...  6.680 sec/batch\n",
      "Epoch number: 5/5...  Step: 835...  loss: 1.7666...  6.630 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 5/5...  Step: 836...  loss: 1.7581...  6.742 sec/batch\n",
      "Epoch number: 5/5...  Step: 837...  loss: 1.7918...  6.778 sec/batch\n",
      "Epoch number: 5/5...  Step: 838...  loss: 1.7276...  6.673 sec/batch\n",
      "Epoch number: 5/5...  Step: 839...  loss: 1.7767...  6.768 sec/batch\n",
      "Epoch number: 5/5...  Step: 840...  loss: 1.7590...  6.705 sec/batch\n",
      "Epoch number: 5/5...  Step: 841...  loss: 1.7686...  6.861 sec/batch\n",
      "Epoch number: 5/5...  Step: 842...  loss: 1.8202...  7.449 sec/batch\n",
      "Epoch number: 5/5...  Step: 843...  loss: 1.7453...  6.766 sec/batch\n",
      "Epoch number: 5/5...  Step: 844...  loss: 1.8302...  6.901 sec/batch\n",
      "Epoch number: 5/5...  Step: 845...  loss: 1.7795...  6.577 sec/batch\n",
      "Epoch number: 5/5...  Step: 846...  loss: 1.7739...  6.641 sec/batch\n",
      "Epoch number: 5/5...  Step: 847...  loss: 1.7686...  6.604 sec/batch\n",
      "Epoch number: 5/5...  Step: 848...  loss: 1.7903...  6.678 sec/batch\n",
      "Epoch number: 5/5...  Step: 849...  loss: 1.7909...  6.605 sec/batch\n",
      "Epoch number: 5/5...  Step: 850...  loss: 1.7660...  6.598 sec/batch\n",
      "Epoch number: 5/5...  Step: 851...  loss: 1.7543...  6.760 sec/batch\n",
      "Epoch number: 5/5...  Step: 852...  loss: 1.8181...  7.354 sec/batch\n",
      "Epoch number: 5/5...  Step: 853...  loss: 1.7738...  6.578 sec/batch\n",
      "Epoch number: 5/5...  Step: 854...  loss: 1.8238...  6.515 sec/batch\n",
      "Epoch number: 5/5...  Step: 855...  loss: 1.8030...  6.597 sec/batch\n",
      "Epoch number: 5/5...  Step: 856...  loss: 1.7903...  6.647 sec/batch\n",
      "Epoch number: 5/5...  Step: 857...  loss: 1.7659...  6.653 sec/batch\n",
      "Epoch number: 5/5...  Step: 858...  loss: 1.7954...  6.618 sec/batch\n",
      "Epoch number: 5/5...  Step: 859...  loss: 1.7929...  6.543 sec/batch\n",
      "Epoch number: 5/5...  Step: 860...  loss: 1.7588...  6.517 sec/batch\n",
      "Epoch number: 5/5...  Step: 861...  loss: 1.7691...  6.574 sec/batch\n",
      "Epoch number: 5/5...  Step: 862...  loss: 1.7754...  6.617 sec/batch\n",
      "Epoch number: 5/5...  Step: 863...  loss: 1.8167...  6.579 sec/batch\n",
      "Epoch number: 5/5...  Step: 864...  loss: 1.7864...  6.524 sec/batch\n",
      "Epoch number: 5/5...  Step: 865...  loss: 1.8026...  6.578 sec/batch\n",
      "Epoch number: 5/5...  Step: 866...  loss: 1.7415...  6.522 sec/batch\n",
      "Epoch number: 5/5...  Step: 867...  loss: 1.7595...  6.539 sec/batch\n",
      "Epoch number: 5/5...  Step: 868...  loss: 1.7986...  6.599 sec/batch\n",
      "Epoch number: 5/5...  Step: 869...  loss: 1.7670...  6.551 sec/batch\n",
      "Epoch number: 5/5...  Step: 870...  loss: 1.7732...  6.647 sec/batch\n",
      "Epoch number: 5/5...  Step: 871...  loss: 1.7345...  6.599 sec/batch\n",
      "Epoch number: 5/5...  Step: 872...  loss: 1.7650...  6.596 sec/batch\n",
      "Epoch number: 5/5...  Step: 873...  loss: 1.7272...  6.970 sec/batch\n",
      "Epoch number: 5/5...  Step: 874...  loss: 1.7817...  6.525 sec/batch\n",
      "Epoch number: 5/5...  Step: 875...  loss: 1.7258...  6.617 sec/batch\n",
      "Epoch number: 5/5...  Step: 876...  loss: 1.7673...  6.546 sec/batch\n",
      "Epoch number: 5/5...  Step: 877...  loss: 1.7285...  6.536 sec/batch\n",
      "Epoch number: 5/5...  Step: 878...  loss: 1.7465...  6.584 sec/batch\n",
      "Epoch number: 5/5...  Step: 879...  loss: 1.7505...  6.563 sec/batch\n",
      "Epoch number: 5/5...  Step: 880...  loss: 1.7358...  6.501 sec/batch\n",
      "Epoch number: 5/5...  Step: 881...  loss: 1.7245...  6.557 sec/batch\n",
      "Epoch number: 5/5...  Step: 882...  loss: 1.7804...  6.536 sec/batch\n",
      "Epoch number: 5/5...  Step: 883...  loss: 1.7375...  6.509 sec/batch\n",
      "Epoch number: 5/5...  Step: 884...  loss: 1.7521...  6.549 sec/batch\n",
      "Epoch number: 5/5...  Step: 885...  loss: 1.7353...  6.764 sec/batch\n",
      "Epoch number: 5/5...  Step: 886...  loss: 1.7416...  6.834 sec/batch\n",
      "Epoch number: 5/5...  Step: 887...  loss: 1.7372...  6.823 sec/batch\n",
      "Epoch number: 5/5...  Step: 888...  loss: 1.7584...  6.640 sec/batch\n",
      "Epoch number: 5/5...  Step: 889...  loss: 1.7473...  6.576 sec/batch\n",
      "Epoch number: 5/5...  Step: 890...  loss: 1.7236...  7.873 sec/batch\n",
      "Epoch number: 5/5...  Step: 891...  loss: 1.7363...  7.255 sec/batch\n",
      "Epoch number: 5/5...  Step: 892...  loss: 1.7105...  6.814 sec/batch\n",
      "Epoch number: 5/5...  Step: 893...  loss: 1.7619...  6.757 sec/batch\n",
      "Epoch number: 5/5...  Step: 894...  loss: 1.7527...  6.732 sec/batch\n",
      "Epoch number: 5/5...  Step: 895...  loss: 1.7385...  6.719 sec/batch\n",
      "Epoch number: 5/5...  Step: 896...  loss: 1.7427...  6.672 sec/batch\n",
      "Epoch number: 5/5...  Step: 897...  loss: 1.7339...  6.833 sec/batch\n",
      "Epoch number: 5/5...  Step: 898...  loss: 1.7471...  7.551 sec/batch\n",
      "Epoch number: 5/5...  Step: 899...  loss: 1.7486...  6.817 sec/batch\n",
      "Epoch number: 5/5...  Step: 900...  loss: 1.7503...  6.755 sec/batch\n",
      "Epoch number: 5/5...  Step: 901...  loss: 1.7534...  6.734 sec/batch\n",
      "Epoch number: 5/5...  Step: 902...  loss: 1.7635...  6.663 sec/batch\n",
      "Epoch number: 5/5...  Step: 903...  loss: 1.7423...  6.660 sec/batch\n",
      "Epoch number: 5/5...  Step: 904...  loss: 1.7486...  6.744 sec/batch\n",
      "Epoch number: 5/5...  Step: 905...  loss: 1.7460...  6.758 sec/batch\n",
      "Epoch number: 5/5...  Step: 906...  loss: 1.7345...  6.948 sec/batch\n",
      "Epoch number: 5/5...  Step: 907...  loss: 1.7293...  6.885 sec/batch\n",
      "Epoch number: 5/5...  Step: 908...  loss: 1.7122...  6.715 sec/batch\n",
      "Epoch number: 5/5...  Step: 909...  loss: 1.7471...  6.691 sec/batch\n",
      "Epoch number: 5/5...  Step: 910...  loss: 1.7414...  6.727 sec/batch\n",
      "Epoch number: 5/5...  Step: 911...  loss: 1.7447...  6.747 sec/batch\n",
      "Epoch number: 5/5...  Step: 912...  loss: 1.7406...  6.716 sec/batch\n",
      "Epoch number: 5/5...  Step: 913...  loss: 1.7534...  6.754 sec/batch\n",
      "Epoch number: 5/5...  Step: 914...  loss: 1.7192...  6.855 sec/batch\n",
      "Epoch number: 5/5...  Step: 915...  loss: 1.7128...  6.759 sec/batch\n",
      "Epoch number: 5/5...  Step: 916...  loss: 1.7632...  7.110 sec/batch\n",
      "Epoch number: 5/5...  Step: 917...  loss: 1.7401...  7.991 sec/batch\n",
      "Epoch number: 5/5...  Step: 918...  loss: 1.7008...  7.458 sec/batch\n",
      "Epoch number: 5/5...  Step: 919...  loss: 1.7632...  6.986 sec/batch\n",
      "Epoch number: 5/5...  Step: 920...  loss: 1.7607...  7.691 sec/batch\n",
      "Epoch number: 5/5...  Step: 921...  loss: 1.7439...  6.693 sec/batch\n",
      "Epoch number: 5/5...  Step: 922...  loss: 1.7414...  6.719 sec/batch\n",
      "Epoch number: 5/5...  Step: 923...  loss: 1.7103...  7.130 sec/batch\n",
      "Epoch number: 5/5...  Step: 924...  loss: 1.7187...  7.160 sec/batch\n",
      "Epoch number: 5/5...  Step: 925...  loss: 1.7546...  9.032 sec/batch\n",
      "Epoch number: 5/5...  Step: 926...  loss: 1.7510...  7.940 sec/batch\n",
      "Epoch number: 5/5...  Step: 927...  loss: 1.7571...  6.765 sec/batch\n",
      "Epoch number: 5/5...  Step: 928...  loss: 1.7392...  6.571 sec/batch\n",
      "Epoch number: 5/5...  Step: 929...  loss: 1.7566...  6.546 sec/batch\n",
      "Epoch number: 5/5...  Step: 930...  loss: 1.7462...  6.632 sec/batch\n",
      "Epoch number: 5/5...  Step: 931...  loss: 1.7663...  6.589 sec/batch\n",
      "Epoch number: 5/5...  Step: 932...  loss: 1.7306...  6.840 sec/batch\n",
      "Epoch number: 5/5...  Step: 933...  loss: 1.7865...  7.054 sec/batch\n",
      "Epoch number: 5/5...  Step: 934...  loss: 1.7400...  7.451 sec/batch\n",
      "Epoch number: 5/5...  Step: 935...  loss: 1.7464...  6.791 sec/batch\n",
      "Epoch number: 5/5...  Step: 936...  loss: 1.7496...  6.832 sec/batch\n",
      "Epoch number: 5/5...  Step: 937...  loss: 1.7257...  6.755 sec/batch\n",
      "Epoch number: 5/5...  Step: 938...  loss: 1.7619...  6.719 sec/batch\n",
      "Epoch number: 5/5...  Step: 939...  loss: 1.7502...  6.752 sec/batch\n",
      "Epoch number: 5/5...  Step: 940...  loss: 1.7773...  6.784 sec/batch\n",
      "Epoch number: 5/5...  Step: 941...  loss: 1.7525...  6.811 sec/batch\n",
      "Epoch number: 5/5...  Step: 942...  loss: 1.7279...  6.773 sec/batch\n",
      "Epoch number: 5/5...  Step: 943...  loss: 1.7079...  6.877 sec/batch\n",
      "Epoch number: 5/5...  Step: 944...  loss: 1.7518...  6.714 sec/batch\n",
      "Epoch number: 5/5...  Step: 945...  loss: 1.7412...  6.720 sec/batch\n",
      "Epoch number: 5/5...  Step: 946...  loss: 1.7464...  6.839 sec/batch\n",
      "Epoch number: 5/5...  Step: 947...  loss: 1.7339...  6.774 sec/batch\n",
      "Epoch number: 5/5...  Step: 948...  loss: 1.7311...  6.799 sec/batch\n",
      "Epoch number: 5/5...  Step: 949...  loss: 1.7428...  6.755 sec/batch\n",
      "Epoch number: 5/5...  Step: 950...  loss: 1.7321...  6.774 sec/batch\n",
      "Epoch number: 5/5...  Step: 951...  loss: 1.7015...  6.623 sec/batch\n",
      "Epoch number: 5/5...  Step: 952...  loss: 1.7665...  6.739 sec/batch\n",
      "Epoch number: 5/5...  Step: 953...  loss: 1.7525...  6.785 sec/batch\n",
      "Epoch number: 5/5...  Step: 954...  loss: 1.7303...  6.728 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 5/5...  Step: 955...  loss: 1.7464...  6.831 sec/batch\n",
      "Epoch number: 5/5...  Step: 956...  loss: 1.7386...  6.845 sec/batch\n",
      "Epoch number: 5/5...  Step: 957...  loss: 1.7323...  6.662 sec/batch\n",
      "Epoch number: 5/5...  Step: 958...  loss: 1.7249...  6.675 sec/batch\n",
      "Epoch number: 5/5...  Step: 959...  loss: 1.7473...  6.753 sec/batch\n",
      "Epoch number: 5/5...  Step: 960...  loss: 1.7893...  6.778 sec/batch\n",
      "Epoch number: 5/5...  Step: 961...  loss: 1.7229...  6.867 sec/batch\n",
      "Epoch number: 5/5...  Step: 962...  loss: 1.7273...  6.781 sec/batch\n",
      "Epoch number: 5/5...  Step: 963...  loss: 1.7156...  6.775 sec/batch\n",
      "Epoch number: 5/5...  Step: 964...  loss: 1.7159...  6.697 sec/batch\n",
      "Epoch number: 5/5...  Step: 965...  loss: 1.7508...  6.746 sec/batch\n",
      "Epoch number: 5/5...  Step: 966...  loss: 1.7287...  6.941 sec/batch\n",
      "Epoch number: 5/5...  Step: 967...  loss: 1.7409...  6.923 sec/batch\n",
      "Epoch number: 5/5...  Step: 968...  loss: 1.7251...  6.853 sec/batch\n",
      "Epoch number: 5/5...  Step: 969...  loss: 1.7042...  6.789 sec/batch\n",
      "Epoch number: 5/5...  Step: 970...  loss: 1.7471...  6.733 sec/batch\n",
      "Epoch number: 5/5...  Step: 971...  loss: 1.7059...  6.734 sec/batch\n",
      "Epoch number: 5/5...  Step: 972...  loss: 1.6928...  6.685 sec/batch\n",
      "Epoch number: 5/5...  Step: 973...  loss: 1.6926...  6.744 sec/batch\n",
      "Epoch number: 5/5...  Step: 974...  loss: 1.7152...  6.829 sec/batch\n",
      "Epoch number: 5/5...  Step: 975...  loss: 1.7217...  6.763 sec/batch\n",
      "Epoch number: 5/5...  Step: 976...  loss: 1.7451...  6.753 sec/batch\n",
      "Epoch number: 5/5...  Step: 977...  loss: 1.7181...  6.718 sec/batch\n",
      "Epoch number: 5/5...  Step: 978...  loss: 1.7681...  6.703 sec/batch\n",
      "Epoch number: 5/5...  Step: 979...  loss: 1.8083...  6.681 sec/batch\n",
      "Epoch number: 5/5...  Step: 980...  loss: 1.7851...  6.813 sec/batch\n",
      "Epoch number: 5/5...  Step: 981...  loss: 1.7780...  6.793 sec/batch\n",
      "Epoch number: 5/5...  Step: 982...  loss: 1.7918...  6.821 sec/batch\n",
      "Epoch number: 5/5...  Step: 983...  loss: 1.7691...  6.753 sec/batch\n",
      "Epoch number: 5/5...  Step: 984...  loss: 1.7543...  6.854 sec/batch\n",
      "Epoch number: 5/5...  Step: 985...  loss: 1.7740...  6.854 sec/batch\n",
      "Epoch number: 5/5...  Step: 986...  loss: 1.7445...  6.712 sec/batch\n",
      "Epoch number: 5/5...  Step: 987...  loss: 1.7425...  6.720 sec/batch\n",
      "Epoch number: 5/5...  Step: 988...  loss: 1.7485...  6.758 sec/batch\n",
      "Epoch number: 5/5...  Step: 989...  loss: 1.7455...  6.894 sec/batch\n",
      "Epoch number: 5/5...  Step: 990...  loss: 1.7326...  6.744 sec/batch\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    # saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network  训练网络\n",
    "        new_state = sess.run(LSTM_model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in generate_character_batches(encoded_vocab, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {LSTM_model.inputs: x,\n",
    "                    LSTM_model.targets: y,\n",
    "                    LSTM_model.keep_prob: keep_probability,\n",
    "                    LSTM_model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([LSTM_model.loss,\n",
    "                                                 LSTM_model.final_state,\n",
    "                                                 LSTM_model.optimizer],\n",
    "                                                feed_dict=feed)\n",
    "\n",
    "            end = time.time()\n",
    "            print('Epoch number: {}/{}... '.format(e + 1, epochs),\n",
    "                  'Step: {}... '.format(counter),\n",
    "                  'loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.3f} sec/batch'.format((end - start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done...\n"
     ]
    }
   ],
   "source": [
    "print('Training is done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining helper functions for sampling from the network  定义函数从网络中抽样\n",
    "def pick_top_n_characters(preds, vocab_size, top_n_chars=4):  #从预测结果中选取前4个最有可能的字符\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n_chars]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]  #随机选取一个字符\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成新文本\n",
    "def sample_from_LSTM_output(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    LSTM_model = CharLSTM(len(language_vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(LSTM_model.initial_state)\n",
    "        for char in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab_to_integer[char]\n",
    "            feed = {LSTM_model.inputs: x,\n",
    "                    LSTM_model.keep_prob: 1.,\n",
    "                    LSTM_model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([LSTM_model.prediction, LSTM_model.final_state],\n",
    "                                        feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n_characters(preds, len(language_vocab))\n",
    "        samples.append(integer_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0, 0] = c\n",
    "            feed = {LSTM_model.inputs: x,\n",
    "                    LSTM_model.keep_prob: 1.,\n",
    "                    LSTM_model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([LSTM_model.prediction, LSTM_model.final_state],\n",
    "                                        feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n_characters(preds, len(language_vocab))\n",
    "            samples.append(integer_to_vocab[c])\n",
    "\n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest checkpoint..\n"
     ]
    }
   ],
   "source": [
    "print('Loading latest checkpoint..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling text frm the trained model....\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\i990_l512.ckpt\n",
      "Fary.\n",
      "\n",
      "A had st pring that he west it that in a shise. He's dind a the seare,\" she tadky to her hims lantence was the counten the steation, shisenting, the sare with stope her hand to heard the race, and thit sen therred whe had been some then thought that tho gand hores,\n",
      "sha dand to\n",
      "ser him, be to say that her\n",
      "himself cound to anst to the sand. She had bo gead hem and her homen welk of serthire while he sand ang to hor samp he with anghen the planct of her and the reations war hes lenghtion was ther heade she was to bo himent over her tanked to her ho donce the\n",
      "shantsors of a to the ran of\n",
      "the sempers the she whot he was ant the reat thit wal so cherd and the clook to bet his sood and to\n",
      "dearity, when he sould, as he sace the began ta thore was sond when see so herr and the righines, and that thinkieg time her and to\n",
      "the ringer and the rood. She had to be that he was shend was not and\n",
      "sond, all agais, but they said\n",
      "that she would have to be senden that the cortors that to this than he had now to\n",
      "be thene, and this in thene was not oncerved haster to the west to both than his heard his handers at the reantion, sto lead a athat of that was to simp asting an the way,\" said the stear that he was sand whe had seed to\n",
      "tike thas then she west oven she what to\n",
      "stomp her head hes tood at to be semeling ta tele he samp his somply anding the stresses and the cancoptor of at the sampon of her he doung ste there thonge the shard, wat she his dinter there whe wan sto thoughter when she had nee ded a monden of the reat that he shattered to her aster him ta tere, who had sand to\n",
      "teen of she\n",
      "dang with the warking thas he chaldes the prences what say\n",
      "thay was stronge that she hes and the chender wands onth a sigling ano the cain sacestof\n",
      "listles and the stople said to her sute the wish and as the pronces of the chartess of all the wond of her as the came to\n",
      "she starding, that seed a stold.\n",
      "\n",
      "\"Why,\n",
      "shan shild so hear ther in to to ho her as this count there was the coust and still,\" said\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#选用最终的训练参数进行文本生成\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "print('Sampling text frm the trained model....')\n",
    "sampled_text = sample_from_LSTM_output(checkpoint, 2000, lstm_size, len(language_vocab), prime=\"Far\")\n",
    "print(sampled_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
