# Chapter 10    循环神经网络RNN——语言建模
&emsp;&emsp;循环神经网络（RNNs）是一类广泛用于自然语言处理（NLP）的深度学习体系结构，这套体系结构使我们能够为当前预测提供语境信息，并且具有特定的体系结构能够处理任一输入序列中的长时依赖关系。在本章中，我们将介绍如何建立序列到序列（seq2seq）的模型，seq2seq在自然语言处理（NLP）的应用中将会非常有用。我们将通过构建字符级语言模型来介绍RNN概念，并了解我们的模型是如何生成类似于原始输入序列的句子。<br>
#### 本章将包含以下三个方面：
*   RNNs的本质
*   LSTM（Long Short Term Memory长短期记忆网络）网络
*   语言模型的实现
## RNNs的本质
&emsp;&emsp;到目前为止，我们学习并运用的深度学习体系结构都没有任何机制能够记忆之前输入的序列或字符的信息。例如，前馈神经网络（FNN），如果输入一串字符序列HELLO，当神经网络接收到E，你会发现它忘记了刚读取过H，更没有保存任何关于H的信息，。这对基于序列的学习而言是非常严重的问题。因为它对以前读取的字符没有记忆，因此运用这类神经网络去训练预测下一个字符是非常困难的。同时这对于语言模型、机器翻译、语音识别等的应用都没有意义。<br>
&emsp;&emsp;由于这个特殊的原因，我们引入了RNNs，这是一组深度学习体系结构，它能够保存信息并记忆刚接收到的信息。<br>
&emsp;&emsp;接下来演示RNNs应该如何处理相同的输入的字符序列，HELLO。当RNNs记忆单元接收到E作为输入字符，它同时也会接收到字符H，字符H比字符E接收的更早。这种把当前的字符以及之前的字符作为对RNN细胞元的输入时，对这种体系结构提供了很大的优势，即短期记忆；在这个特定的字符序列中，它还能使这些体系结构可用于预测或猜测H之后最可能出现的字符，即E。<br>
&emsp;&emsp;我们已经了解到，以前的体系结构为其输入分配权重；RNNs遵循相同的优化过程，为其多个输入分配权重，即当前的输入和过去的输入。所以在这个案例中，神经网络将会对当输入前和上一时刻的输出作为这一时刻的输入分配两个不同的权重矩阵。为了做到这一点，我们将使用梯度下降和重配比的反向传播，即基于时间的反向传播算法（BPTT）。<br>
## 循环神经网络体系结构
&emsp;&emsp;基于我们之前使用的深度学习体系结构的背景，你会发现RNNs的特别之处。我们之前学习的结构体系在输入或训练方面并不灵活。这些结构体系接收固定大小的序列、向量、图像作为输入并产生另一个固定大小的序列、向量、图像作为输出。RNN体系结构在某种程度上是不同的，因为它可以输入一个序列但输出另一个序列，或者如图一所示，输入序列但是单输出，或单输入但是输出为序列。这种灵活性对于如语言建模和情绪分析的多种应用程序非常有用:<br>
![image](https://github.com/yanjiusheng2018/dlt/blob/master/src/content/Chapter10/chapter10_image/%E5%9B%BE1.jpg)图1：在输入或输出形状方面RNNs的灵活性<br>
&emsp;&emsp;这类体系结构的本质是模仿人类处理信息的方式。在任何谈话过程中，你对对方话语的理解完全取决于他之前所讲的话，你甚至可以根据对方刚才讲的话预测他接下来会将什么。<br>
&emsp;&emsp;RNN在运用过程中也应该遵循完全相同的过程。例如，假设你想要翻译某一个句子中的一个特定的单词。你不会使用传统的前馈神经网络，因为传统的神经网络没有将之前接收到的单词的翻译的输出作为我们想要翻译的当前单词的输入的能力，并且也会因为缺少单词的上下文的信息而导致翻译错误。<br>
&emsp;&emsp;RNNs保留过去的信息，并具有某种循环方式，允许在任何给定的点上使用之前学习到的信息进行当前预测：<br>
![image](https://github.com/yanjiusheng2018/dlt/blob/master/src/content/Chapter10/chapter10_image/%E5%9B%BE2.jpg)图2：RNNs体系结构具有保留过去步骤的信息的循环<br>
&emsp;&emsp;在图2中，A是接收X(t)作为输入的一些神经网络，并产生和输出h(t)。此外，在这个循环的辅助下接收前一个步骤的信息。<br>
&emsp;&emsp;这个循环看上去不是那么清晰，但是如果我们把循环展开，如图2所示，你会发现循环非常简单和直观，RNN只不过是同一个网络（可能是普通FNN）的重复，如图3所示：<br>
![image](https://github.com/yanjiusheng2018/dlt/blob/master/src/content/Chapter10/chapter10_image/%E5%9B%BE3.jpg)图3：RNN体系结构展开图<br>
&emsp;&emsp;RNNs这种直观的结构及其在输入输出形状方面的灵活性使其非常适用于基于序列的学习任务，例如机器翻译、语言建模、情绪分析、图像字幕等。<br>
## RNNs的案例
&emsp;&emsp;现在，我们对RNNs的工作原理以及它在不同的基于序列的示例中的有用性有了直观的了解。让我们进一步了解其中一些有趣的例子。<br>
### 字符级语言模型
&emsp;&emsp;语言建模是语音识别、机器翻译等许多应用的一项必要且重要的任务。在本节中，我们尝试模拟RNN的训练过程，并深入了解这些网络的工作原理。我们将建立一个对字符进行操作的语言模型。因此，我们将一堆文本信息作为对神经网络的输入，目的是试图建立下一个字符的概率分布，因为前面输出的字符将允许我们生成与在培训过程中作为输入提供的文本相似的文本。<br>
&emsp;&emsp;例如，假设我们有一种语言，它只有四个字母作为其词汇，helo。<br>
&emsp;&emsp;我们要做的任务是输入特定的字符序列（如HELLO）来训练循环神经网络。在这个特殊示例中，我们有四个训练样本：<br>
&emsp;&emsp;1.根据输入的第一个字符h的上下文计算字符e的概率，<br>
&emsp;&emsp;2.根据给定的he的上下文计算字符l的概率，<br>
&emsp;&emsp;3.根据给定的hel的上下文计算字符l的概率，<br>
&emsp;&emsp;4.最后根据给定的hell的上下文计算字符0的概率。<br>
&emsp;&emsp;正如我们前几章学到的，机器学习技术通常是深度学习的一部分，只接受实值数字作为输入。所以，我们需要以某种方式转换或编码或输入字符为数字形式。为此，我们使用独热（one-hot）向量编码，这是一种通过具有零向量的方法对文本进行编码, 向量中除了词汇中字符的索引是1，其余位置均是0（本例词汇helo）。在对训练样本进行编码后，我们将一次性把编码后的训练样本输入到RNN类型的模型中。对每个给定的字符，RNN类型的模型的输出结果都是一个四维的向量（向量的维度对应于词汇数量），它表示词汇中每个字符作为给定输入字符之后的下一个字符的概率。图4表明了该过程：<br>
![image](https://github.com/yanjiusheng2018/dlt/blob/master/src/content/Chapter10/chapter10_image/%E5%9B%BE4.jpg)图4：以独热编码字符作为RNN类型的网络的输入以及输出是基于当前输入之后最有可能出现的字符的概率分布示例<br>
&emsp;&emsp;正如图4所示，你会发现我们将输入序列中的第一个字符h输入模型，输出的四维向量表示下一个字符的置信度。因此，在输入h之后，出现的下一个字符是h的置信度是1，出现下一个字符是e的置信度是2.2，出现下一个字符是l的置信度是-3.0，出现下一个字符是o的置信度4.1。在这个特殊示例中，基于我们得训练样本的序列是hello，我们知道正确的下一个字符是e。因此，我们在训练这个RNN型网络的同时，主要目标是增加e作为下一个字符的可信度，降低其他字符的可信度。为了达到优化目的，我们将使用梯度下降和反向传播算法进行权重的更新并影响网络，以便对下个出现的准确的字符e生成更高的可信度，以此类推，对其他三个训练样本也进行权重更新以降低损失。<br>
&emsp;&emsp;正如你所看到的，RNN型网络的输出会对词汇中的所有字符作为下一个字符出现生成置信分布。我们可以将这种置信分布转化为概率分布，这样某一个字符作为下一个字符出现的概率增加会导致其他字符出现的概率，因为概率和恒为1。对于这种特殊优化，我们可以对每个输出向量进行标准softmax函数的转换。<br>
&emsp;&emsp;为了从这类网络中生成文本，我们可以对这个模型输入一个初始字符并得到接下来有可能出现的字符的概率分布，然后可以从这些字符中进行采样，并将其作为输入字符返回输入到模型中。通过一遍又一遍的重复这个过程我们可以得到一系列字符，也就是我们想要生成的固定长度的文本。<br>
### 使用莎士比亚数据建立语言模型
&emsp;&emsp;从上述例子中，我们可以通过模型生成文本。但另我们惊奇的是，神经网络不仅会生成文本，还会学习训练数据的风格和结构。我们可以通过对某一具有结构和风格的特定文本进行RNN型模型的训练来阐明这个有趣的过程，比如下面莎士比亚的作品:<br>
&emsp;&emsp;让我们来看看通过训练网络生成输出的作品：
Second Senator: 　<br>
    They are away this miseries, produced upon my soul,<br>
    Breaking and strongly should be buried, when I perish<br>
    The earth and thoughts of many states.<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
&emsp;&emsp;<br>
