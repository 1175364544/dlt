# Chapter 10    循环神经网络RNN——语言建模
&emsp;&emsp;循环神经网络（RNNs）是一类广泛用于自然语言处理（NLP）的深度学习体系结构，这套体系结构使我们能够为当前预测提供语境信息，并且具有特定的体系结构能够处理任一输入序列中的长时依赖关系。在本章中，我们将介绍如何建立序列到序列（seq2seq）的模型，seq2seq在自然语言处理（NLP）的应用中将会非常有用。我们将通过构建字符级语言模型来介绍RNN概念，并了解我们的模型是如何生成类似于原始输入序列的句子。<br>
#### 本章将包含以下三个方面：
*   RNNs的本质
*   LSTM（Long Short Term Memory长短期记忆网络）网络
*   语言模型的实现
## RNNs的本质
&emsp;&emsp;到目前为止，我们学习并运用的深度学习体系结构都没有任何机制能够记忆之前输入的序列或字符的信息。例如，前馈神经网络（FNN），如果输入一串字符序列HELLO，当神经网络接收到E，你会发现它忘记了刚读取过H，更没有保存任何关于H的信息，。这对基于序列的学习而言是非常严重的问题。因为它对以前读取的字符没有记忆，因此运用这类神经网络去训练预测下一个字符是非常困难的。同时这对于语言模型、机器翻译、语音识别等的应用都没有意义。<br>
&emsp;&emsp;由于这个特殊的原因，我们引入了RNNs，这是一组深度学习体系结构，它能够保存信息并记忆刚接收到的信息。<br>
&emsp;&emsp;接下来演示RNNs应该如何处理相同的输入的字符序列，HELLO。当RNNs记忆单元接收到E作为输入字符，它同时也会接收到字符H，字符H比字符E接收的更早。这种把当前的字符以及之前的字符作为对RNN细胞元的输入时，对这种体系结构提供了很大的优势，即短期记忆；在这个特定的字符序列中，它还能使这些体系结构可用于预测或猜测H之后最可能出现的字符，即E。<br>
&emsp;&emsp;我们已经了解到，以前的体系结构为其输入分配权重；RNNs遵循相同的优化过程，为其多个输入分配权重，即当前的输入和过去的输入。所以在这个案例中，神经网络将会对当输入前和上一时刻的输出作为这一时刻的输入分配两个不同的权重矩阵。为了做到这一点，我们将使用梯度下降和重配比的反向传播，即基于时间的反向传播算法（BPTT）。<br>
## 循环神经网络体系结构
&emsp;&emsp;基于我们之前使用的深度学习体系结构的背景，你会发现RNNs的特别之处。我们之前学习的结构体系在输入或训练方面并不灵活。这些结构体系接收固定大小的序列、向量、图像作为输入并产生另一个固定大小的序列、向量、图像作为输出。RNN体系结构在某种程度上是不同的，因为它可以输入一个序列但输出另一个序列，或者如图一所示，输入序列但是单输出，或单输入但是输出为序列。这种灵活性对于如语言建模和情绪分析的多种应用程序非常有用:<br>
![image](https://github.com/yanjiusheng2018/dlt/blob/master/src/content/Chapter10/chapter10_image/%E5%9B%BE1.jpg)图1：在输入或输出形状方面RNNs的灵活性
&emsp;&emsp;这类体系结构的本质是模仿人类处理信息的方式。在任何谈话过程中，你对对方话语的理解完全取决于他之前所讲的话，你甚至可以根据对方刚才讲的话预测他接下来会将什么。<br>
&emsp;&emsp;RNN在运用过程中也应该遵循完全相同的过程。例如，假设你想要翻译某一个句子中的一个特定的单词。你不会使用传统的前馈神经网络，因为传统的神经网络没有将之前接收到的单词的翻译的输出作为我们想要翻译的当前单词的输入的能力，并且也会因为缺少单词的上下文的信息而导致翻译错误。<br>
&emsp;&emsp;RNNs保留过去的信息，并具有某种循环方式，允许在任何给定的点上使用之前学习到的信息进行当前预测：<br>
![image]

&emsp;&emsp;<br>
