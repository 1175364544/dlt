# &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;13、自动编码器-特征提取和降噪

&emsp;&emsp;自动编码器网络是当前广泛应用的深度学习体系结构之一。 主要用于高效解码任务的无监督学习。它还可以通过学习编码或特定数据集的表示来进行降维。在本章中，我们将使用autoencoders，通过构造另一个具有相同尺寸但噪音更小的数据集来减少数据集的噪音。为了在实践中使用这个概念，我们将从MNIST数据集中提取重要的特性，并尝试了解如何这将显著提高性能。

本章将讨论以下主题:

- **自动编码器介绍**
- **自动编码器的示例**
- **自动编码器架构**
- **压缩MNIST数据集**
- **卷积式自动编码器**
- **去噪自动编码器**
- **自动编码器的应用**

## 自动编码器的介绍
&emsp;&emsp;自动编码器是另一种可用于许多有趣任务的深度学习体系结构，但它也可以看作是普通前馈神经网络的变体，其中输出具有与输入相同的维度。如图1所示，自动编码器的工作方式是将数据样本(x1，…，x6)发送到网络。它将尝试学习L2层中该数据的较低表示形式，您可以将其称为以较低表示形式编码数据集的方法。然后，网络的第二部分(您可能称之为解码器)负责从该表示构造输出。您可以将网络从输入数据中学到的中间低层表示形式看作是它的压缩版本。
与我们目前看到的所有其他深度学习架构没有太大区别，自动编码器使用反向传播算法。

&emsp;&emsp;与我们目前看到的所有其他深度学习架构没有太大区别，自动编码器使用反向传播算法。

&emsp;&emsp;自编码器神经网络是一种应用反向传播的无监督学习算法，其目标值等于输入:
![image.png](https://raw.githubusercontent.com/yanjiusheng2018/dlt/master/src/content/Chapter13/chapter13_image/%E5%9B%BE1.jpg)

## 自编码的例子
&emsp;&emsp;在本章中，我们将展示一些使用MNIST数据集的自动编码器的不同变体的例子。例如，假设输入x为28×28图像(784像素)的像素强度值;输入数据样本的个数是n=784。在L2层有s2=392个隐藏单元。由于输出和输入数据样本的尺寸相同，y∈R784。输入层神经元数量为784个，中间层L2神经元数量为392个;所以网络将是一个较低的表示，这是一个压缩版本的输出。然后，网络将把输入a(L2) ∈R392压缩后的较低表示形式提供给网络的第二部分，后者将努力从这个压缩版本重构输入像素784。

&emsp;&emsp;自动编码器依赖于这样一个事实，即由图像像素表示的输入样本将以某种方式相互关联，然后它将利用这个事实来重构它们。因此，自动编码器有点类似于降维技术，因为它们还可以学习输入数据的更低表示形式。

&emsp;&emsp;综上所述，一个典型的自动编码器由三部分组成: 1.编码器部分，负责将输入压缩成较低的表示形式 2.代码，它是编码器的中间结果 3.解码器，它负责用这个代码重建原始输入

&emsp;&emsp;下图显示了一个典型的自动编码器的三个主要组件:
![image.png](https://raw.githubusercontent.com/yanjiusheng2018/dlt/master/src/content/Chapter13/chapter13_image/%E5%9B%BE%E4%BA%8C.jpg)
&emsp;&emsp;正如我们所提到的，自动编码器部分学习输入的压缩表示，然后将其输入到第三部分，第三部分试图重构输入。重构后的输入与输出类似，但与原始输出并不完全相同，因此自动编码器不能用于压缩任务。

## 自编码器的架构
&emsp;&emsp;正如我们所提到的，一个典型的自动编码器由三个部分组成。让我们更详细地探讨这三个部分。为了激励你们，我们不打算在这一章里重复发明轮子。编码器-解码器部分是一个完全连接的神经网络，而代码部分是另一个神经网络，但它没有完全连接。该代码部分的维度是可控的，我们可以把它当作一个超参数:
![image.png](https://raw.githubusercontent.com/yanjiusheng2018/dlt/master/src/content/Chapter13/chapter13_image/%E5%9B%BE%E4%B8%89.jpg)

&emsp;&emsp;在深入使用自动编码器压缩MNIST数据集之前，我们将列出一组超参数，我们可以使用它们来微调自动编码器模型。超参数主要有四个:<br>
**1.代码部分大小:** 这是中间层的单元数。这一层的单元数越少，我们得到的输入表示就越压缩。<br>
**2.编码器和解码器的层数:** 正如我们所提到的，编码器和解码器只是一个完全连接的神经网络，我们可以通过添加更多的层来尽可能深入地构建它。<br>
**3.每个层的单元数:** 我们也可以在每个层中使用不同数量的单元数。编码器和解码器的形状非常类似于反解码器，当我们接近代码部分时，编码器中的层数会减少，然后在接近解码器的最后一层时开始增加。<br>
**4.模型损失函数:** 我们可以使用不同的损失函数，如MSE或交叉熵。<br>
在定义这些超参数并给出初始值之后，我们可以使用反向传播算法对网络进行训练。<br>
## 压缩MNIST数据集<br>
&emsp;&emsp;在这一部分中，我们将构建一个简单的自编码器，用于压缩MNIST数据集。因此，我们将把这个数据集的图像输入到编码器部分，编码器将尝试为它们学习一个较低的压缩表示;然后在解码器部分尝试重新构造输入图像。<br>
## MNIST数据集
&emsp;&emsp;我们将通过获取MNIST数据集，使用TensorFlow的辅助函数开始实现。
&emsp;&emsp;让我们导入这个实现所需的包:
""import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
