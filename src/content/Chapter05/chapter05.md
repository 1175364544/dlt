
# &#8195;&#8195;&#8195;&#8195;第五章     TensorFlow实战——一些基本的例子

&#8195;&#8195;在本章中，我们将阐述TensorFlow的主要计算概念，并演示如何通过实现线性回归和 logistic回归使大家步入学习TensorFlow的正轨。
  

&#8195;&#8195;本章将讨论以下主题:

    1. 单神经元和激活函数的作用。
    2. 激活函数。
    3. 前向神经网络（前馈式神经元网络）。
    4. 多层神经网络的需求概述。
    5. TensorFlow基本术语概述。
    6. 线性回归模型的建立和训练。
    7. Logistic回归模型的建立和训练。

&#8195;&#8195;我们将从解释单个神经元的实际功能/模型开始，在此基础上，将会出现对多层神经网络的需求。接下来，我们将进一步阐述在TensorFlow中使用的主要概念和工具，以及如何使用这些工具来构建像线性回归和Logistic回归这样的简单的示例。

## 1.单神经元的作用

&#8195;&#8195;神经网络是一种计算模型，其灵感主要来自于人脑的生物神经网络处理输入信息的方式。神经网络在机器学习研究(特别是深度学习)和工业应用方面取得了巨大突破，例如在计算机视觉、语音识别和文本处理方面取得了突破性成果。在这一章中，我们将试着去理解一种叫做多层前向感知的神经网络。

### 生物机动与联系

&#8195;&#8195;我们大脑的基本计算单元被称为神经元，我们的神经系统中大约有860亿个神经元，这些神经元与大约10的14次方到10的15次个突触相连。

&#8195;&#8195;图1显示了一个生物神经元。图2显示了相应的数学模型。在绘制生物神经元的过程中，每个神经元接收来自树突的传入信号，然后沿着轴突产生输出信号，轴突在轴突处分裂并通过突触与其他神经元连接。

&#8195;&#8195;在一个神经元的相应数学计算模型中，沿着轴突x0运动的信号与系统中另一个神经元以w0表示的树突相互作用，该突触的突触强度是的乘法操作x0w0。这个想法是突触的强度w是通过网状传播得到的。它们是从一个特定神经元影响另一个神经元的。

&#8195;&#8195;此外，在基本计算模型中，树突将信号传送到主细胞体，并将其全部相加。如果最终结果超过某个上限值，就会激发在计算模型中的神经元。

&#8195;&#8195;另外，值得一提的是我们需要控制轴突输出的峰值，所以我们引出了激活函数。实际上，一个常见的激活函数选择是sigmoid函数，因为它需要一个实值输入(和之后的信号强度)，并将输出结果压缩为0到1之间。我们将在下面的部分中看到这些激活函数的详细信息:

![图一](https://github.com/yanjiusheng2018/dlt/blob/master/src/content/Chapter05/chapter05_image/1.png?raw=true)

生物模型有相应的基本数学模型:

![图二](./2.jpg)

&#8195;&#8195;神经网络的基本计算单位是神经元，通常称为节点或单位。它从其他节点或外部源接收输入并计算输出。每个输入都有一个相关的权重(w)，它是根据相对于其他输入的重要性来分配的。该节点用函数f(我们稍后定义)计算其输入的加权和。

&#8195;&#8195;因此，神经网络的基本计算单元一般称为神经元/节点/单元。这个神经元从前一个神经元甚至外部源接收它的输入，然后它对这个输入做一些处理就是所谓的激活。这个神经元的每个输入都与它自己的权重w相关联，权重w代表这个神经元的强度、连接以及输入的重要性。因此，神经网络的这个基本构件的最终输出是输入的以其重要性w加权一个加总，然后神经元通过一个激活函数传递加总输出。

![图三](./3.png)

## 2.激活函数

&#8195;&#8195;神经元的输出如图3所示，并通过一个向输出引入非线性的激活函数进行计算。这个f叫做激活函数。激活功能的主要作用是:

    1.在神经元的输出中引入非线性。这一点很重要，因为现实世界中的大多数数据都是非线性的，我们希望神经元能够学习这些非线性表示。
    2.将输出压缩结果到一个特定的范围内。

&#8195;&#8195;每个激活函数(可能是非线性)都获取一个输入的数字，并对其进行某种固定的数学运算。在实际操作中，我们可能会遇到一些常用的激活函数。

&#8195;&#8195;我们将简要介绍几种常见的激活函数。

### Sigmoid激活函数

&#8195;&#8195;以往，Sigmoid激活函数被研究人员广泛使用。该函数接受实值输入并将其输出结果压缩到0 - 1之间，如下图所示:

<center>f(x) = 1 / (1 + exp(-x))</center>

![图四](./4.png)

### Tanh（双曲正切）激活函数

&#8195;&#8195;Tanh是一个允许负值的激活函数。Tanh接受一个实值输入并将其输出值压缩到（- 1,1）:

<center>tanh(x) = 2/(1+exp(-2x))-1</center>

![图五](./5.png)

### ReLU激活函数

&#8195;&#8195;修正线性单元(ReLU)因为它接受实值输入并且输出下限值为0(用0代替负值)，所以输出结果不允许有负值。

<center>f(x) = max(0, x)</center>

![图六](./6.png)

&#8195;&#8195;偏差的重要性:偏差的主要功能是为每个节点提供一个可训练的常量值(除了节点接收到的正常输入之外)请参阅https://stackoveflow.com/questions/2480650/role-of-bias-in-neural-networks 了解更多关于偏差在神经元中的作用。

## 3.前向神经网络

&#8195;&#8195;前向神经网络是第一个也是最简单的人工神经网络。它包含多层排列的多个神经元(节点)。相邻层的节点之间有连接或边。所有这些连接都有相应的权重。

&#8195;&#8195;前向神经网络的一个例子下图所示:

![图七](./7.png)

&#8195;&#8195;在前向网络中，信息每次只能向前移动一个方向，从输入节点、通过隐藏节点(如果有的话)并最终输出节点。在前向网络中没有循环(前向网络的这种特性不同于在节点之间的连接形成一个循环的循环神经网络)。

## 4.多层神经网络的需求概述

&#8195;&#8195;多层神经网络(MLP)包含一个或多个隐藏层(除了一个输入层和一个输出层)。单层神经网络只能学习线性函数，而MLP也可以学习非线性函数。上图显示了带有一个隐藏层的MLP。注意，所有连接都有相关的权重，图中只显示了三个权重(w0、w1和w2)。

&#8195;&#8195;输入层:输入层有三个节点。偏差节点的值为1。其他两个节点以X1和X2作为外部输入(它们是数值，根据输入数据集的不同而不同)。如前所述，输入层不执行任何计算，因此输入层节点的输出分别为1、X1和X2，并被输入到隐藏层中。

&#8195;&#8195;隐藏层:隐藏层也有三个节点，偏差节点的输出为1。隐藏层中其他两个节点的输出取决于输入层(1、X1和X2)的数值以及与连接(边)相关的权重。记住，f指的是激活函数。然后将这些输出发送到输出层的节点。

![图八](./8.png)

&#8195;&#8195;输出层:输出层有两个节点;它们从隐藏层获取输入，并对突出显示的隐藏节点执行类似的计算。这些计算结果的计算值(Y1和Y2)作为多层神经网络的输出。

&#8195;&#8195;给定一组特征X = (x1, x2, k)和一个目标y，多层神经网络可以学习特征和目标之间的关系，进行分类或回归。

&#8195;&#8195;让我们举个例子来更好地理解多层神经网络。假设我们有以下数据集标记的学生:

<center>表1 数据集标记学生样本</center>


|学习时间|期中成绩|期末结果|
| ------ | ------ | ------ |
|35|67|及格|
|12|75|不及格|
|16|89|及格|
|45|56|及格|
|10|90|不及格|

&#8195;&#8195;两个输入栏显示学生学习的时间和学生获得的期中成绩。期末结果可以有两个值，1或0，表示学生在最后一学期是否通过了考试。例如，我们可以看到，如果学生学习了35个小时，在期中取得了67分，他/她最终通过了期末考试。

&#8195;&#8195;现在，假设我们想预测一个学生学习了25小时，期中考试70分，那他是否会通过期末考试？

<center>表2 期末成绩未知的学生样本</center>


|学习时间|期中成绩|期末结果|
| ------ | ------ | ------ |
|25|70|？|

&#8195;&#8195;这是一个二元分类问题，MLP可以从给定的例子(训练数据)中学习，并在给定一个新的数据点时做出有根据的预测。我们将看到MLP如何学习这种关系。

### 训练MLP ——反向传播算法

&#8195;&#8195;多层神经网络学习的过程称为反向传播算法。我建议大家读读这个果壳问答网站的回答，https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri (稍后引用)，作者是Hemanth Kumar，他很清楚的解释了反向传播。

&#8195;&#8195;“误差反向传播，通常缩写为BackProp是人工神经网络(ANN)训练的几种方式之一。它是一种监督训练方案，也就是说，它从标记的训练数据中学习(有一个监督者，来指导它的学习)。  
&#8195;&#8195;简单来说，BackProp就像“从错误中学习”。每当人工神经系统出现错误时，监督者就会予以纠正。  
&#8195;&#8195;神经网络由不同层次的节点组成：输入层、中间隐藏层和输出层。相邻层节点之间的连接具有与其相关联的“权值”。学习的目标是为这些边分配正确的权重。给定一个输入向量，这些权重决定了输出向量是什么。  
&#8195;&#8195;在监督学习中，训练集被标记。这意味着，对于某些给定的输入，我们知道期望/期望输出。  
BackProp算法:  
&#8195;&#8195;最初，所有的边的权重都是随机分配的。对于训练数据集中的每一个输入，ANN都被激活，并且我们可以观察到它的输出。将此输出与我们已经知道的期望输出进行比较，错误的话将“传播”回前一层。我们认识到这个错误，并相应地“调整”权重。重复此过程，直到输出错误低于预定的上限值。  
&#8195;&#8195;一旦上述算法终止，我们就有了一个“学会的”ANN，我们认为它已经准备好处理“新的”输入。这个ANN可以说是从几个例子(标记数据)和它的错误(错误传播)中学到了东西。”  
——Hemanth Kumar

&#8195;&#8195;现在我们已经了解了反向传播的工作原理，让我们回到刚刚的学生数据集。

&#8195;&#8195;在分类应用中，我们广泛使用softmax函数（http://cs231n.github.io/linear-classify/#softmax) 作为MLP输出层中的激活函数，以确保输出为概率，且它们之和为1。softmax函数接受一个任意实值分数的向量，并将其输出值控制在0和1之间的向量，其总和为1。在这种情况下:

<center>p(及格)+p(不及格)=1</center>

### 第一步——前向传播

&#8195;&#8195;网络中的所有权值都是随机初始化的。让我们考虑一个特定的隐藏层节点，并将其称为V。假设从输入到该节点的连接的权重是w1、w2和w3。

&#8195;&#8195;然后神经网络将第一个训练样本作为输入(我们知道对于输入35和67，通过（及格）的概率是1):

    1.输入= [35,67]
    2.期望输出(目标)= [1,0]

&#8195;&#8195;然后考虑节点输出V，可如下计算(f为sigmod激活函数):

<center>V = f (1*w1 + 35*w2 + 67*w3)</center>

&#8195;&#8195;同样，也计算隐藏层中其他节点的输出。隐藏层中两个节点的输出作为输出层中两个节点的输入。这样我们能够计算输出层中的两个节点的输出概率。

&#8195;&#8195;假设输出层中两个节点的输出概率分别为0.4和0.6(由于权值是随机分配的，输出也是随机的)。我们可以看到，计算的概率(0.4和0.6)与期望的概率(1和0分别)相差很远;因此，这种网络的输出是不正确的。

### 第二步——反向传播和重置权重

&#8195;&#8195;我们计算输出节点的总误差，并使用反向传播计算这些误差反馈到网络。然后，采用梯度下降法等优化方法对网络中的权值进行调整，以减少输出层的误差。可以假设与所考虑的节点相关联的新权值是w4、w5和w6(在反向传播和调整权值之后)。如果我们现在将相同的示例作为输入输入到网络中，那么这个网络的功能应该会比初始运行时更好，因为现在已经对权重进行了优化，以减小预测中的错误。与前面的[0.6，-0.6]相比，输出节点的错误现在减少到[0.2，-0.2]。这意味着我们的网络已经学会正确地判断我们的第一个训练样本。

&#8195;&#8195;我们对数据集中的所有其他训练样本重复这个过程。然后，我们的网络据说已经学习了这些例子。

&#8195;&#8195;如果我们现在想要预测一个学习了25小时，在期中考试中得到70分的学生是否能通过期末考试，我们要通过正向传播步骤，找到通过（及格）和失败（不及格）的输出概率。

&#8195;&#8195;在这里，我们不对使用的数学方程和梯度下降法等概念的解释，而是尝试为算法开发一种直觉。有关反向传播算法的数学更复杂的讨论，请参阅以下链接:http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html.

## TensorFlow术语概述 

&#8195;&#8195;在本节中，我们将概述TensorFlow库以及基本的TensorFlow应用程序的结构。TensorFlow是一个用于创建大型机器学习应用程序的开源库;它可以在从android设备到gpu系统等各种各样的硬件上建模计算。

&#8195;&#8195;TensorFlow使用一种特殊的结构，以便在不同的设备(如cpu和gpu)上执行代码。计算指令被定义为一个graph(numeric computations 数值计算)，也被称为ops，每个graph都由各种操作组成，所以每当我们使用TensorFlow时，我们都会在一个graph中定义一系列的操作。

&#8195;&#8195;要运行这些操作，我们需要将图形启动到session（会话控制）中。会话控制将操作转换并将其传递给执行设备。例如，下面的图像表示TensorFlow中的一个图形。W，x，b是这个图边缘上的张量。矩阵是对张量W和x的运算;在此之后，调用Add，并将前面操作符的结果与b相加。每个操作的结果张量交叉到下一个，在那里得到我们需要的结果。

![图九](./9.png)

&#8195;&#8195;为了使用TensorFlow，我们需要导入库;我们给它命名为tf这样我们就可以通过写入tf来访问一个模块和模块的名字:


```python
import tensorflow as tf
```

    C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters
    

&#8195;&#8195;要创建第一个图，我们将首先使用不需要任何输入的源操作。这些源操作将把它们的信息传递给其他实际运行计算的操作。

&#8195;&#8195;让我们创建两个输出数字的源操作。我们将它们定义为A和B，如下面的代码所示:


```python
A = tf.constant([2])
```


```python
B = tf.constant([3])
```

之后，我们定义一个简单的计算操作tf.add()用来表示这两个元素的和,我们还可以使用C=A+B，代码如下所示:


```python
C = tf.add(A,B)  #C = A + B 是两个元素和的另一种定义方式
```

由于代码需要在会话操作上下文中执行，我们需要创建一个会话对象:


```python
session = tf.Session()
```

为了查看结果，让我们从前面定义的C运行会话操作获得结果:


```python
result = session.run(C)
print(result)
```

    [5]
    

&#8195;&#8195;你可能认为仅仅把两个数字加在一起就需要做很多工作，但是这对理解TensorFlow的基本结构是非常重要的。你这样做了，你就可以定义任何你想要的计算;同样，TensorFlow的结构允许它在不同的设备(CPU或GPU)甚至集群中处理计算。如果您想了解更多这方面的信息，可以运行tf.device()方法。

&#8195;&#8195;我们也可以自由地尝试TensorFlow的结构以便更好地了解它是如何工作的。如果您想要得到TensorFlow支持的所有数学操作的列表，您可以查看文档。现在，我们应该了解了TensorFlow的结构以及如何创建基本应用程序。

### 使用TensorFlow定义多维数组

&#8195;&#8195;现在我们将尝试使用TensorFlow来定义这样的数组:


```python
salar = tf.constant([2])
vector = tf.constant([5,6,2])
matrix = tf.constant([1,2,3,2,3,4,3,4,5],shape=[3,3])
tensor = tf.constant([1,2,3,2,3,4,3,4,5,4,5,6,5,6,7,6,7,8,7,8,9,8,9,10,9,10,11],shape=[3,3,3])
with tf.Session() as session:
  result = session.run(salar)
  print ("Vector (1 entry):\n %s \n" % result)
  result = session.run(vector)
  print ("Vector (3 entries):\n %s \n" % result)
  result = session.run(matrix)
  print ("Matrix (3*3 entries):\n %s \n" % result)
  result = session.run(tensor)
  print ("Tensor (3*3*3 entries):\n %s \n" % result)
```

    Vector (1 entry):
     [2] 
    
    Vector (3 entries):
     [5 6 2] 
    
    Matrix (3*3 entries):
     [[1 2 3]
     [2 3 4]
     [3 4 5]] 
    
    Tensor (3*3*3 entries):
     [[[ 1  2  3]
      [ 2  3  4]
      [ 3  4  5]]
    
     [[ 4  5  6]
      [ 5  6  7]
      [ 6  7  8]]
    
     [[ 7  8  9]
      [ 8  9 10]
      [ 9 10 11]]] 
    
    

&#8195;&#8195;现在我们已经了解了这些数据的结构，鼓励大家使用以前的一些函数根据它们的结构类型来了解它们的运算:


```python
Matrix_one = tf.constant([1,2,3,2,3,4,3,4,5],shape = [3,3])
Matrix_two = tf.constant([2,2,2,2,2,2,2,2,2],shape = [3,3])
first_operation = tf.add(Matrix_one , Matrix_two)
second_operation = Matrix_one + Matrix_two
with tf.Session() as session:
    result_a = session.run(first_operation)
    print("Defined using tensorflow function :")
    print(result_a)
    result_b = session.run(second_operation)
    print("Defined using normal expresssions :")
    print(result_b)
```

    Defined using tensorflow function :
    [[3 4 5]
     [4 5 6]
     [5 6 7]]
    Defined using normal expresssions :
    [[3 4 5]
     [4 5 6]
     [5 6 7]]
    

&#8195;&#8195;有了常规的符号定义和tensorflow的功能函数，我们可以得到一个元素的乘法，也称为Hadamard（阿达玛）乘积。但如果我们想要正则矩阵乘积呢?我们需要使用另一个名为tf.matmul()的TensorFlow功能函数:


```python
Matrix_one = tf.constant([[2,3],[3,4]])
Matrix_two = tf.constant([[2,3],[3,4]])
first_operation = tf.matmul(Matrix_one , Matrix_two)
with tf.Session() as session:
    result_c = session.run(first_operation)
    print("Defined using tensorflow function :")
    print(result_c)
```

    Defined using tensorflow function :
    [[13 18]
     [18 25]]
    

&#8195;&#8195;我们也可以自己定义这个乘法，但是有一个函数已经这样做了，所以没有必要重新创造一个乘法规则!

### 为什么使用张量（tensors）？ 

&#8195;&#8195;张量结构以我们想要的方式给了我们形状数据集的自由。基于图像编码的原理，这在处理图像时特别有用。考虑到图像，很容易理解它有高度和宽度，所以用二维结构(矩阵)来表示包含在其中的信息是有意义的……但是你还记得图像有颜色。为了增加关于颜色的信息，我们需要另一个维度，这时张量就变得特别有用了。

&#8195;&#8195;图像被编码成彩色信道;图像数据以颜色信道中每个颜色在给定点上的强度表示，最常见的是RGB(表示红、蓝、绿)。图像中包含的信息是图像宽度和高度中各位置颜色的强度，如下图所示:

![图十](./10.jpg)

&#8195;&#8195;因此，红色信道在每个点上的宽度和高度可以用矩阵表示;蓝色和绿色的信道也是如此。所以，我们最终得到了三个矩阵，当它们结合时，它们形成了一个张量。

### 变量 

&#8195;&#8195;现在我们对数据结构更熟悉了，接下来我们来看看TensorFlow如何处理变量。要定义变量，我们使用命令tf.variables()。为了能够在计算图中使用变量，必须在会话操作中运行代码之前对它们进行初始化。这是通过执行tf.global_variables_initializer()代码进行。

&#8195;&#8195;要更新变量的值，我们只需运行赋值操作，该操作为变量赋值:


```python
state = tf.Variable(0)
```

&#8195;&#8195;让我们首先创建一个简单的计数器，一个每次增加一个单位的变量:


```python
one = tf.constant(1)
new_value = tf.add(state,one)
update = tf.assign(state,new_value)
```

&#8195;&#8195;在运行代码之后，必须通过运行初始化操作来初始化变量。我们首先要向代码中添加初始化操作:


```python
init_op = tf.global_variables_initializer()
```

&#8195;&#8195;然后我们启动一个会话操作来运行代码。首先对变量进行初始化，然后输出状态变量的初始值，最后运行变量更新操作，每次更新后输出结果:


```python
with tf.Session() as session:
    session.run(init_op)
    print(session.run(state))
    for _ in range(3):
        session.run(update)
        print(session.run(state))
```

    0
    1
    2
    3
    

### 命令占位符 

&#8195;&#8195;现在，我们知道了如何在TensorFlow中操作变量，但是如何在TensorFlow模型之外输入数据呢?如果希望从模型外部将数据提供给TensorFlow模型，则需要使用占位符。

&#8195;&#8195;那么，这些占位符是什么呢?占位符可以看作是模型中的holes(洞，空间，内存)，您将把数据传递给这些holes。你可以用tf.placeholder(datatype)来创建它，其中datatype指定数据的类型(整数、浮点数、字符串和布尔值)及其精度(8、16、32和64字节)。

&#8195;&#8195;每个具有各自Python语法的数据类型的定义定义如下:

<center>表三 TensorFlow数据类型</center>

|数据类型|Python类型|类型说明|
| ------ | ------ | ------ |
|DT_FLOAT|tf.float32|32位浮点数|
|DT_DOUBLE|tf.float64|64位浮点数|
|DT_INT8|tf.int8|8位带符号整数|
|DT_INT16|tf.int16|16位带符号整数|
|DT_INT32|tf.int32|32位带符号整数|
|DT_INT64|tf.int64|64位带符号整数|
|DT_UNIT8|tf.unit8|8位无符号整数|
|DT_STRING|tf.string|可变长量字符串|
|DT_BOOL|tf.bool|布尔型|
|DT_COMPLEX64|tf.complex64|由两个32位浮点数组成的复数:实部和虚部|
|DT_COMPLEX128|tf.complex128|由两个64位浮点数组成的复数:实部和虚部|
|DT_QINT8|ty.qint8|在量化操作中使用的8位有符号整数|
|DT_QINT32|ty.qint32|在量化操作中使用的32位有符号整数|
|DT_QUINT8|ty.quint|在量化操作中使用的8位无符号整数|


&#8195;&#8195;我们来创建一个占位符:


```python
a = tf.placeholder(tf.float32)
```

&#8195;&#8195;并定义一个简单的乘法运算:


```python
b = a*2
```

&#8195;&#8195;现在，我们需要定义和运行会话控制，但是我们要在模型中创建了一个内存空间，以便在初始化会话时传递数据。我们不得不约束数据大小;否则我们会得到一个错误。

&#8195;&#8195;为了将数据传递给模型，我们用一个额外的参数feed_dict调用会话，我们应该传递一个字典，其中每个占位符的名称后面跟着它各自的数据，就像这样:


```python
with tf.Session() as sess:
    result = sess.run(b,feed_dict = {a:3.5})
    print(result)
```

    7.0
    

&#8195;&#8195;由于TensorFlow中的数据以多维数组的形式传递，我们可以通过占位符传递任何张量来得到简单乘法运算的答案:


```python
dictionary = {a:[[[1,2,3],[4,5,6],[7,8,9],[10,11,12]],[[13,14,15],[16,17,18],[19,20,21],[22,23,24]]]}
with tf.Session() as sess:
    result = sess.run(b,feed_dict=dictionary)
    print(result)
```

    [[[ 2.  4.  6.]
      [ 8. 10. 12.]
      [14. 16. 18.]
      [20. 22. 24.]]
    
     [[26. 28. 30.]
      [32. 34. 36.]
      [38. 40. 42.]
      [44. 46. 48.]]]
    

### 运算 

&#8195;&#8195;运算是表示在代码中张量上的数学运算的节点。这些运算可以是任意类型的函数，比如加和减张量，也可以是激活函数。tf.matmul,tf.add和tf.sigmod都是TensorFlow中的运算。它们就像Python中的函数，但是它是直接在张量上操作，每个张量都有特定的功能。其他操作也很容易找到:https://www.tensorflow.org/api_guides/python/math_ops .

&#8195;&#8195;让我们来看看其中的一些运算:


```python
a = tf.constant([5])
b = tf.constant([2])
c = tf.add(a,b)
d = tf.subtract(a,b)
with tf.Session() as session:
    result = session.run(c)
    print('c = : %s '% result)
    result = session.run(d)
    print('d = : %s '% result)
```

    c = : [7] 
    d = : [3] 
    

&#8195;&#8195;tf.nn.sigmod是一个激活函数:这有点复杂，但这个功能帮助学习模型评估哪种信息是好是坏。
