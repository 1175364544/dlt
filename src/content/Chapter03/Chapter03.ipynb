{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:159: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:246: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:123: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:131: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:192: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:209: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:214: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:297: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:298: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:273: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:283: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:71: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:90: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 105 manually constructing features based on the interaction between them...\n",
      " ['Survived' 'PassengerId' 'Pclass' 'Age' 'SibSp' 'Parch' 'Fare' 'Embarked' 'CabinLetter' 'CabinLetter_0' 'CabinLetter_1' 'CabinLetter_2' 'CabinLetter_3'\n",
      " 'CabinLetter_4' 'CabinLetter_5' 'CabinLetter_6' 'CabinLetter_7' 'CabinLetter_8' 'CabinNumber' 'CabinNumber_scaled' 'TicketPrefixId' 'TicketPrefix_A'\n",
      " 'TicketPrefix_AQ' 'TicketPrefix_AS' 'TicketPrefix_C' 'TicketPrefix_CA' 'TicketPrefix_CASOTON' 'TicketPrefix_FA' 'TicketPrefix_FC' 'TicketPrefix_FCC'\n",
      " 'TicketPrefix_LINE' 'TicketPrefix_LP' 'TicketPrefix_PC' 'TicketPrefix_PP' 'TicketPrefix_PPP' 'TicketPrefix_SC' 'TicketPrefix_SCA' 'TicketPrefix_SCAH'\n",
      " 'TicketPrefix_SCOW' 'TicketPrefix_SCPARIS' 'TicketPrefix_SOC' 'TicketPrefix_SOP' 'TicketPrefix_SOPP' 'TicketPrefix_SOTONO' 'TicketPrefix_SOTONOQ'\n",
      " 'TicketPrefix_SP' 'TicketPrefix_SWPP' 'TicketPrefix_U' 'TicketPrefix_WC' 'TicketPrefix_WEP' 'TicketNumberDigits' 'TicketNumberStart' 'TicketNumber_scaled'\n",
      " 'Names' 'Title_Dr' 'Title_Lady' 'Title_Master' 'Title_Miss' 'Title_Mr' 'Title_Mrs' 'Title_Rev' 'Title_Sir' 'Names_scaled' 'Title_id' 'Title_id_scaled'\n",
      " 'Fare_(0.316, 7.896]' 'Fare_(7.896, 14.454]' 'Fare_(14.454, 31.275]' 'Fare_(31.275, 512.329]' 'Fare_bin_id' 'Fare_scaled' 'Fare_bin_id_scaled' 'Embarked_0'\n",
      " 'Embarked_1' 'Embarked_2' 'SibSp_scaled' 'Parch_scaled' 'SibSp_1' 'SibSp_2' 'SibSp_3' 'SibSp_4' 'SibSp_5' 'SibSp_6' 'SibSp_9' 'Parch_1' 'Parch_2' 'Parch_3'\n",
      " 'Parch_4' 'Parch_5' 'Parch_6' 'Parch_7' 'Parch_10' 'Gender' 'Pclass_1' 'Pclass_2' 'Pclass_3' 'Pclass_scaled' 'Age_scaled' 'isChild' 'Age_(0.169, 21.0]'\n",
      " 'Age_(21.0, 28.035]' 'Age_(28.035, 38.0]' 'Age_(38.0, 80.0]' 'Age_bin_id' 'Age_bin_id_scaled']\n",
      "\n",
      "Using only numeric features for automated feature generation:\n",
      "    Age_scaled  Fare_scaled  Pclass_scaled  Parch_scaled  SibSp_scaled  Names_scaled  CabinNumber_scaled  Age_bin_id_scaled  Fare_bin_id_scaled\n",
      "0     -0.5789      -0.5034         0.8419       -0.4450        0.4813       -0.0755             -0.4207            -1.3561             -1.3233\n",
      "1      0.5982       0.7347        -1.5461       -0.4450        0.4813        2.4586              2.8435            -0.4618             -0.4345\n",
      "2     -0.2847      -0.4903         0.8419       -0.4450       -0.4791       -0.9202             -0.4207            -1.3561              0.4542\n",
      "3      0.3775       0.3831        -1.5461       -0.4450        0.4813        2.4586              4.3027            -0.4618             -0.4345\n",
      "4      0.3775      -0.4879         0.8419       -0.4450       -0.4791       -0.0755             -0.4207            -0.4618              0.4542\n",
      "5     -0.0054      -0.4800         0.8419       -0.4450       -0.4791       -0.9202             -0.4207            -0.4618              0.4542\n",
      "6      1.7753       0.3592        -1.5461       -0.4450       -0.4791       -0.0755              1.3458             0.4324             -0.4345\n",
      "7     -2.0504      -0.2361         0.8419        0.7108        2.4020       -0.0755             -0.4207             1.3267              1.3430\n",
      "8     -0.2111      -0.4283         0.8419        1.8665       -0.4791        2.4586             -0.4207            -1.3561              0.4542\n",
      "9     -1.1675      -0.0622        -0.3521       -0.4450        0.4813        0.7692             -0.4207             1.3267              1.3430\n",
      "\n",
      " 176 new features constructed\n",
      "\n",
      "We are going to drop 44  which are highly correlated features...\n",
      "\n",
      "\n",
      " 237 initial features generated...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:501: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:502: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass   Age  SibSp  Parch     Fare  Embarked  CabinLetter  CabinLetter_1  CabinLetter_2  CabinLetter_3  CabinLetter_4  CabinLetter_5  CabinLetter_6  CabinLetter_7  CabinLetter_8  CabinNumber  TicketPrefixId  TicketPrefix_A  TicketPrefix_AQ  TicketPrefix_AS  TicketPrefix_C  TicketPrefix_CA  TicketPrefix_CASOTON  TicketPrefix_FA  TicketPrefix_FC  TicketPrefix_FCC  TicketPrefix_LINE  TicketPrefix_LP  TicketPrefix_PC  TicketPrefix_PP  TicketPrefix_PPP  TicketPrefix_SC  TicketPrefix_SCA  TicketPrefix_SCAH  TicketPrefix_SCOW  TicketPrefix_SCPARIS  TicketPrefix_SOC  TicketPrefix_SOP  TicketPrefix_SOPP  TicketPrefix_SOTONO  TicketPrefix_SOTONOQ  TicketPrefix_SP  TicketPrefix_SWPP  TicketPrefix_U  TicketPrefix_WC  TicketPrefix_WEP  TicketNumberDigits  TicketNumberStart  TicketNumber_scaled  Names  Title_Dr  Title_Lady  Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Rev  Title_Sir  Title_id  Fare_(0.316, 7.896]  Fare_(7.896, 14.454]  Fare_(14.454, 31.275]  Fare_(31.275, 512.329]  Fare_bin_id  Embarked_1  Embarked_2  SibSp_2  SibSp_3  SibSp_4  SibSp_5  SibSp_6  SibSp_9  Parch_2  Parch_3  Parch_4  Parch_5  Parch_6  Parch_7  Parch_10  Gender  Pclass_1  Pclass_2  Pclass_3  isChild  Age_(0.169, 21.0]  Age_(21.0, 28.035]  Age_(28.035, 38.0]  Age_(38.0, 80.0]  Age_bin_id  Age_scaled*Age_scaled  Age_scaled*Fare_scaled  Age_scaled+Fare_scaled  Age_scaled/Fare_scaled  Age_scaled-Fare_scaled  Age_scaled*Pclass_scaled  Age_scaled+Pclass_scaled  Age_scaled/Pclass_scaled  Age_scaled-Pclass_scaled  Age_scaled*Parch_scaled  Age_scaled+Parch_scaled  Age_scaled/Parch_scaled  Age_scaled-Parch_scaled  Age_scaled*SibSp_scaled  Age_scaled+SibSp_scaled  Age_scaled/SibSp_scaled  Age_scaled-SibSp_scaled  Age_scaled*Names_scaled  Age_scaled+Names_scaled  Age_scaled/Names_scaled  Age_scaled-Names_scaled  Age_scaled*CabinNumber_scaled  Age_scaled+CabinNumber_scaled  Age_scaled/CabinNumber_scaled  Age_scaled-CabinNumber_scaled  Age_scaled*Age_bin_id_scaled  Age_scaled+Age_bin_id_scaled  Age_scaled/Age_bin_id_scaled  Age_scaled-Age_bin_id_scaled  Fare_scaled/Age_scaled  Fare_scaled*Fare_scaled  Fare_scaled*Pclass_scaled  Fare_scaled+Pclass_scaled  Fare_scaled/Pclass_scaled  Fare_scaled-Pclass_scaled  Fare_scaled*Parch_scaled  Fare_scaled+Parch_scaled  Fare_scaled/Parch_scaled  Fare_scaled-Parch_scaled  Fare_scaled*SibSp_scaled  Fare_scaled+SibSp_scaled  Fare_scaled/SibSp_scaled  Fare_scaled-SibSp_scaled  Fare_scaled*Names_scaled  Fare_scaled+Names_scaled  Fare_scaled/Names_scaled  Fare_scaled-Names_scaled  Fare_scaled*CabinNumber_scaled  Fare_scaled+CabinNumber_scaled  Fare_scaled/CabinNumber_scaled  Fare_scaled-CabinNumber_scaled  Fare_scaled*Age_bin_id_scaled  Fare_scaled+Age_bin_id_scaled  Fare_scaled/Age_bin_id_scaled  Fare_scaled-Age_bin_id_scaled  Pclass_scaled/Age_scaled  Pclass_scaled/Fare_scaled  Pclass_scaled*Pclass_scaled  Pclass_scaled*Parch_scaled  Pclass_scaled+Parch_scaled  Pclass_scaled/Parch_scaled  Pclass_scaled-Parch_scaled  Pclass_scaled*SibSp_scaled  Pclass_scaled+SibSp_scaled  Pclass_scaled/SibSp_scaled  Pclass_scaled-SibSp_scaled  Pclass_scaled*Names_scaled  Pclass_scaled+Names_scaled  Pclass_scaled/Names_scaled  Pclass_scaled-Names_scaled  Pclass_scaled*CabinNumber_scaled  Pclass_scaled+CabinNumber_scaled  Pclass_scaled/CabinNumber_scaled  Pclass_scaled-CabinNumber_scaled  Pclass_scaled*Age_bin_id_scaled  Pclass_scaled+Age_bin_id_scaled  Pclass_scaled/Age_bin_id_scaled  Pclass_scaled-Age_bin_id_scaled  Parch_scaled/Age_scaled  Parch_scaled/Fare_scaled  Parch_scaled/Pclass_scaled  Parch_scaled*SibSp_scaled  Parch_scaled+SibSp_scaled  Parch_scaled/SibSp_scaled  Parch_scaled-SibSp_scaled  Parch_scaled*Names_scaled  Parch_scaled+Names_scaled  Parch_scaled/Names_scaled  Parch_scaled-Names_scaled  Parch_scaled*CabinNumber_scaled  Parch_scaled+CabinNumber_scaled  Parch_scaled/CabinNumber_scaled  Parch_scaled-CabinNumber_scaled  Parch_scaled*Age_bin_id_scaled  Parch_scaled+Age_bin_id_scaled  Parch_scaled/Age_bin_id_scaled  Parch_scaled-Age_bin_id_scaled  SibSp_scaled/Age_scaled  SibSp_scaled/Fare_scaled  SibSp_scaled/Pclass_scaled  SibSp_scaled/Parch_scaled  SibSp_scaled*Names_scaled  SibSp_scaled+Names_scaled  SibSp_scaled/Names_scaled  SibSp_scaled-Names_scaled  SibSp_scaled*CabinNumber_scaled  SibSp_scaled+CabinNumber_scaled  SibSp_scaled/CabinNumber_scaled  SibSp_scaled-CabinNumber_scaled  SibSp_scaled*Age_bin_id_scaled  SibSp_scaled+Age_bin_id_scaled  SibSp_scaled/Age_bin_id_scaled  SibSp_scaled-Age_bin_id_scaled  Names_scaled/Age_scaled  Names_scaled/Fare_scaled  Names_scaled/Pclass_scaled  Names_scaled/Parch_scaled  Names_scaled/SibSp_scaled  Names_scaled*Names_scaled  Names_scaled*CabinNumber_scaled  Names_scaled+CabinNumber_scaled  Names_scaled/CabinNumber_scaled  Names_scaled-CabinNumber_scaled  Names_scaled*Age_bin_id_scaled  Names_scaled+Age_bin_id_scaled  Names_scaled/Age_bin_id_scaled  Names_scaled-Age_bin_id_scaled  CabinNumber_scaled/Age_scaled  CabinNumber_scaled/Fare_scaled  CabinNumber_scaled/Pclass_scaled  CabinNumber_scaled/Parch_scaled  CabinNumber_scaled/SibSp_scaled  CabinNumber_scaled/Names_scaled  CabinNumber_scaled*CabinNumber_scaled  CabinNumber_scaled*Age_bin_id_scaled  CabinNumber_scaled+Age_bin_id_scaled  CabinNumber_scaled/Age_bin_id_scaled  CabinNumber_scaled-Age_bin_id_scaled  Age_bin_id_scaled/Age_scaled  Age_bin_id_scaled/Fare_scaled  Age_bin_id_scaled/Pclass_scaled  Age_bin_id_scaled/Parch_scaled  Age_bin_id_scaled/SibSp_scaled  Age_bin_id_scaled/Names_scaled  Age_bin_id_scaled/CabinNumber_scaled  Age_bin_id_scaled*Age_bin_id_scaled\n",
      "0       0.0       3  22.0      2      1   7.2500         0            0              0              0              0              0              0              0              0              0            1               0               1                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               0                0                 0                   5                  2              -0.4123      4         0           0             0           0         1          0          0          0         1                    1                     0                      0                       0            1           0           0        1        0        0        0        0        0        0        0        0        0        0        0         0       1         0         0         1        0                  0                   1                   0                 0           1                 0.3352                  0.2914                 -1.0823                  1.1501                 -0.0755                   -0.4874                    0.2630                   -0.6876                   -1.4209                   0.2576                  -1.0239                   1.3010                  -0.1339                  -0.2786                  -0.0976                  -1.2029                  -1.0602                   0.0437                  -0.6544                   7.6679                  -0.5034                         0.2435                        -0.9996                         1.3763                        -0.1583                        0.7851                       -1.9350                        0.4269                        0.7771                  0.8695                   0.2534                    -0.4238                     0.3385                    -0.5979                    -1.3453                    0.2240                   -0.9484                    1.1312                   -0.0584                   -0.2423                   -0.0221                   -1.0459                   -0.9847                    0.0380                   -0.5789                    6.6674                   -0.4279                          0.2118                         -0.9241                          1.1967                         -0.0827                         0.6826                        -1.8595                         0.3712                         0.8527                   -1.4542                    -1.6725                       0.7088                     -0.3747                      0.3969                     -1.8919                      1.2869                      0.4052                      1.3232                      1.7493                      0.3606                     -0.0636                      0.7664                    -11.1511                      0.9174                           -0.3542                            0.4213                           -2.0014                            1.2626                          -1.1417                          -0.5141                          -0.6209                           2.1980                   0.7687                    0.8840                     -0.5286                    -0.2142                     0.0363                    -0.9246                    -0.9263                     0.0336                    -0.5205                     5.8940                    -0.3695                           0.1872                          -0.8657                           1.0579                          -0.0243                          0.6034                         -1.8011                          0.3282                          0.9111                  -0.8313                   -0.9561                      0.5717                    -1.0815                    -0.0363                     0.4058                    -6.3746                     0.5568                          -0.2025                           0.0606                          -1.1441                           0.9019                         -0.6527                         -0.8748                         -0.3549                          1.8374                   0.1304                    0.1500                     -0.0897                     0.1697                    -0.1569                     0.0057                           0.0318                          -0.4962                           0.1795                           0.3452                          0.1024                         -1.4316                          0.0557                          1.2806                         0.7266                          0.8356                           -0.4996                           0.9453                           -0.874                           5.5715                                 0.1769                                0.5704                               -1.7767                                0.3102                                0.9354                        2.3423                         2.6938                          -1.6107                          3.0473                         -2.8176                         17.9609                                3.2237                               1.8389\n",
      "1       1.0       1  38.0      2      1  71.2833         1            1              1              0              0              0              0              0              0              0           86               1               0                0                0               0                0                     0                0                0                 0                  0                0                1                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               0                0                 0                   5                  1              -0.4180      7         0           0             0           0         0          1          0          0         2                    0                     0                      0                       1            2           1           0        1        0        0        0        0        0        0        0        0        0        0        0         0       0         1         0         0        0                  0                   0                   1                 0           2                 0.3578                  0.4395                  1.3329                  0.8142                 -0.1365                   -0.9249                   -0.9479                   -0.3869                    2.1443                  -0.2662                   0.1532                  -1.3443                   1.0432                   0.2879                   1.0795                   1.2429                   0.1169                   1.4708                   3.0568                   0.2433                  -1.8604                         1.7010                         3.4417                         0.2104                        -2.2452                       -0.2763                        0.1364                       -1.2953                        1.0600                  1.2282                   0.5398                    -1.1359                    -0.8114                    -0.4752                     2.2808                   -0.3269                    0.2897                   -1.6510                    1.1797                    0.3536                    1.2160                    1.5265                    0.2534                    1.8064                    3.1933                    0.2988                   -1.7239                          2.0891                          3.5782                          0.2584                         -2.1088                        -0.3393                         0.2729                        -1.5909                         1.1965                   -2.5846                    -2.1044                       2.3904                      0.6880                     -1.9911                      3.4744                     -1.1011                     -0.7441                     -1.0648                     -3.2124                     -2.0274                     -3.8013                      0.9125                     -0.6288                     -4.0047                           -4.3963                            1.2974                           -0.5437                           -4.3896                           0.7140                          -2.0079                           3.3479                          -1.0843                  -0.7439                   -0.6057                      0.2878                    -0.2142                     0.0363                    -0.9246                    -0.9263                    -1.0941                     2.0136                    -0.1810                    -2.9036                          -1.2653                           2.3985                          -0.1565                          -3.2885                          0.2055                         -0.9068                          0.9636                          0.0168                   0.8046                    0.6551                     -0.3113                    -1.0815                     1.1833                     2.9399                     0.1958                    -1.9773                           1.3685                           3.3247                           0.1693                          -2.3622                         -0.2223                          0.0195                         -1.0422                          0.9431                   4.1100                    3.3464                     -1.5902                    -5.5250                     5.1084                     6.0448                           6.9910                           5.3021                           0.8647                          -0.3848                         -1.1354                          1.9968                         -5.3238                          2.9204                         4.7533                          3.8702                           -1.8391                          -6.3898                            5.908                           1.1565                                 8.0852                               -1.3131                                2.3816                               -6.1572                                3.3053                       -0.7720                        -0.6286                           0.2987                          1.0378                         -0.9595                         -0.1878                               -0.1624                               0.2133\n",
      "2       1.0       3  26.0      1      1   7.9250         0            0              0              0              0              0              0              0              0              0            1               2               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    1                     0                0                  0               0                0                 0                   7                  3               4.4371      3         0           0             0           1         0          0          0          0         3                    0                     1                      0                       0            3           0           0        0        0        0        0        0        0        0        0        0        0        0        0         0       0         0         0         1        0                  0                   1                   0                 0           1                 0.0810                  0.1396                 -0.7750                  0.5805                  0.2057                   -0.2397                    0.5573                   -0.3381                   -1.1266                   0.1267                  -0.7296                   0.6397                   0.1603                   0.1364                  -0.7637                   0.5942                   0.1944                   0.2619                  -1.2049                   0.3093                   0.6356                         0.1197                        -0.7053                         0.6767                         0.1360                        0.3860                       -1.6407                        0.2099                        1.0714                  1.7226                   0.2404                    -0.4128                     0.3516                    -0.5824                    -1.3323                    0.2182                   -0.9353                    1.1019                   -0.0453                    0.2349                   -0.9694                    1.0235                   -0.0113                    0.4512                   -1.4106                    0.5329                    0.4299                          0.2063                         -0.9110                          1.1657                         -0.0697                         0.6649                        -1.8464                         0.3616                         0.8657                   -2.9577                    -1.7170                       0.7088                     -0.3747                      0.3969                     -1.8919                      1.2869                     -0.4034                      0.3628                     -1.7573                      1.3210                     -0.7747                     -0.0783                     -0.9149                      1.7621                           -0.3542                            0.4213                           -2.0014                            1.2626                          -1.1417                          -0.5141                          -0.6209                           2.1980                   1.5633                    0.9075                     -0.5286                     0.2132                    -0.9241                     0.9288                     0.0341                     0.4095                    -1.3652                     0.4836                     0.4752                           0.1872                          -0.8657                           1.0579                          -0.0243                          0.6034                         -1.8011                          0.3282                          0.9111                   1.6831                    0.9770                     -0.5690                     1.0766                     0.4409                    -1.3993                     0.5206                     0.4411                           0.2015                          -0.8997                           1.1389                          -0.0584                          0.6497                         -1.8352                          0.3533                          0.8770                   3.2328                    1.8767                     -1.0930                     2.0679                     1.9208                     0.8468                           0.3871                          -1.3409                           2.1876                          -0.4996                          1.2479                         -2.2763                          0.6786                          0.4359                         1.4778                          0.8579                           -0.4996                           0.9453                            0.878                           0.4571                                 0.1769                                0.5704                               -1.7767                                0.3102                                0.9354                        4.7640                         2.7655                          -1.6107                          3.0473                          2.8305                          1.4736                                3.2237                               1.8389\n",
      "3       1.0       1  35.0      2      1  53.1000         0            1              1              0              0              0              0              0              0              0          124               3               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               1                0                 0                   6                  1              -0.2665      7         0           0             0           0         0          1          0          0         2                    0                     0                      0                       1            2           0           0        1        0        0        0        0        0        0        0        0        0        0        0         0       0         1         0         0        0                  0                   0                   1                 0           2                 0.1425                  0.1446                  0.7606                  0.9853                 -0.0056                   -0.5836                   -1.1686                   -0.2442                    1.9236                  -0.1680                  -0.0675                  -0.8483                   0.8225                   0.1817                   0.8588                   0.7843                  -0.1038                   0.9281                   2.8361                   0.1535                  -2.0811                         1.6242                         4.6802                         0.0877                        -3.9252                       -0.1743                       -0.0843                       -0.8174                        0.8393                  1.0149                   0.1468                    -0.5923                    -1.1630                    -0.2478                     1.9292                   -0.1705                   -0.0619                   -0.8610                    0.8281                    0.1844                    0.8644                    0.7960                   -0.0982                    0.9420                    2.8417                    0.1558                   -2.0755                          1.6485                          4.6858                          0.0890                         -3.9196                        -0.1769                        -0.0787                        -0.8296                         0.8449                   -4.0957                    -4.0355                       2.3904                      0.6880                     -1.9911                      3.4744                     -1.1011                     -0.7441                     -1.0648                     -3.2124                     -2.0274                     -3.8013                      0.9125                     -0.6288                     -4.0047                           -6.6524                            2.7566                           -0.3593                           -5.8488                           0.7140                          -2.0079                           3.3479                          -1.0843                  -1.1788                   -1.1615                      0.2878                    -0.2142                     0.0363                    -0.9246                    -0.9263                    -1.0941                     2.0136                    -0.1810                    -2.9036                          -1.9147                           3.8577                          -0.1034                          -4.7477                          0.2055                         -0.9068                          0.9636                          0.0168                   1.2750                    1.2562                     -0.3113                    -1.0815                     1.1833                     2.9399                     0.1958                    -1.9773                           2.0708                           4.7840                           0.1119                          -3.8214                         -0.2223                          0.0195                         -1.0422                          0.9431                   6.5130                    6.4173                     -1.5902                    -5.5250                     5.1084                     6.0448                          10.5787                           6.7613                           0.5714                          -1.8441                         -1.1354                          1.9968                         -5.3238                          2.9204                        11.3981                         11.2306                           -2.7829                          -9.6690                            8.940                           1.7500                                18.5133                               -1.9870                                3.8409                               -9.3170                                4.7645                       -1.2234                        -1.2054                           0.2987                          1.0378                         -0.9595                         -0.1878                               -0.1073                               0.2133\n",
      "4       0.0       3  35.0      1      1   8.0500         0            0              0              0              0              0              0              0              0              0            1               3               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               1                0                 0                   6                  3               0.1423      4         0           0             0           0         1          0          0          0         1                    0                     1                      0                       0            3           0           0        0        0        0        0        0        0        0        0        0        0        0        0         0       1         0         0         1        0                  0                   0                   1                 0           2                 0.1425                 -0.1842                 -0.1104                 -0.7737                  0.8654                    0.3178                    1.2194                    0.4484                   -0.4644                  -0.1680                  -0.0675                  -0.8483                   0.8225                  -0.1809                  -0.1016                  -0.7879                   0.8566                  -0.0285                   0.3020                  -4.9998                   0.4530                        -0.1588                        -0.0432                        -0.8974                         0.7981                       -0.1743                       -0.0843                       -0.8174                        0.8393                 -1.2926                   0.2381                    -0.4108                     0.3540                    -0.5795                    -1.3298                    0.2171                   -0.9329                    1.0965                   -0.0429                    0.2338                   -0.9670                    1.0185                   -0.0088                    0.0368                   -0.5634                    6.4625                   -0.4124                          0.2052                         -0.9086                          1.1599                         -0.0673                         0.2253                        -0.9497                         1.0565                        -0.0261                    2.2303                    -1.7255                       0.7088                     -0.3747                      0.3969                     -1.8919                      1.2869                     -0.4034                      0.3628                     -1.7573                      1.3210                     -0.0636                      0.7664                    -11.1511                      0.9174                           -0.3542                            0.4213                           -2.0014                            1.2626                          -0.3888                           0.3801                          -1.8231                           1.3037                  -1.1788                    0.9120                     -0.5286                     0.2132                    -0.9241                     0.9288                     0.0341                     0.0336                    -0.5205                     5.8940                    -0.3695                           0.1872                          -0.8657                           1.0579                          -0.0243                          0.2055                         -0.9068                          0.9636                          0.0168                  -1.2691                    0.9819                     -0.5690                     1.0766                     0.0362                    -0.5546                     6.3454                    -0.4036                           0.2015                          -0.8997                           1.1389                          -0.0584                          0.2212                         -0.9409                          1.0374                         -0.0173                  -0.2000                    0.1547                     -0.0897                     0.1697                     0.1576                     0.0057                           0.0318                          -0.4962                           0.1795                           0.3452                          0.0349                         -0.5373                          0.1635                          0.3863                        -1.1143                          0.8621                           -0.4996                           0.9453                            0.878                           5.5715                                 0.1769                                0.1943                               -0.8825                                0.9109                                0.0412                       -1.2234                         0.9465                          -0.5485                          1.0378                          0.9639                          6.1167                                1.0978                               0.2133\n",
      "[[   3.       22.        2.        1.        7.25      0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        5.        2.       -0.4123    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        1.\n",
      "     0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.3352\n",
      "     0.2914   -1.0823    1.1501   -0.0755   -0.4874    0.263    -0.6876   -1.4209    0.2576   -1.0239    1.301    -0.1339   -0.2786   -0.0976   -1.2029\n",
      "    -1.0602    0.0437   -0.6544    7.6679   -0.5034    0.2435   -0.9996    1.3763   -0.1583    0.7851   -1.935     0.4269    0.7771    0.8695    0.2534\n",
      "    -0.4238    0.3385   -0.5979   -1.3453    0.224    -0.9484    1.1312   -0.0584   -0.2423   -0.0221   -1.0459   -0.9847    0.038    -0.5789    6.6674\n",
      "    -0.4279    0.2118   -0.9241    1.1967   -0.0827    0.6826   -1.8595    0.3712    0.8527   -1.4542   -1.6725    0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869    0.4052    1.3232    1.7493    0.3606   -0.0636    0.7664  -11.1511    0.9174   -0.3542    0.4213   -2.0014    1.2626   -1.1417   -0.5141\n",
      "    -0.6209    2.198     0.7687    0.884    -0.5286   -0.2142    0.0363   -0.9246   -0.9263    0.0336   -0.5205    5.894    -0.3695    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.6034   -1.8011    0.3282    0.9111   -0.8313   -0.9561    0.5717   -1.0815   -0.0363    0.4058   -6.3746    0.5568   -0.2025\n",
      "     0.0606   -1.1441    0.9019   -0.6527   -0.8748   -0.3549    1.8374    0.1304    0.15     -0.0897    0.1697   -0.1569    0.0057    0.0318   -0.4962\n",
      "     0.1795    0.3452    0.1024   -1.4316    0.0557    1.2806    0.7266    0.8356   -0.4996    0.9453   -0.874     5.5715    0.1769    0.5704   -1.7767\n",
      "     0.3102    0.9354    2.3423    2.6938   -1.6107    3.0473   -2.8176   17.9609    3.2237    1.8389]\n",
      " [   1.       38.        2.        1.       71.2833    1.        1.        1.        0.        0.        0.        0.        0.        0.        0.\n",
      "    86.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        5.        1.       -0.418     7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     0.        0.        1.        2.        1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.        1.        0.        2.        0.3578\n",
      "     0.4395    1.3329    0.8142   -0.1365   -0.9249   -0.9479   -0.3869    2.1443   -0.2662    0.1532   -1.3443    1.0432    0.2879    1.0795    1.2429\n",
      "     0.1169    1.4708    3.0568    0.2433   -1.8604    1.701     3.4417    0.2104   -2.2452   -0.2763    0.1364   -1.2953    1.06      1.2282    0.5398\n",
      "    -1.1359   -0.8114   -0.4752    2.2808   -0.3269    0.2897   -1.651     1.1797    0.3536    1.216     1.5265    0.2534    1.8064    3.1933    0.2988\n",
      "    -1.7239    2.0891    3.5782    0.2584   -2.1088   -0.3393    0.2729   -1.5909    1.1965   -2.5846   -2.1044    2.3904    0.688    -1.9911    3.4744\n",
      "    -1.1011   -0.7441   -1.0648   -3.2124   -2.0274   -3.8013    0.9125   -0.6288   -4.0047   -4.3963    1.2974   -0.5437   -4.3896    0.714    -2.0079\n",
      "     3.3479   -1.0843   -0.7439   -0.6057    0.2878   -0.2142    0.0363   -0.9246   -0.9263   -1.0941    2.0136   -0.181    -2.9036   -1.2653    2.3985\n",
      "    -0.1565   -3.2885    0.2055   -0.9068    0.9636    0.0168    0.8046    0.6551   -0.3113   -1.0815    1.1833    2.9399    0.1958   -1.9773    1.3685\n",
      "     3.3247    0.1693   -2.3622   -0.2223    0.0195   -1.0422    0.9431    4.11      3.3464   -1.5902   -5.525     5.1084    6.0448    6.991     5.3021\n",
      "     0.8647   -0.3848   -1.1354    1.9968   -5.3238    2.9204    4.7533    3.8702   -1.8391   -6.3898    5.908     1.1565    8.0852   -1.3131    2.3816\n",
      "    -6.1572    3.3053   -0.772    -0.6286    0.2987    1.0378   -0.9595   -0.1878   -0.1624    0.2133]\n",
      " [   3.       26.        1.        1.        7.925     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        2.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.\n",
      "     0.        7.        3.        4.4371    3.        0.        0.        0.        1.        0.        0.        0.        0.        3.        0.\n",
      "     1.        0.        0.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.081\n",
      "     0.1396   -0.775     0.5805    0.2057   -0.2397    0.5573   -0.3381   -1.1266    0.1267   -0.7296    0.6397    0.1603    0.1364   -0.7637    0.5942\n",
      "     0.1944    0.2619   -1.2049    0.3093    0.6356    0.1197   -0.7053    0.6767    0.136     0.386    -1.6407    0.2099    1.0714    1.7226    0.2404\n",
      "    -0.4128    0.3516   -0.5824   -1.3323    0.2182   -0.9353    1.1019   -0.0453    0.2349   -0.9694    1.0235   -0.0113    0.4512   -1.4106    0.5329\n",
      "     0.4299    0.2063   -0.911     1.1657   -0.0697    0.6649   -1.8464    0.3616    0.8657   -2.9577   -1.717     0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869   -0.4034    0.3628   -1.7573    1.321    -0.7747   -0.0783   -0.9149    1.7621   -0.3542    0.4213   -2.0014    1.2626   -1.1417   -0.5141\n",
      "    -0.6209    2.198     1.5633    0.9075   -0.5286    0.2132   -0.9241    0.9288    0.0341    0.4095   -1.3652    0.4836    0.4752    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.6034   -1.8011    0.3282    0.9111    1.6831    0.977    -0.569     1.0766    0.4409   -1.3993    0.5206    0.4411    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.6497   -1.8352    0.3533    0.877     3.2328    1.8767   -1.093     2.0679    1.9208    0.8468    0.3871   -1.3409\n",
      "     2.1876   -0.4996    1.2479   -2.2763    0.6786    0.4359    1.4778    0.8579   -0.4996    0.9453    0.878     0.4571    0.1769    0.5704   -1.7767\n",
      "     0.3102    0.9354    4.764     2.7655   -1.6107    3.0473    2.8305    1.4736    3.2237    1.8389]\n",
      " [   1.       35.        2.        1.       53.1       0.        1.        1.        0.        0.        0.        0.        0.        0.        0.\n",
      "   124.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        1.       -0.2665    7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     0.        0.        1.        2.        0.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.        1.        0.        2.        0.1425\n",
      "     0.1446    0.7606    0.9853   -0.0056   -0.5836   -1.1686   -0.2442    1.9236   -0.168    -0.0675   -0.8483    0.8225    0.1817    0.8588    0.7843\n",
      "    -0.1038    0.9281    2.8361    0.1535   -2.0811    1.6242    4.6802    0.0877   -3.9252   -0.1743   -0.0843   -0.8174    0.8393    1.0149    0.1468\n",
      "    -0.5923   -1.163    -0.2478    1.9292   -0.1705   -0.0619   -0.861     0.8281    0.1844    0.8644    0.796    -0.0982    0.942     2.8417    0.1558\n",
      "    -2.0755    1.6485    4.6858    0.089    -3.9196   -0.1769   -0.0787   -0.8296    0.8449   -4.0957   -4.0355    2.3904    0.688    -1.9911    3.4744\n",
      "    -1.1011   -0.7441   -1.0648   -3.2124   -2.0274   -3.8013    0.9125   -0.6288   -4.0047   -6.6524    2.7566   -0.3593   -5.8488    0.714    -2.0079\n",
      "     3.3479   -1.0843   -1.1788   -1.1615    0.2878   -0.2142    0.0363   -0.9246   -0.9263   -1.0941    2.0136   -0.181    -2.9036   -1.9147    3.8577\n",
      "    -0.1034   -4.7477    0.2055   -0.9068    0.9636    0.0168    1.275     1.2562   -0.3113   -1.0815    1.1833    2.9399    0.1958   -1.9773    2.0708\n",
      "     4.784     0.1119   -3.8214   -0.2223    0.0195   -1.0422    0.9431    6.513     6.4173   -1.5902   -5.525     5.1084    6.0448   10.5787    6.7613\n",
      "     0.5714   -1.8441   -1.1354    1.9968   -5.3238    2.9204   11.3981   11.2306   -2.7829   -9.669     8.94      1.75     18.5133   -1.987     3.8409\n",
      "    -9.317     4.7645   -1.2234   -1.2054    0.2987    1.0378   -0.9595   -0.1878   -0.1073    0.2133]\n",
      " [   3.       35.        1.        1.        8.05      0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.1423    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.\n",
      "     1.        0.        0.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        1.        0.        2.        0.1425\n",
      "    -0.1842   -0.1104   -0.7737    0.8654    0.3178    1.2194    0.4484   -0.4644   -0.168    -0.0675   -0.8483    0.8225   -0.1809   -0.1016   -0.7879\n",
      "     0.8566   -0.0285    0.302    -4.9998    0.453    -0.1588   -0.0432   -0.8974    0.7981   -0.1743   -0.0843   -0.8174    0.8393   -1.2926    0.2381\n",
      "    -0.4108    0.354    -0.5795   -1.3298    0.2171   -0.9329    1.0965   -0.0429    0.2338   -0.967     1.0185   -0.0088    0.0368   -0.5634    6.4625\n",
      "    -0.4124    0.2052   -0.9086    1.1599   -0.0673    0.2253   -0.9497    1.0565   -0.0261    2.2303   -1.7255    0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869   -0.4034    0.3628   -1.7573    1.321    -0.0636    0.7664  -11.1511    0.9174   -0.3542    0.4213   -2.0014    1.2626   -0.3888    0.3801\n",
      "    -1.8231    1.3037   -1.1788    0.912    -0.5286    0.2132   -0.9241    0.9288    0.0341    0.0336   -0.5205    5.894    -0.3695    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.2055   -0.9068    0.9636    0.0168   -1.2691    0.9819   -0.569     1.0766    0.0362   -0.5546    6.3454   -0.4036    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.2212   -0.9409    1.0374   -0.0173   -0.2       0.1547   -0.0897    0.1697    0.1576    0.0057    0.0318   -0.4962\n",
      "     0.1795    0.3452    0.0349   -0.5373    0.1635    0.3863   -1.1143    0.8621   -0.4996    0.9453    0.878     5.5715    0.1769    0.1943   -0.8825\n",
      "     0.9109    0.0412   -1.2234    0.9465   -0.5485    1.0378    0.9639    6.1167    1.0978    0.2133]\n",
      " [   3.       29.796     1.        1.        8.4583    2.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.0753    3.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.\n",
      "     1.        0.        0.        3.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        1.        0.        2.        0.\n",
      "     0.0026   -0.4854    0.0112    0.4747   -0.0045    0.8365   -0.0064   -0.8473    0.0024   -0.4504    0.0121    0.4396    0.0026   -0.4845    0.0112\n",
      "     0.4737    0.0049   -0.9256    0.0058    0.9148    0.0023   -0.426     0.0128    0.4153    0.0025   -0.4672    0.0116    0.4564   89.3161    0.2304\n",
      "    -0.4041    0.3619   -0.5702   -1.322     0.2136   -0.925     1.0787   -0.035     0.23     -0.9591    1.002    -0.0009    0.4417   -1.4002    0.5217\n",
      "     0.4402    0.2019   -0.9007    1.1412   -0.0594    0.2217   -0.9418    1.0395   -0.0182 -156.6489   -1.7539    0.7088   -0.3747    0.3969   -1.8919\n",
      "     1.2869   -0.4034    0.3628   -1.7573    1.321    -0.7747   -0.0783   -0.9149    1.7621   -0.3542    0.4213   -2.0014    1.2626   -0.3888    0.3801\n",
      "    -1.8231    1.3037   82.7976    0.927    -0.5286    0.2132   -0.9241    0.9288    0.0341    0.4095   -1.3652    0.4836    0.4752    0.1872   -0.8657\n",
      "     1.0579   -0.0243    0.2055   -0.9068    0.9636    0.0168   89.14      0.998    -0.569     1.0766    0.4409   -1.3993    0.5206    0.4411    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.2212   -0.9409    1.0374   -0.0173  171.2159    1.917    -1.093     2.0679    1.9208    0.8468    0.3871   -1.3409\n",
      "     2.1876   -0.4996    0.425    -1.382     1.9926   -0.4584   78.2678    0.8763   -0.4996    0.9453    0.878     0.4571    0.1769    0.1943   -0.8825\n",
      "     0.9109    0.0412   85.926     0.962    -0.5485    1.0378    0.9639    0.5019    1.0978    0.2133]\n",
      " [   1.       54.        1.        1.       51.8625    0.        2.        0.        1.        0.        0.        0.        0.        0.        0.\n",
      "    47.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        5.        1.       -0.4182    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.\n",
      "     0.        0.        1.        2.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        1.        1.        0.        0.        0.        0.        0.        0.        1.        3.        3.1519\n",
      "     0.6377    2.1345    4.9426    1.4162   -2.7449    0.2292   -1.1483    3.3214   -0.79      1.3303   -3.9895    2.2203   -0.8505    1.2963   -3.7057\n",
      "     2.2544   -0.134     1.6998  -23.5142    1.8508    2.3893    3.1212    1.3192    0.4295    0.7677    2.2078    4.1054    1.3429    0.2023    0.129\n",
      "    -0.5554   -1.1869   -0.2323    1.9053   -0.1598   -0.0858   -0.8072    0.8042   -0.1721   -0.1199   -0.7498    0.8383   -0.0271    0.2837   -4.7575\n",
      "     0.4347    0.4834    1.705     0.2669   -0.9866    0.1553    0.7916    0.8306   -0.0732   -0.8709   -4.3043    2.3904    0.688    -1.9911    3.4744\n",
      "    -1.1011    0.7407   -2.0252    3.2272   -1.067     0.1167   -1.6216   20.4779   -1.4706   -2.0807   -0.2003   -1.1488   -2.8919   -0.6686   -1.1137\n",
      "    -3.5753   -1.9785   -0.2507   -1.2389    0.2878    0.2132   -0.9241    0.9288    0.0341    0.0336   -0.5205    5.894    -0.3695   -0.5989    0.9008\n",
      "    -0.3307   -1.7908   -0.1924   -0.0126   -1.029    -0.8774   -0.2699   -1.3338    0.3099    1.0766    0.0362   -0.5546    6.3454   -0.4036   -0.6448\n",
      "     0.8667   -0.356    -1.8249   -0.2072   -0.0466   -1.1079   -0.9115   -0.0425   -0.2102    0.0488    0.1697    0.1576    0.0057   -0.1016    1.2703\n",
      "    -0.0561   -1.4213   -0.0326    0.3569   -0.1746   -0.5079    0.7581    3.7467   -0.8705   -3.0243   -2.8091  -17.825     1.8112    0.582     1.7782\n",
      "     3.1121    0.9134    0.2436    1.2039   -0.2797   -0.9718   -0.9026   -5.7276    0.3213    0.187 ]\n",
      " [   3.        2.        4.        2.       21.075     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.1052    4.        0.        0.        1.        0.        0.        0.        0.        0.        4.        0.\n",
      "     0.        1.        0.        4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.        0.\n",
      "     0.        0.        0.        0.        1.        0.        0.        1.        1.        1.        0.        0.        0.        4.        4.204\n",
      "     0.4841   -2.2864    8.6848   -1.8143   -1.7262   -1.2084   -2.4354   -2.8923   -1.4573   -1.3396   -2.8847   -2.7611   -4.925     0.3517   -0.8536\n",
      "    -4.4524    0.1548   -2.1259   27.1568   -1.9749    0.8625   -2.471     4.8742   -1.6297   -2.7202   -0.7237   -1.5455   -3.377     0.1151    0.0557\n",
      "    -0.1988    0.6058   -0.2804   -1.078    -0.1678    0.4747   -0.3322   -0.9469   -0.5671    2.1659   -0.0983   -2.6381    0.0178   -0.3116    3.1269\n",
      "    -0.1606    0.0993   -0.6567    0.5612    0.1846   -0.3132    1.0906   -0.178    -1.5628   -0.4106   -3.5661    0.7088    0.5984    1.5527    1.1845\n",
      "     0.1312    2.0223    3.244     0.3505   -1.5601   -0.0636    0.7664  -11.1511    0.9174   -0.3542    0.4213   -2.0014    1.2626    1.117     2.1686\n",
      "     0.6346   -0.4848   -0.3467   -3.0106    0.8442    1.7073    3.1128    0.2959   -1.6913   -0.0537    0.6353   -9.414     0.7863   -0.299     0.2901\n",
      "    -1.6897    1.1314    0.943     2.0375    0.5357   -0.6159   -1.1715  -10.1744    2.8531    3.3795   -0.1814    2.3265  -31.8147    2.4775   -1.0104\n",
      "     1.9814   -5.7102    2.8227    3.1868    3.7287    1.8106    1.0753    0.0368    0.3198   -0.0897   -0.1062   -0.0314    0.0057    0.0318   -0.4962\n",
      "     0.1795    0.3452   -0.1002    1.2512   -0.0569   -1.4022    0.2052    1.7818   -0.4996   -0.5918   -0.1751    5.5715    0.1769   -0.5581    0.906\n",
      "    -0.3171   -1.7473   -0.6471   -5.6195    1.5758    1.8666    0.5523  -17.5718   -3.1539    1.7601]\n",
      " [   3.       27.        1.        3.       11.1333    0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        3.        0.1018    7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     1.        0.        0.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        1.        0.0446\n",
      "     0.0904   -0.6394    0.4928    0.2172   -0.1777    0.6308   -0.2507   -1.053    -0.394     1.6554   -0.1131   -2.0776    0.1011   -0.6902    0.4406\n",
      "     0.268    -0.519     2.2475   -0.0859   -2.6697    0.0888   -0.6317    0.5018    0.2096    0.2862   -1.5671    0.1557    1.145     2.0292    0.1835\n",
      "    -0.3606    0.4136   -0.5087   -1.2702   -0.7995    1.4382   -0.2295   -2.2948    0.2052   -0.9074    0.894     0.0508   -1.0531    2.0303   -0.1742\n",
      "    -2.8869    0.1802   -0.849     1.0182   -0.0077    0.5808   -1.7844    0.3158    0.9278   -3.9886   -1.9657    0.7088    1.5715    2.7084    0.4511\n",
      "    -1.0246   -0.4034    0.3628   -1.7573    1.321     2.07      3.3005    0.3424   -1.6167   -0.3542    0.4213   -2.0014    1.2626   -1.1417   -0.5141\n",
      "    -0.6209    2.198    -8.8428   -4.3579    2.217    -0.8942    1.3874   -3.896     2.3456    4.5891    4.3251    0.7592   -0.5921   -0.7852    1.4459\n",
      "    -4.4372    2.2872   -2.5311    0.5105   -1.3764    3.2226    2.2697    1.1185   -0.569    -0.2567   -1.1779    1.9795   -0.1949   -2.9377    0.2015\n",
      "    -0.8997    1.1389   -0.0584    0.6497   -1.8352    0.3533    0.877   -11.6479   -5.7403    2.9203    1.3172   -5.1319    6.0448   -1.0342    2.038\n",
      "    -5.8448    2.8793   -3.334     1.1026   -1.8131    3.8147    1.9929    0.9821   -0.4996   -0.2254    0.878    -0.1711    0.1769    0.5704   -1.7767\n",
      "     0.3102    0.9354    6.4244    3.1661   -1.6107   -0.7265    2.8305   -0.5516    3.2237    1.8389]\n",
      " [   2.       14.        2.        1.       30.0708    1.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.\n",
      "     0.        6.        2.       -0.0714    5.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.\n",
      "     0.        1.        0.        4.        1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        1.        0.        0.        1.        0.        0.        0.        4.        1.3631\n",
      "     0.0726   -1.2297   18.7849   -1.1054    0.4111   -1.5196    3.3159   -0.8154    0.5195   -1.6125    2.6236   -0.7225   -0.5619   -0.6862   -2.4258\n",
      "    -1.6488   -0.8981   -0.3983   -1.5178   -1.9367    0.4911   -1.5882    2.7755   -0.7469   -1.5489    0.1592   -0.88     -2.4942    0.0532    0.0039\n",
      "     0.0219   -0.4142    0.1765    0.2899    0.0277   -0.5072    0.1397    0.3828   -0.0299    0.4191   -0.1291   -0.5434   -0.0478    0.7071   -0.0808\n",
      "    -0.8314    0.0261   -0.4828    0.1477    0.3585   -0.0825    1.2645   -0.0468   -1.3888    0.3016    5.6651    0.124     0.1567   -0.7971    0.7912\n",
      "     0.0929   -0.1695    0.1292   -0.7316   -0.8334   -0.2708    0.4171   -0.4577   -1.1213    0.1481   -0.7727    0.837     0.0686   -0.4671    0.9746\n",
      "    -0.2654   -1.6788    0.3812    7.16      1.2639   -0.2142    0.0363   -0.9246   -0.9263   -0.3423    0.3242   -0.5785   -1.2142    0.1872   -0.8657\n",
      "     1.0579   -0.0243   -0.5904    0.8817   -0.3354   -1.7717   -0.4122   -7.7438   -1.3669   -1.0815    0.3702    1.2505    0.6257   -0.2879   -0.2025\n",
      "     0.0606   -1.1441    0.9019    0.6385    1.808     0.3628   -0.8454   -0.6588  -12.3764   -2.1847   -1.7286    1.5982    0.5917   -0.3236    0.3486\n",
      "    -1.8286    1.1899    1.0205    2.0959    0.5798   -0.5575    0.3603    6.7682    1.1947    0.9453   -0.874    -0.5469    0.1769   -0.5581    0.906\n",
      "    -0.3171   -1.7473   -1.1363  -21.3461   -3.768    -2.9813    2.7565    1.7248   -3.1539    1.7601]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31  reduced components which describe  .99 % of the variance\n",
      "['Survived' 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:491: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\anaconda\\envs\\tensorFlow\\lib\\site-packages\\ipykernel_launcher.py:492: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#打印设置，原始的threshold=10000,linewidth=160\n",
    "np.set_printoptions(precision=4, threshold=10000, linewidth=160, edgeitems=999, suppress=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 160)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "\n",
    "#构建二元特征\n",
    "def process_embarked():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #用众数代替缺失值\n",
    "    df_titanic_data.Embarked[df_titanic_data.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values\n",
    "\n",
    "    #将值转换为数字\n",
    "    df_titanic_data['Embarked'] = pd.factorize(df_titanic_data['Embarked'])[0]\n",
    "\n",
    "    #对构造的特征进行二值化\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat([df_titanic_data, pd.get_dummies(df_titanic_data['Embarked']).rename(\n",
    "            columns=lambda x: 'Embarked_' + str(x))], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#定义一个辅助函数，可以使用RandomForestClassifier（随机森林分类器）来处理age变量的缺失值\n",
    "def set_missing_ages():\n",
    "    global df_titanic_data\n",
    "\n",
    "    age_data = df_titanic_data[\n",
    "        ['Age', 'Embarked', 'Fare', 'Parch', 'SibSp', 'Title_id', 'Pclass', 'Names', 'CabinLetter']]\n",
    "    input_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 1::]\n",
    "    target_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 0]\n",
    "\n",
    "    #从sklearn的随机森林回归函数创建对象\n",
    "    regressor = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n",
    "\n",
    "    #根据上面的输入值和目标值构建模型\n",
    "    regressor.fit(input_values_RF, target_values_RF)\n",
    "\n",
    "    #使用训练的模型来预测缺失值\n",
    "    predicted_ages = regressor.predict(age_data.loc[(df_titanic_data.Age.isnull())].values[:, 1::])\n",
    "\n",
    "    #在原始的泰坦数据帧中填充预测的年龄\n",
    "    #(此处源代码age_data.loc[(age_data.Age.isnull()), 'Age'] = predicted_ages存在错误，导致运行结果出现空值，现在已经改正））\n",
    "    \n",
    "    df_titanic_data.loc[(df_titanic_data.Age.isnull()), 'Age'] = predicted_ages\n",
    "    \n",
    "\n",
    "#辅助函数用于从年龄变量构造特征\n",
    "def process_age():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #调用set_missing_ages辅助函数以使用随机森林回归来预测缺失的年龄值\n",
    "    set_missing_ages()\n",
    "\n",
    "    #通过以单位方差为中心围绕均值来缩放年龄变量\n",
    "    #(下面这段是执行命令，源代码将此处设为注释，此处已经改动)\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Age_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Age.reshape(-1, 1))\n",
    "\n",
    "    #为儿童构建一个特征\n",
    "    df_titanic_data['isChild'] = np.where(df_titanic_data.Age < 13, 1, 0)\n",
    "\n",
    "    #分成四分位数并创建二进制特征\n",
    "    df_titanic_data['Age_bin'] = pd.qcut(df_titanic_data['Age'], 4)\n",
    "\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Age_bin']).rename(columns=lambda y: 'Age_' + str(y))],\n",
    "            axis=1)\n",
    "\n",
    "    if keep_bins:\n",
    "        df_titanic_data['Age_bin_id'] = pd.factorize(df_titanic_data['Age_bin'])[0] + 1\n",
    "\n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Age_bin_id_scaled'] = scaler_processing.fit_transform(\n",
    "            df_titanic_data.Age_bin_id.reshape(-1, 1))\n",
    "      \n",
    "    \n",
    "    if not keep_strings:\n",
    "        df_titanic_data.drop('Age_bin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#辅助函数，用于构建乘客/机组人员姓名的特征\n",
    "def process_name():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #在names变量中获取不同的名称\n",
    "    df_titanic_data['Names'] = df_titanic_data['Name'].map(lambda y: len(re.split(' ', y)))\n",
    "\n",
    "    #获得每个人的头衔\n",
    "    df_titanic_data['Title'] = df_titanic_data['Name'].map(lambda y: re.compile(\", (.*?)\\.\").findall(y)[0])\n",
    "\n",
    "    #处理出现次数少的头衔\n",
    "    df_titanic_data['Title'][df_titanic_data.Title == 'Jonkheer'] = 'Master'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title.isin(['Ms', 'Mlle'])] = 'Miss'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title == 'Mme'] = 'Mrs'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\n",
    "    df_titanic_data['Title'][df_titanic_data.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n",
    "\n",
    "    #使所有特征二值化\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Title']).rename(columns=lambda x: 'Title_' + str(x))],\n",
    "            axis=1)\n",
    "\n",
    "    #缩放\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Names_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Names.reshape(-1, 1))\n",
    "\n",
    "    #分级\n",
    "    if keep_bins:\n",
    "        df_titanic_data['Title_id'] = pd.factorize(df_titanic_data['Title'])[0] + 1\n",
    "\n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Title_id_scaled'] = scaler.fit_transform(df_titanic_data.Title_id.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "#从客舱输入变量产生特征\n",
    "def process_cabin():\n",
    "#指的是包含泰坦尼克例子的全局变量\n",
    "    global df_titanic_data\n",
    "\n",
    "    #用U0来替换客舱变量的缺失值\n",
    "    df_titanic_data['Cabin'][df_titanic_data.Cabin.isnull()] = 'U0'\n",
    "\n",
    "    #客舱编号是一系列字母数字，因此我们将创建一些特征\n",
    "    #来自客舱编号字母的一部分\n",
    "    df_titanic_data['CabinLetter'] = df_titanic_data['Cabin'].map(lambda l: get_cabin_letter(l))\n",
    "    df_titanic_data['CabinLetter'] = pd.factorize(df_titanic_data['CabinLetter'])[0]\n",
    "\n",
    "    #将客舱字母特征二值化\n",
    "    if keep_binary:\n",
    "        cletters = pd.get_dummies(df_titanic_data['CabinLetter']).rename(columns=lambda x: 'CabinLetter_' + str(x))\n",
    "        df_titanic_data = pd.concat([df_titanic_data, cletters], axis=1)\n",
    "\n",
    "    #从客舱的数字侧创建特征\n",
    "    df_titanic_data['CabinNumber'] = df_titanic_data['Cabin'].map(lambda x: get_cabin_num(x)).astype(int) + 1\n",
    "\n",
    "    #缩放特征\n",
    "    if keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['CabinNumber_scaled'] = scaler_processing.fit_transform(df_titanic_data.CabinNumber.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def get_cabin_letter(cabin_value):\n",
    "#搜索客舱字母数字值中的字母\n",
    "    letter_match = re.compile(\"([a-zA-Z]+)\").search(cabin_value)\n",
    "    \n",
    "    if letter_match:\n",
    "        return letter_match.group()\n",
    "    else:\n",
    "        return 'U'\n",
    "\n",
    "\n",
    "def get_cabin_num(cabin_value):\n",
    "#搜索客舱字母数字值中的数字\n",
    "    number_match = re.compile(\"([0-9]+)\").search(cabin_value)\n",
    "\n",
    "    if number_match:\n",
    "        return number_match.group()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#用于从票价变量构造特征的辅助函数\n",
    "def process_fare():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #用票价的中位数来替换缺失值\n",
    "    df_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()\n",
    "\n",
    "    #票价中的0会导致一些分区问题，因此我们将它们设置为最低票价的十分之一\n",
    "    df_titanic_data['Fare'][np.where(df_titanic_data['Fare'] == 0)[0]] = df_titanic_data['Fare'][\n",
    "                                                                             df_titanic_data['Fare'].nonzero()[\n",
    "                                                                                 0]].min() / 10\n",
    "\n",
    "    #通过将特征分类为分位数来对特征进行二值化\n",
    "    df_titanic_data['Fare_bin'] = pd.qcut(df_titanic_data['Fare'], 4)\n",
    "\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Fare_bin']).rename(columns=lambda x: 'Fare_' + str(x))],\n",
    "            axis=1)\n",
    "\n",
    "    #分级\n",
    "    if keep_bins:\n",
    "        df_titanic_data['Fare_bin_id'] = pd.factorize(df_titanic_data['Fare_bin'])[0] + 1\n",
    "\n",
    "    #缩放值\n",
    "    if keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Fare_scaled'] = scaler_processing.fit_transform(df_titanic_data.Fare.reshape(-1, 1))\n",
    "\n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Fare_bin_id_scaled'] = scaler_processing.fit_transform(\n",
    "            df_titanic_data.Fare_bin_id.reshape(-1, 1))\n",
    "\n",
    "    if not keep_strings:\n",
    "        df_titanic_data.drop('Fare_bin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#辅助函数用于从票证变量中构建特征\n",
    "def process_ticket():\n",
    "    global df_titanic_data\n",
    "\n",
    "    df_titanic_data['TicketPrefix'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_prefix(y.upper()))\n",
    "    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('[\\.?\\/?]', '', y))\n",
    "    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('STON', 'SOTON', y))\n",
    "\n",
    "    df_titanic_data['TicketPrefixId'] = pd.factorize(df_titanic_data['TicketPrefix'])[0]\n",
    "\n",
    "    #每个票证层进行二值化\n",
    "    if keep_binary:\n",
    "        prefixes = pd.get_dummies(df_titanic_data['TicketPrefix']).rename(columns=lambda y: 'TicketPrefix_' + str(y))\n",
    "        df_titanic_data = pd.concat([df_titanic_data, prefixes], axis=1)\n",
    "\n",
    "    df_titanic_data.drop(['TicketPrefix'], axis=1, inplace=True)\n",
    "\n",
    "    df_titanic_data['TicketNumber'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_num(y))\n",
    "    df_titanic_data['TicketNumberDigits'] = df_titanic_data['TicketNumber'].map(lambda y: len(y)).astype(np.int)\n",
    "    df_titanic_data['TicketNumberStart'] = df_titanic_data['TicketNumber'].map(lambda y: y[0:1]).astype(np.int)\n",
    "\n",
    "    df_titanic_data['TicketNumber'] = df_titanic_data.TicketNumber.astype(np.int)\n",
    "\n",
    "    if keep_scaled:\n",
    "        scaler_processing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['TicketNumber_scaled'] = scaler_processing.fit_transform(\n",
    "            df_titanic_data.TicketNumber.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def get_ticket_prefix(ticket_value):\n",
    "#搜索票证字母数字值中的字母\n",
    "    match_letter = re.compile(\"([a-zA-Z\\.\\/]+)\").search(ticket_value)\n",
    "    if match_letter:\n",
    "        return match_letter.group()\n",
    "    else:\n",
    "        return 'U'\n",
    "\n",
    "\n",
    "def get_ticket_num(ticket_value):\n",
    "#搜索票证字母数字值中的数字\n",
    "\n",
    "    match_number = re.compile(\"([\\d]+$)\").search(ticket_value)\n",
    "    if match_number:\n",
    "        return match_number.group()\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "\n",
    "    #从乘客类变量中提取特征\n",
    "def process_PClass():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #用众数来替换缺失值\n",
    "    df_titanic_data.Pclass[df_titanic_data.Pclass.isnull()] = df_titanic_data.Pclass.dropna().mode().values\n",
    "\n",
    "    #对特征进行二值化\n",
    "    if keep_binary:\n",
    "        df_titanic_data = pd.concat(\n",
    "            [df_titanic_data, pd.get_dummies(df_titanic_data['Pclass']).rename(columns=lambda y: 'Pclass_' + str(y))],\n",
    "            axis=1)\n",
    "\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['Pclass_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Pclass.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    #基于SibSp和Parch等家庭变量构建特征\n",
    "def process_family():\n",
    "    global df_titanic_data\n",
    "\n",
    "    #确保在使用交互变量中没有零\n",
    "    df_titanic_data['SibSp'] = df_titanic_data['SibSp'] + 1\n",
    "    df_titanic_data['Parch'] = df_titanic_data['Parch'] + 1\n",
    "\n",
    "    #缩放\n",
    "    if keep_scaled:\n",
    "        scaler_preprocessing = preprocessing.StandardScaler()\n",
    "        df_titanic_data['SibSp_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.SibSp.reshape(-1, 1))\n",
    "        df_titanic_data['Parch_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Parch.reshape(-1, 1))\n",
    "\n",
    "    #对所有特征进行二值化\n",
    "    if keep_binary:\n",
    "        sibsps_var = pd.get_dummies(df_titanic_data['SibSp']).rename(columns=lambda y: 'SibSp_' + str(y))\n",
    "        parchs_var = pd.get_dummies(df_titanic_data['Parch']).rename(columns=lambda y: 'Parch_' + str(y))\n",
    "        df_titanic_data = pd.concat([df_titanic_data, sibsps_var, parchs_var], axis=1)\n",
    "\n",
    "\n",
    "   #将性别变量二值化\n",
    "def process_sex():\n",
    "    global df_titanic_data\n",
    "    df_titanic_data['Gender'] = np.where(df_titanic_data['Sex'] == 'male', 1, 0)\n",
    "\n",
    "\n",
    "   #删除掉原始的变量\n",
    "def process_drops():\n",
    "    global df_titanic_data\n",
    "    drops = ['Name', 'Names', 'Title', 'Sex', 'SibSp', 'Parch', 'Pclass', 'Embarked', \\\n",
    "             'Cabin', 'CabinLetter', 'CabinNumber', 'Age', 'Fare', 'Ticket', 'TicketNumber']\n",
    "    string_drops = ['Title', 'Name', 'Cabin', 'Ticket', 'Sex', 'Ticket', 'TicketNumber']\n",
    "    if not keep_raw:\n",
    "        df_titanic_data.drop(drops, axis=1, inplace=True)\n",
    "    elif not keep_strings:\n",
    "        df_titanic_data.drop(string_drops, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #处理所有特征工程\n",
    "def get_titanic_dataset(binary=False, bins=False, scaled=False, strings=False, raw=True, pca=False, balanced=False):\n",
    "    global keep_binary, keep_bins, keep_scaled, keep_raw, keep_strings, df_titanic_data\n",
    "    keep_binary = binary\n",
    "    keep_bins = bins\n",
    "    keep_scaled = scaled\n",
    "    keep_raw = raw\n",
    "    keep_strings = strings\n",
    "\n",
    "    #用Pandas读取训练集与测试集\n",
    "    train_data = pd.read_csv('data/train.csv', header=0)\n",
    "    test_data = pd.read_csv('data/test.csv', header=0)\n",
    "\n",
    "    #将训练集和测试集连接在一起，以完成整体特征工程\n",
    "    df_titanic_data = pd.concat([train_data, test_data])\n",
    "\n",
    "    #通过重新索引数据来移除训练集和测试集所导致的重复索引\n",
    "    df_titanic_data.reset_index(inplace=True)\n",
    "\n",
    "    #删除reset_index（）函数生成的索引列\n",
    "    df_titanic_data.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    #将列索引为从1开始的索引\n",
    "    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n",
    "\n",
    "    #使用我们在上面定义的辅助函数处理titanic原始变量\n",
    "    process_cabin()\n",
    "    process_ticket()\n",
    "    process_name()\n",
    "    process_fare()\n",
    "    process_embarked()\n",
    "    process_family()\n",
    "    process_sex()\n",
    "    process_PClass()\n",
    "    process_age()\n",
    "    process_drops()\n",
    "\n",
    "    #将servived列移动到第一个列\n",
    "    columns_list = list(df_titanic_data.columns.values)\n",
    "    columns_list.remove('Survived')\n",
    "    new_col_list = list(['Survived'])\n",
    "    new_col_list.extend(columns_list)\n",
    "    df_titanic_data = df_titanic_data.reindex(columns=new_col_list)\n",
    "    \n",
    "\n",
    "    print(\"Starting with\", df_titanic_data.columns.size,\n",
    "          \"manually constructing features based on the interaction between them...\\n\", df_titanic_data.columns.values)\n",
    "\n",
    "    #基于个体特征相互作用的特征构造\n",
    "    numeric_features = df_titanic_data.loc[:,\n",
    "                       ['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled',\n",
    "                        'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']]\n",
    "    print(\"\\nUsing only numeric features for automated feature generation:\\n\", numeric_features.head(10))\n",
    "\n",
    "    new_fields_count = 0\n",
    "    for i in range(0, numeric_features.columns.size - 1):\n",
    "        for j in range(0, numeric_features.columns.size - 1):\n",
    "            if i <= j:\n",
    "                name = str(numeric_features.columns.values[i]) + \"*\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] * numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                new_fields_count += 1\n",
    "            if i < j:\n",
    "                name = str(numeric_features.columns.values[i]) + \"+\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] + numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                new_fields_count += 1\n",
    "            if not i == j:\n",
    "                name = str(numeric_features.columns.values[i]) + \"/\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] / numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                name = str(numeric_features.columns.values[i]) + \"-\" + str(numeric_features.columns.values[j])\n",
    "                df_titanic_data = pd.concat(\n",
    "                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] - numeric_features.iloc[:, j], name=name)],\n",
    "                    axis=1)\n",
    "                new_fields_count += 2\n",
    "\n",
    "    print(\"\\n\", new_fields_count, \"new features constructed\")\n",
    "\n",
    "    #使用Spearman相关方法去除具有高相关性的特征\n",
    "\n",
    "    #计算相关矩阵\n",
    "    df_titanic_data_cor = df_titanic_data.drop(['Survived', 'PassengerId'], axis=1).corr(method='spearman')\n",
    "\n",
    "    #创建一个能忽略相关的掩码\n",
    "    mask_ignore = np.ones(df_titanic_data_cor.columns.size) - np.eye(df_titanic_data_cor.columns.size)\n",
    "    df_titanic_data_cor = mask_ignore * df_titanic_data_cor\n",
    "\n",
    "    features_to_drop = []\n",
    "\n",
    "    #删除相关的特征\n",
    "    for column in df_titanic_data_cor.columns.values:\n",
    "\n",
    "        #检查我们是否已决定删除此变量\n",
    "        if np.in1d([column], features_to_drop):\n",
    "            continue\n",
    "\n",
    "        #找到高度相关的变量\n",
    "        corr_vars = df_titanic_data_cor[abs(df_titanic_data_cor[column]) > 0.98].index\n",
    "        features_to_drop = np.union1d(features_to_drop, corr_vars)\n",
    "\n",
    "    print(\"\\nWe are going to drop\", features_to_drop.shape[0], \" which are highly correlated features...\\n\")\n",
    "    df_titanic_data.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    #拆分数据集以进行训练和测试并进行PCA\n",
    "    #(原始代码)train_data = df_titanic_data[:train_data.shape[0]]\n",
    "    #（原始代码）test_data = df_titanic_data[test_data.shape[0]:]\n",
    "    train_data = df_titanic_data[:train_data.shape[0]]\n",
    "    test_data = df_titanic_data[train_data.shape [0]:]\n",
    "\n",
    "\n",
    "    if pca:\n",
    "        print(\"reducing number of variables...\")\n",
    "        train_data, test_data = reduce(train_data, test_data)\n",
    "    else:\n",
    "        #删除在集合连接期间创建的测试集的空“Survived”列\n",
    "        test_data.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "    print(\"\\n\", train_data.columns.size, \"initial features generated...\\n\")  # , input_df.columns.values\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "   #将训练集和测试集降维\n",
    "    \n",
    "def reduce(train_data, test_data):\n",
    "    #将全部数据加到一起\n",
    "    df_titanic_data = pd.concat([train_data, test_data])\n",
    "    df_titanic_data.reset_index(inplace=True)\n",
    "    df_titanic_data.drop('index', axis=1, inplace=True)\n",
    "    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n",
    "\n",
    "    #将survived 这一列转化格式为series\n",
    "    #(此处 survived_series = pd.Series(df['Survived'], name='Survived')存在错误，已经改正）\n",
    "    survived_series = pd.Series(df_titanic_data['Survived'], name='Survived')\n",
    "\n",
    "    print(df_titanic_data.head())\n",
    "\n",
    "    #获取输入和目标值\n",
    "    input_values = df_titanic_data.values[:, 1::]\n",
    "    target_values = df_titanic_data.values[:, 0]\n",
    "\n",
    "    print(input_values[0:10])\n",
    "\n",
    "    #减少到的变量数量应涵盖的最小方差百分比（即为主成分的累积贡献率）\n",
    "    variance_percentage = .99\n",
    "\n",
    "    #创建PCA对象\n",
    "    pca_object = PCA(n_components=variance_percentage)\n",
    "\n",
    "    #特征转\n",
    "    input_values_transformed = pca_object.fit_transform(input_values, target_values)\n",
    "\n",
    "    #用PCA为变换后的变量创建数据帧\n",
    "    pca_df = pd.DataFrame(input_values_transformed)\n",
    "\n",
    "    print(pca_df.shape[1], \" reduced components which describe \", str(variance_percentage)[1:], \"% of the variance\")\n",
    "\n",
    "    #构建一个包含新减少的PCA变量的新数据帧\n",
    "    df_titanic_data = pd.concat([survived_series, pca_df], axis=1)\n",
    "\n",
    "    #再次分成单独的输入和测试集\n",
    "    train_data = df_titanic_data[:train_data.shape[0]]\n",
    "    test_data = df_titanic_data[train_data.shape[0]:]\n",
    "    test_data.reset_index(inplace=True)\n",
    "    test_data.drop('index', axis=1, inplace=True)\n",
    "    test_data.drop('Survived', axis=1, inplace=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#调用辅助函数\n",
    "if __name__ == '__main__':\n",
    "    train,test = get_titanic_dataset(bins=True, scaled=True, binary=True)\n",
    "    initial_drops = ['PassengerId']\n",
    "    train.drop(initial_drops, axis=1, inplace=True)\n",
    "    test.drop(initial_drops, axis=1, inplace=True)\n",
    "\n",
    "    train, test = reduce(train, test)\n",
    "\n",
    "    print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
