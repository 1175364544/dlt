{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、情感分析——模型实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、数据分析和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）导入所需模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "imdb.maybe_download_and_extract()     #下载并解压imdb数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the trainig set:  25000\n",
      "Size of the testing set:   25000\n"
     ]
    }
   ],
   "source": [
    "input_text_train, target_train = imdb.load_data(train=True)\n",
    "input_text_test,  target_test  = imdb.load_data(train=False)\n",
    "\n",
    "print(\"Size of the trainig set: \", len(input_text_train))\n",
    "print(\"Size of the testing set:  \", len(input_text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这个结果可以看到，这里训练数据与测试数据各有25000项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们将举一个例子来看看数据集的输入以及输出外观。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = input_text_train + input_text_test\n",
    "\n",
    "input_text_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的输出值为1，这意味着它是一个积极的情感。所以，无论是什么电影，这是一个积极的评论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （3）建立字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们讨论tokenizer，这是处理原始数据的第一步，因为神经网络不能处理文本数据。Keras实现了所谓的tokenizer，用于构建词汇表并从单词映射到整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10000\n",
    "tokenizer_obj = Tokenizer(num_words=num_top_words)     #使用Tokenizer建立单词数为10000的字典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们从数据集中获取所有文本，并在文本上调用函数fit，按照每一个单词在影评中出现的次数进行排序，前10000名的单词会列入字典中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj.fit_on_texts(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'br': 7,\n",
       " 'in': 8,\n",
       " 'it': 9,\n",
       " 'i': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'for': 15,\n",
       " 'with': 16,\n",
       " 'movie': 17,\n",
       " 'but': 18,\n",
       " 'film': 19,\n",
       " 'on': 20,\n",
       " 'not': 21,\n",
       " 'you': 22,\n",
       " 'are': 23,\n",
       " 'his': 24,\n",
       " 'have': 25,\n",
       " 'be': 26,\n",
       " 'one': 27,\n",
       " 'he': 28,\n",
       " 'all': 29,\n",
       " 'at': 30,\n",
       " 'by': 31,\n",
       " 'an': 32,\n",
       " 'they': 33,\n",
       " 'so': 34,\n",
       " 'who': 35,\n",
       " 'from': 36,\n",
       " 'like': 37,\n",
       " 'or': 38,\n",
       " 'just': 39,\n",
       " 'her': 40,\n",
       " 'out': 41,\n",
       " 'about': 42,\n",
       " 'if': 43,\n",
       " \"it's\": 44,\n",
       " 'has': 45,\n",
       " 'there': 46,\n",
       " 'some': 47,\n",
       " 'what': 48,\n",
       " 'good': 49,\n",
       " 'when': 50,\n",
       " 'more': 51,\n",
       " 'very': 52,\n",
       " 'up': 53,\n",
       " 'no': 54,\n",
       " 'time': 55,\n",
       " 'my': 56,\n",
       " 'even': 57,\n",
       " 'would': 58,\n",
       " 'she': 59,\n",
       " 'which': 60,\n",
       " 'only': 61,\n",
       " 'really': 62,\n",
       " 'see': 63,\n",
       " 'story': 64,\n",
       " 'their': 65,\n",
       " 'had': 66,\n",
       " 'can': 67,\n",
       " 'me': 68,\n",
       " 'well': 69,\n",
       " 'were': 70,\n",
       " 'than': 71,\n",
       " 'much': 72,\n",
       " 'we': 73,\n",
       " 'bad': 74,\n",
       " 'been': 75,\n",
       " 'get': 76,\n",
       " 'do': 77,\n",
       " 'great': 78,\n",
       " 'other': 79,\n",
       " 'will': 80,\n",
       " 'also': 81,\n",
       " 'into': 82,\n",
       " 'people': 83,\n",
       " 'because': 84,\n",
       " 'how': 85,\n",
       " 'first': 86,\n",
       " 'him': 87,\n",
       " 'most': 88,\n",
       " \"don't\": 89,\n",
       " 'made': 90,\n",
       " 'then': 91,\n",
       " 'its': 92,\n",
       " 'them': 93,\n",
       " 'make': 94,\n",
       " 'way': 95,\n",
       " 'too': 96,\n",
       " 'movies': 97,\n",
       " 'could': 98,\n",
       " 'any': 99,\n",
       " 'after': 100,\n",
       " 'think': 101,\n",
       " 'characters': 102,\n",
       " 'watch': 103,\n",
       " 'films': 104,\n",
       " 'two': 105,\n",
       " 'many': 106,\n",
       " 'seen': 107,\n",
       " 'character': 108,\n",
       " 'being': 109,\n",
       " 'never': 110,\n",
       " 'plot': 111,\n",
       " 'love': 112,\n",
       " 'acting': 113,\n",
       " 'life': 114,\n",
       " 'did': 115,\n",
       " 'best': 116,\n",
       " 'where': 117,\n",
       " 'know': 118,\n",
       " 'show': 119,\n",
       " 'little': 120,\n",
       " 'over': 121,\n",
       " 'off': 122,\n",
       " 'ever': 123,\n",
       " 'does': 124,\n",
       " 'your': 125,\n",
       " 'better': 126,\n",
       " 'end': 127,\n",
       " 'man': 128,\n",
       " 'scene': 129,\n",
       " 'still': 130,\n",
       " 'say': 131,\n",
       " 'these': 132,\n",
       " 'here': 133,\n",
       " 'scenes': 134,\n",
       " 'why': 135,\n",
       " 'while': 136,\n",
       " 'something': 137,\n",
       " 'such': 138,\n",
       " 'go': 139,\n",
       " 'through': 140,\n",
       " 'back': 141,\n",
       " 'should': 142,\n",
       " 'those': 143,\n",
       " 'real': 144,\n",
       " \"i'm\": 145,\n",
       " 'now': 146,\n",
       " 'watching': 147,\n",
       " 'thing': 148,\n",
       " \"doesn't\": 149,\n",
       " 'actors': 150,\n",
       " 'though': 151,\n",
       " 'funny': 152,\n",
       " 'years': 153,\n",
       " \"didn't\": 154,\n",
       " 'old': 155,\n",
       " '10': 156,\n",
       " 'another': 157,\n",
       " 'work': 158,\n",
       " 'before': 159,\n",
       " 'actually': 160,\n",
       " 'nothing': 161,\n",
       " 'makes': 162,\n",
       " 'look': 163,\n",
       " 'director': 164,\n",
       " 'find': 165,\n",
       " 'going': 166,\n",
       " 'same': 167,\n",
       " 'new': 168,\n",
       " 'lot': 169,\n",
       " 'every': 170,\n",
       " 'few': 171,\n",
       " 'again': 172,\n",
       " 'part': 173,\n",
       " 'cast': 174,\n",
       " 'down': 175,\n",
       " 'us': 176,\n",
       " 'things': 177,\n",
       " 'want': 178,\n",
       " 'quite': 179,\n",
       " 'pretty': 180,\n",
       " 'world': 181,\n",
       " 'horror': 182,\n",
       " 'around': 183,\n",
       " 'seems': 184,\n",
       " \"can't\": 185,\n",
       " 'young': 186,\n",
       " 'take': 187,\n",
       " 'however': 188,\n",
       " 'got': 189,\n",
       " 'thought': 190,\n",
       " 'big': 191,\n",
       " 'fact': 192,\n",
       " 'enough': 193,\n",
       " 'long': 194,\n",
       " 'both': 195,\n",
       " \"that's\": 196,\n",
       " 'give': 197,\n",
       " \"i've\": 198,\n",
       " 'own': 199,\n",
       " 'may': 200,\n",
       " 'between': 201,\n",
       " 'comedy': 202,\n",
       " 'right': 203,\n",
       " 'series': 204,\n",
       " 'action': 205,\n",
       " 'must': 206,\n",
       " 'music': 207,\n",
       " 'without': 208,\n",
       " 'times': 209,\n",
       " 'saw': 210,\n",
       " 'always': 211,\n",
       " 'original': 212,\n",
       " \"isn't\": 213,\n",
       " 'role': 214,\n",
       " 'come': 215,\n",
       " 'almost': 216,\n",
       " 'gets': 217,\n",
       " 'interesting': 218,\n",
       " 'guy': 219,\n",
       " 'point': 220,\n",
       " 'done': 221,\n",
       " \"there's\": 222,\n",
       " 'whole': 223,\n",
       " 'least': 224,\n",
       " 'far': 225,\n",
       " 'bit': 226,\n",
       " 'script': 227,\n",
       " 'minutes': 228,\n",
       " 'feel': 229,\n",
       " '2': 230,\n",
       " 'anything': 231,\n",
       " 'making': 232,\n",
       " 'might': 233,\n",
       " 'since': 234,\n",
       " 'am': 235,\n",
       " 'family': 236,\n",
       " \"he's\": 237,\n",
       " 'last': 238,\n",
       " 'probably': 239,\n",
       " 'tv': 240,\n",
       " 'performance': 241,\n",
       " 'kind': 242,\n",
       " 'away': 243,\n",
       " 'yet': 244,\n",
       " 'fun': 245,\n",
       " 'worst': 246,\n",
       " 'sure': 247,\n",
       " 'rather': 248,\n",
       " 'hard': 249,\n",
       " 'anyone': 250,\n",
       " 'girl': 251,\n",
       " 'each': 252,\n",
       " 'played': 253,\n",
       " 'day': 254,\n",
       " 'found': 255,\n",
       " 'looking': 256,\n",
       " 'woman': 257,\n",
       " 'screen': 258,\n",
       " 'although': 259,\n",
       " 'our': 260,\n",
       " 'especially': 261,\n",
       " 'believe': 262,\n",
       " 'having': 263,\n",
       " 'trying': 264,\n",
       " 'course': 265,\n",
       " 'dvd': 266,\n",
       " 'everything': 267,\n",
       " 'set': 268,\n",
       " 'goes': 269,\n",
       " 'comes': 270,\n",
       " 'put': 271,\n",
       " 'ending': 272,\n",
       " 'maybe': 273,\n",
       " 'place': 274,\n",
       " 'book': 275,\n",
       " 'shows': 276,\n",
       " 'three': 277,\n",
       " 'worth': 278,\n",
       " 'different': 279,\n",
       " 'main': 280,\n",
       " 'once': 281,\n",
       " 'sense': 282,\n",
       " 'american': 283,\n",
       " 'reason': 284,\n",
       " 'looks': 285,\n",
       " 'effects': 286,\n",
       " 'watched': 287,\n",
       " 'play': 288,\n",
       " 'true': 289,\n",
       " 'money': 290,\n",
       " 'actor': 291,\n",
       " \"wasn't\": 292,\n",
       " 'job': 293,\n",
       " 'together': 294,\n",
       " 'war': 295,\n",
       " 'someone': 296,\n",
       " 'plays': 297,\n",
       " 'instead': 298,\n",
       " 'high': 299,\n",
       " 'during': 300,\n",
       " 'said': 301,\n",
       " 'year': 302,\n",
       " 'half': 303,\n",
       " 'everyone': 304,\n",
       " 'later': 305,\n",
       " 'takes': 306,\n",
       " '1': 307,\n",
       " 'seem': 308,\n",
       " 'audience': 309,\n",
       " 'special': 310,\n",
       " 'beautiful': 311,\n",
       " 'left': 312,\n",
       " 'himself': 313,\n",
       " 'seeing': 314,\n",
       " 'john': 315,\n",
       " 'night': 316,\n",
       " 'black': 317,\n",
       " 'version': 318,\n",
       " 'shot': 319,\n",
       " 'excellent': 320,\n",
       " 'idea': 321,\n",
       " 'house': 322,\n",
       " 'mind': 323,\n",
       " 'star': 324,\n",
       " 'wife': 325,\n",
       " 'fan': 326,\n",
       " 'death': 327,\n",
       " 'used': 328,\n",
       " 'else': 329,\n",
       " 'simply': 330,\n",
       " 'nice': 331,\n",
       " 'budget': 332,\n",
       " 'poor': 333,\n",
       " 'short': 334,\n",
       " 'completely': 335,\n",
       " 'second': 336,\n",
       " \"you're\": 337,\n",
       " '3': 338,\n",
       " 'read': 339,\n",
       " 'less': 340,\n",
       " 'along': 341,\n",
       " 'top': 342,\n",
       " 'help': 343,\n",
       " 'home': 344,\n",
       " 'men': 345,\n",
       " 'either': 346,\n",
       " 'line': 347,\n",
       " 'boring': 348,\n",
       " 'dead': 349,\n",
       " 'friends': 350,\n",
       " 'kids': 351,\n",
       " 'try': 352,\n",
       " 'production': 353,\n",
       " 'enjoy': 354,\n",
       " 'camera': 355,\n",
       " 'use': 356,\n",
       " 'wrong': 357,\n",
       " 'given': 358,\n",
       " 'low': 359,\n",
       " 'classic': 360,\n",
       " 'father': 361,\n",
       " 'need': 362,\n",
       " 'full': 363,\n",
       " 'stupid': 364,\n",
       " 'next': 365,\n",
       " 'until': 366,\n",
       " 'performances': 367,\n",
       " 'school': 368,\n",
       " 'hollywood': 369,\n",
       " 'rest': 370,\n",
       " 'truly': 371,\n",
       " 'awful': 372,\n",
       " 'video': 373,\n",
       " 'couple': 374,\n",
       " 'start': 375,\n",
       " 'sex': 376,\n",
       " 'recommend': 377,\n",
       " 'women': 378,\n",
       " 'let': 379,\n",
       " 'tell': 380,\n",
       " 'terrible': 381,\n",
       " 'remember': 382,\n",
       " 'mean': 383,\n",
       " 'came': 384,\n",
       " 'getting': 385,\n",
       " 'understand': 386,\n",
       " 'perhaps': 387,\n",
       " 'moments': 388,\n",
       " 'name': 389,\n",
       " 'keep': 390,\n",
       " 'face': 391,\n",
       " 'itself': 392,\n",
       " 'wonderful': 393,\n",
       " 'playing': 394,\n",
       " 'human': 395,\n",
       " 'style': 396,\n",
       " 'small': 397,\n",
       " 'episode': 398,\n",
       " 'perfect': 399,\n",
       " 'others': 400,\n",
       " 'person': 401,\n",
       " 'doing': 402,\n",
       " 'often': 403,\n",
       " 'early': 404,\n",
       " 'stars': 405,\n",
       " 'definitely': 406,\n",
       " 'written': 407,\n",
       " 'head': 408,\n",
       " 'lines': 409,\n",
       " 'dialogue': 410,\n",
       " 'gives': 411,\n",
       " 'piece': 412,\n",
       " \"couldn't\": 413,\n",
       " 'went': 414,\n",
       " 'finally': 415,\n",
       " 'mother': 416,\n",
       " 'case': 417,\n",
       " 'title': 418,\n",
       " 'absolutely': 419,\n",
       " 'live': 420,\n",
       " 'boy': 421,\n",
       " 'yes': 422,\n",
       " 'laugh': 423,\n",
       " 'certainly': 424,\n",
       " 'liked': 425,\n",
       " 'become': 426,\n",
       " 'entertaining': 427,\n",
       " 'worse': 428,\n",
       " 'oh': 429,\n",
       " 'sort': 430,\n",
       " 'loved': 431,\n",
       " 'lost': 432,\n",
       " 'hope': 433,\n",
       " 'called': 434,\n",
       " 'picture': 435,\n",
       " 'felt': 436,\n",
       " 'overall': 437,\n",
       " 'entire': 438,\n",
       " 'mr': 439,\n",
       " 'several': 440,\n",
       " 'based': 441,\n",
       " 'supposed': 442,\n",
       " 'cinema': 443,\n",
       " 'friend': 444,\n",
       " 'guys': 445,\n",
       " 'sound': 446,\n",
       " '5': 447,\n",
       " 'problem': 448,\n",
       " 'drama': 449,\n",
       " 'against': 450,\n",
       " 'waste': 451,\n",
       " 'white': 452,\n",
       " 'beginning': 453,\n",
       " '4': 454,\n",
       " 'fans': 455,\n",
       " 'totally': 456,\n",
       " 'dark': 457,\n",
       " 'care': 458,\n",
       " 'direction': 459,\n",
       " 'humor': 460,\n",
       " 'wanted': 461,\n",
       " \"she's\": 462,\n",
       " 'seemed': 463,\n",
       " 'under': 464,\n",
       " 'game': 465,\n",
       " 'children': 466,\n",
       " 'despite': 467,\n",
       " 'lives': 468,\n",
       " 'lead': 469,\n",
       " 'guess': 470,\n",
       " 'example': 471,\n",
       " 'already': 472,\n",
       " 'final': 473,\n",
       " 'throughout': 474,\n",
       " \"you'll\": 475,\n",
       " 'evil': 476,\n",
       " 'turn': 477,\n",
       " 'becomes': 478,\n",
       " 'unfortunately': 479,\n",
       " 'able': 480,\n",
       " 'quality': 481,\n",
       " \"i'd\": 482,\n",
       " 'days': 483,\n",
       " 'history': 484,\n",
       " 'fine': 485,\n",
       " 'side': 486,\n",
       " 'wants': 487,\n",
       " 'heart': 488,\n",
       " 'horrible': 489,\n",
       " 'writing': 490,\n",
       " 'amazing': 491,\n",
       " 'b': 492,\n",
       " 'flick': 493,\n",
       " 'killer': 494,\n",
       " 'run': 495,\n",
       " 'son': 496,\n",
       " '\\x96': 497,\n",
       " 'michael': 498,\n",
       " 'works': 499,\n",
       " 'close': 500,\n",
       " \"they're\": 501,\n",
       " 'act': 502,\n",
       " 'art': 503,\n",
       " 'matter': 504,\n",
       " 'kill': 505,\n",
       " 'etc': 506,\n",
       " 'tries': 507,\n",
       " \"won't\": 508,\n",
       " 'past': 509,\n",
       " 'town': 510,\n",
       " 'turns': 511,\n",
       " 'enjoyed': 512,\n",
       " 'brilliant': 513,\n",
       " 'gave': 514,\n",
       " 'behind': 515,\n",
       " 'parts': 516,\n",
       " 'stuff': 517,\n",
       " 'genre': 518,\n",
       " 'eyes': 519,\n",
       " 'car': 520,\n",
       " 'favorite': 521,\n",
       " 'directed': 522,\n",
       " 'late': 523,\n",
       " 'hand': 524,\n",
       " 'expect': 525,\n",
       " 'soon': 526,\n",
       " 'hour': 527,\n",
       " 'obviously': 528,\n",
       " 'themselves': 529,\n",
       " 'sometimes': 530,\n",
       " 'killed': 531,\n",
       " 'actress': 532,\n",
       " 'thinking': 533,\n",
       " 'child': 534,\n",
       " 'girls': 535,\n",
       " 'viewer': 536,\n",
       " 'starts': 537,\n",
       " 'city': 538,\n",
       " 'myself': 539,\n",
       " 'decent': 540,\n",
       " 'highly': 541,\n",
       " 'stop': 542,\n",
       " 'type': 543,\n",
       " 'self': 544,\n",
       " 'god': 545,\n",
       " 'says': 546,\n",
       " 'group': 547,\n",
       " 'anyway': 548,\n",
       " 'voice': 549,\n",
       " 'took': 550,\n",
       " 'known': 551,\n",
       " 'blood': 552,\n",
       " 'kid': 553,\n",
       " 'heard': 554,\n",
       " 'happens': 555,\n",
       " 'except': 556,\n",
       " 'fight': 557,\n",
       " 'feeling': 558,\n",
       " 'experience': 559,\n",
       " 'coming': 560,\n",
       " 'slow': 561,\n",
       " 'daughter': 562,\n",
       " 'writer': 563,\n",
       " 'stories': 564,\n",
       " 'moment': 565,\n",
       " 'leave': 566,\n",
       " 'told': 567,\n",
       " 'extremely': 568,\n",
       " 'score': 569,\n",
       " 'violence': 570,\n",
       " 'police': 571,\n",
       " 'involved': 572,\n",
       " 'strong': 573,\n",
       " 'chance': 574,\n",
       " 'lack': 575,\n",
       " 'cannot': 576,\n",
       " 'hit': 577,\n",
       " 'roles': 578,\n",
       " 'hilarious': 579,\n",
       " 's': 580,\n",
       " 'wonder': 581,\n",
       " 'happen': 582,\n",
       " 'particularly': 583,\n",
       " 'ok': 584,\n",
       " 'including': 585,\n",
       " 'living': 586,\n",
       " 'save': 587,\n",
       " 'looked': 588,\n",
       " \"wouldn't\": 589,\n",
       " 'crap': 590,\n",
       " 'please': 591,\n",
       " 'simple': 592,\n",
       " 'murder': 593,\n",
       " 'cool': 594,\n",
       " 'obvious': 595,\n",
       " 'happened': 596,\n",
       " 'complete': 597,\n",
       " 'cut': 598,\n",
       " 'age': 599,\n",
       " 'serious': 600,\n",
       " 'gore': 601,\n",
       " 'attempt': 602,\n",
       " 'hell': 603,\n",
       " 'ago': 604,\n",
       " 'song': 605,\n",
       " 'shown': 606,\n",
       " 'taken': 607,\n",
       " 'english': 608,\n",
       " 'james': 609,\n",
       " 'robert': 610,\n",
       " 'david': 611,\n",
       " 'seriously': 612,\n",
       " 'released': 613,\n",
       " 'reality': 614,\n",
       " 'opening': 615,\n",
       " 'jokes': 616,\n",
       " 'interest': 617,\n",
       " 'across': 618,\n",
       " 'none': 619,\n",
       " 'hero': 620,\n",
       " 'today': 621,\n",
       " 'possible': 622,\n",
       " 'exactly': 623,\n",
       " 'alone': 624,\n",
       " 'sad': 625,\n",
       " 'brother': 626,\n",
       " 'number': 627,\n",
       " 'saying': 628,\n",
       " 'career': 629,\n",
       " \"film's\": 630,\n",
       " 'usually': 631,\n",
       " 'hours': 632,\n",
       " 'cinematography': 633,\n",
       " 'talent': 634,\n",
       " 'view': 635,\n",
       " 'yourself': 636,\n",
       " 'annoying': 637,\n",
       " 'running': 638,\n",
       " 'relationship': 639,\n",
       " 'documentary': 640,\n",
       " 'wish': 641,\n",
       " 'order': 642,\n",
       " 'huge': 643,\n",
       " 'whose': 644,\n",
       " 'shots': 645,\n",
       " 'ridiculous': 646,\n",
       " 'taking': 647,\n",
       " 'important': 648,\n",
       " 'light': 649,\n",
       " 'body': 650,\n",
       " 'middle': 651,\n",
       " 'level': 652,\n",
       " 'ends': 653,\n",
       " 'female': 654,\n",
       " 'started': 655,\n",
       " 'call': 656,\n",
       " \"i'll\": 657,\n",
       " 'husband': 658,\n",
       " 'four': 659,\n",
       " 'power': 660,\n",
       " 'major': 661,\n",
       " 'word': 662,\n",
       " 'turned': 663,\n",
       " 'opinion': 664,\n",
       " 'change': 665,\n",
       " 'mostly': 666,\n",
       " 'usual': 667,\n",
       " 'scary': 668,\n",
       " 'silly': 669,\n",
       " 'rating': 670,\n",
       " 'beyond': 671,\n",
       " 'somewhat': 672,\n",
       " 'ones': 673,\n",
       " 'happy': 674,\n",
       " 'words': 675,\n",
       " 'room': 676,\n",
       " 'knew': 677,\n",
       " 'knows': 678,\n",
       " 'country': 679,\n",
       " 'disappointed': 680,\n",
       " 'talking': 681,\n",
       " 'novel': 682,\n",
       " 'apparently': 683,\n",
       " 'non': 684,\n",
       " 'strange': 685,\n",
       " 'attention': 686,\n",
       " 'upon': 687,\n",
       " 'finds': 688,\n",
       " 'single': 689,\n",
       " 'basically': 690,\n",
       " 'cheap': 691,\n",
       " 'modern': 692,\n",
       " 'due': 693,\n",
       " 'jack': 694,\n",
       " 'television': 695,\n",
       " 'musical': 696,\n",
       " 'problems': 697,\n",
       " 'miss': 698,\n",
       " 'episodes': 699,\n",
       " 'clearly': 700,\n",
       " 'local': 701,\n",
       " '7': 702,\n",
       " 'british': 703,\n",
       " 'thriller': 704,\n",
       " 'talk': 705,\n",
       " 'events': 706,\n",
       " 'five': 707,\n",
       " 'sequence': 708,\n",
       " \"aren't\": 709,\n",
       " 'class': 710,\n",
       " 'french': 711,\n",
       " 'moving': 712,\n",
       " 'ten': 713,\n",
       " 'fast': 714,\n",
       " 'review': 715,\n",
       " 'earth': 716,\n",
       " 'tells': 717,\n",
       " 'predictable': 718,\n",
       " 'team': 719,\n",
       " 'songs': 720,\n",
       " 'comic': 721,\n",
       " 'straight': 722,\n",
       " '8': 723,\n",
       " 'whether': 724,\n",
       " 'die': 725,\n",
       " 'add': 726,\n",
       " 'dialog': 727,\n",
       " 'entertainment': 728,\n",
       " 'above': 729,\n",
       " 'sets': 730,\n",
       " 'future': 731,\n",
       " 'enjoyable': 732,\n",
       " 'appears': 733,\n",
       " 'near': 734,\n",
       " 'space': 735,\n",
       " 'easily': 736,\n",
       " 'hate': 737,\n",
       " 'soundtrack': 738,\n",
       " 'bring': 739,\n",
       " 'giving': 740,\n",
       " 'lots': 741,\n",
       " 'similar': 742,\n",
       " 'romantic': 743,\n",
       " 'george': 744,\n",
       " 'supporting': 745,\n",
       " 'release': 746,\n",
       " 'mention': 747,\n",
       " 'within': 748,\n",
       " 'filmed': 749,\n",
       " 'message': 750,\n",
       " 'sequel': 751,\n",
       " 'clear': 752,\n",
       " 'falls': 753,\n",
       " 'needs': 754,\n",
       " \"haven't\": 755,\n",
       " 'dull': 756,\n",
       " 'suspense': 757,\n",
       " 'bunch': 758,\n",
       " 'eye': 759,\n",
       " 'surprised': 760,\n",
       " 'showing': 761,\n",
       " 'tried': 762,\n",
       " 'sorry': 763,\n",
       " 'certain': 764,\n",
       " 'working': 765,\n",
       " 'easy': 766,\n",
       " 'ways': 767,\n",
       " 'theme': 768,\n",
       " 'theater': 769,\n",
       " 'named': 770,\n",
       " 'among': 771,\n",
       " \"what's\": 772,\n",
       " 'storyline': 773,\n",
       " 'monster': 774,\n",
       " 'king': 775,\n",
       " 'stay': 776,\n",
       " 'effort': 777,\n",
       " 'fall': 778,\n",
       " 'stand': 779,\n",
       " 'minute': 780,\n",
       " 'gone': 781,\n",
       " 'rock': 782,\n",
       " 'using': 783,\n",
       " '9': 784,\n",
       " 'feature': 785,\n",
       " 'comments': 786,\n",
       " 'buy': 787,\n",
       " \"'\": 788,\n",
       " 'typical': 789,\n",
       " 't': 790,\n",
       " 'editing': 791,\n",
       " 'sister': 792,\n",
       " 'avoid': 793,\n",
       " 'tale': 794,\n",
       " 'mystery': 795,\n",
       " 'deal': 796,\n",
       " 'dr': 797,\n",
       " 'doubt': 798,\n",
       " 'fantastic': 799,\n",
       " 'kept': 800,\n",
       " 'nearly': 801,\n",
       " 'feels': 802,\n",
       " 'okay': 803,\n",
       " 'subject': 804,\n",
       " 'viewing': 805,\n",
       " 'elements': 806,\n",
       " 'oscar': 807,\n",
       " 'check': 808,\n",
       " 'realistic': 809,\n",
       " 'points': 810,\n",
       " 'greatest': 811,\n",
       " 'means': 812,\n",
       " 'herself': 813,\n",
       " 'parents': 814,\n",
       " 'famous': 815,\n",
       " 'imagine': 816,\n",
       " 'rent': 817,\n",
       " 'viewers': 818,\n",
       " 'richard': 819,\n",
       " 'crime': 820,\n",
       " 'form': 821,\n",
       " 'peter': 822,\n",
       " 'actual': 823,\n",
       " 'lady': 824,\n",
       " 'general': 825,\n",
       " 'dog': 826,\n",
       " 'follow': 827,\n",
       " 'believable': 828,\n",
       " 'period': 829,\n",
       " 'red': 830,\n",
       " 'move': 831,\n",
       " 'brought': 832,\n",
       " 'material': 833,\n",
       " 'forget': 834,\n",
       " 'somehow': 835,\n",
       " 'begins': 836,\n",
       " 're': 837,\n",
       " 'reviews': 838,\n",
       " 'animation': 839,\n",
       " 'paul': 840,\n",
       " \"you've\": 841,\n",
       " 'leads': 842,\n",
       " 'weak': 843,\n",
       " 'figure': 844,\n",
       " 'surprise': 845,\n",
       " 'hear': 846,\n",
       " 'sit': 847,\n",
       " 'average': 848,\n",
       " 'open': 849,\n",
       " 'sequences': 850,\n",
       " 'atmosphere': 851,\n",
       " 'killing': 852,\n",
       " 'eventually': 853,\n",
       " 'tom': 854,\n",
       " 'learn': 855,\n",
       " 'premise': 856,\n",
       " '20': 857,\n",
       " 'wait': 858,\n",
       " 'sci': 859,\n",
       " 'deep': 860,\n",
       " 'fi': 861,\n",
       " 'expected': 862,\n",
       " 'whatever': 863,\n",
       " 'indeed': 864,\n",
       " 'particular': 865,\n",
       " 'poorly': 866,\n",
       " 'note': 867,\n",
       " 'lame': 868,\n",
       " 'dance': 869,\n",
       " 'imdb': 870,\n",
       " 'situation': 871,\n",
       " 'shame': 872,\n",
       " 'third': 873,\n",
       " 'york': 874,\n",
       " 'box': 875,\n",
       " 'truth': 876,\n",
       " 'decided': 877,\n",
       " 'free': 878,\n",
       " 'hot': 879,\n",
       " \"who's\": 880,\n",
       " 'difficult': 881,\n",
       " 'needed': 882,\n",
       " 'season': 883,\n",
       " 'acted': 884,\n",
       " 'leaves': 885,\n",
       " 'unless': 886,\n",
       " 'possibly': 887,\n",
       " 'emotional': 888,\n",
       " 'romance': 889,\n",
       " 'gay': 890,\n",
       " 'sexual': 891,\n",
       " 'boys': 892,\n",
       " 'footage': 893,\n",
       " 'write': 894,\n",
       " 'western': 895,\n",
       " 'forced': 896,\n",
       " 'credits': 897,\n",
       " 'reading': 898,\n",
       " 'memorable': 899,\n",
       " 'became': 900,\n",
       " 'doctor': 901,\n",
       " 'otherwise': 902,\n",
       " 'crew': 903,\n",
       " 'begin': 904,\n",
       " 'air': 905,\n",
       " 'de': 906,\n",
       " 'question': 907,\n",
       " 'society': 908,\n",
       " 'meet': 909,\n",
       " 'male': 910,\n",
       " 'meets': 911,\n",
       " \"let's\": 912,\n",
       " 'plus': 913,\n",
       " 'cheesy': 914,\n",
       " 'hands': 915,\n",
       " 'superb': 916,\n",
       " 'screenplay': 917,\n",
       " 'interested': 918,\n",
       " 'beauty': 919,\n",
       " 'street': 920,\n",
       " 'features': 921,\n",
       " 'masterpiece': 922,\n",
       " 'perfectly': 923,\n",
       " 'whom': 924,\n",
       " 'laughs': 925,\n",
       " 'nature': 926,\n",
       " 'stage': 927,\n",
       " 'effect': 928,\n",
       " 'forward': 929,\n",
       " 'comment': 930,\n",
       " 'nor': 931,\n",
       " 'previous': 932,\n",
       " 'badly': 933,\n",
       " 'sounds': 934,\n",
       " 'e': 935,\n",
       " 'japanese': 936,\n",
       " 'weird': 937,\n",
       " 'island': 938,\n",
       " 'personal': 939,\n",
       " 'inside': 940,\n",
       " 'quickly': 941,\n",
       " 'total': 942,\n",
       " 'keeps': 943,\n",
       " 'towards': 944,\n",
       " 'result': 945,\n",
       " 'america': 946,\n",
       " 'battle': 947,\n",
       " 'crazy': 948,\n",
       " 'worked': 949,\n",
       " 'setting': 950,\n",
       " 'incredibly': 951,\n",
       " 'background': 952,\n",
       " 'earlier': 953,\n",
       " 'mess': 954,\n",
       " 'cop': 955,\n",
       " 'writers': 956,\n",
       " 'fire': 957,\n",
       " 'copy': 958,\n",
       " 'dumb': 959,\n",
       " 'unique': 960,\n",
       " 'realize': 961,\n",
       " 'powerful': 962,\n",
       " 'lee': 963,\n",
       " 'mark': 964,\n",
       " 'business': 965,\n",
       " 'rate': 966,\n",
       " 'dramatic': 967,\n",
       " 'older': 968,\n",
       " 'pay': 969,\n",
       " 'following': 970,\n",
       " 'directors': 971,\n",
       " 'girlfriend': 972,\n",
       " 'joke': 973,\n",
       " 'plenty': 974,\n",
       " 'directing': 975,\n",
       " 'various': 976,\n",
       " 'creepy': 977,\n",
       " 'baby': 978,\n",
       " 'development': 979,\n",
       " 'appear': 980,\n",
       " 'brings': 981,\n",
       " 'front': 982,\n",
       " 'ask': 983,\n",
       " 'dream': 984,\n",
       " 'water': 985,\n",
       " 'rich': 986,\n",
       " 'admit': 987,\n",
       " 'bill': 988,\n",
       " 'apart': 989,\n",
       " 'joe': 990,\n",
       " 'fairly': 991,\n",
       " 'political': 992,\n",
       " 'leading': 993,\n",
       " 'reasons': 994,\n",
       " 'portrayed': 995,\n",
       " 'spent': 996,\n",
       " 'telling': 997,\n",
       " 'cover': 998,\n",
       " 'outside': 999,\n",
       " 'fighting': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index  #字典数据类型，显示每一个单词单词在所有文章中出现的次数的排名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，每个单词都与一个整数相关联"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，the单词是数字1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，and是数字2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index['and']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单词a是数字3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到movie是数字17："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Film是数字19："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index['film']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这意味着the是数据集中使用最多的词，而and是数据集中使用第二多的词。因此，每当我们想要将单词映射到整数tokens时，我们就会得到这些数字。\n",
    "让我们试着以数字743为例，这是单词romantic：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.word_index['romantic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，每当我们在输入文本中看到单词romantic时，我们就将它映射到token整数743。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们再次使用tokenizer将训练集中第一个文本中的所有单词转换为整数tokens，指令及结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_train[1]   #输入训练集中第一个文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_tokens = tokenizer_obj.texts_to_sequences(input_text_train)   #将文本转换为整数tokens时，它将变成一个整数数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([299, 6, 3, 1059, 202, 9, 2119, 30, 1, 167, 55, 14, 47, 79, 6274, 42, 368, 114, 138, 14, 5103, 56, 4515, 153, 8, 1, 4233, 5799, 469, 68, 5, 262, 12, 2072, 6, 72, 2556, 5, 614, 71, 6, 5103, 1, 5, 1897, 1, 5540, 1469, 35, 67, 63, 203, 140, 65, 1151, 1, 4, 1, 223, 871, 29, 3195, 68, 4, 1, 5510, 10, 677, 2, 65, 1469, 50, 10, 210, 1, 398, 8, 60, 3, 1425, 3345, 762, 5, 3491, 175, 1, 368, 10, 1220, 30, 299, 3, 360, 347, 3471, 145, 133, 5, 8306, 27, 4, 125, 5103, 1425, 2563, 5, 299, 10, 525, 12, 106, 1540, 4, 56, 599, 101, 12, 299, 6, 225, 3994, 48, 3, 2244, 12, 9, 213]),\n",
       "       list([38, 14, 744, 3506, 45, 75, 32, 1771, 15, 153, 18, 110, 3, 1344, 5, 343, 143, 20, 1, 920, 12, 70, 281, 1228, 395, 35, 115, 267, 36, 166, 5, 368, 158, 38, 2058, 15, 1, 504, 88, 83, 101, 4, 1, 4339, 14, 39, 3, 432, 1148, 136, 8697, 42, 177, 138, 14, 2791, 1, 295, 20, 5276, 351, 5, 3029, 2310, 1, 38, 8697, 43, 3611, 26, 365, 5, 127, 53, 20, 1, 2032, 7, 7, 18, 48, 43, 22, 70, 358, 3, 2343, 5, 420, 20, 1, 2032, 15, 3, 3346, 208, 1, 22, 281, 66, 36, 3, 344, 1, 728, 730, 3, 3864, 1320, 20, 1, 1543, 3, 1293, 2, 267, 22, 281, 2734, 5, 63, 48, 44, 37, 5, 26, 4339, 12, 6, 2079, 7, 7, 3425, 2891, 35, 4446, 35, 405, 14, 297, 3, 986, 128, 35, 45, 267, 8, 1, 181, 366, 6951, 5, 94, 3, 2343, 16, 3, 7017, 3090, 5, 63, 43, 28, 67, 420, 8, 1, 2032, 15, 3082, 483, 208, 1, 43, 2802, 28, 67, 77, 48, 28, 487, 16, 3, 731, 1146, 4, 232, 51, 4161, 1, 20, 117, 6, 1334, 20, 1, 920, 16, 3, 20, 24, 4086, 5, 24, 170, 831, 117, 28, 185, 1562, 122, 1, 7951, 237, 358, 1, 31, 3, 100, 44, 407, 20, 24, 9597, 117, 911, 79, 102, 585, 3, 257, 31, 1, 389, 4, 5176, 2137, 4636, 32, 1222, 3303, 35, 189, 4287, 159, 2320, 40, 344, 2, 40, 8527, 6229, 1955, 4910, 2, 7720, 2618, 35, 23, 472, 328, 5, 1, 2032, 501, 4392, 213, 237, 21, 328, 5, 4805, 6768, 37, 28, 281, 115, 50, 109, 986, 117, 44, 557, 38, 2574, 505, 38, 26, 531, 7, 7, 136, 1, 112, 1906, 201, 5176, 2, 292, 1731, 5, 111, 10, 255, 114, 4541, 5, 26, 27, 4, 3425, 104, 117, 2557, 5, 109, 3, 202, 9, 276, 3, 4317, 486, 1107, 5, 24, 2347, 158, 138, 14, 8161, 186, 3889, 38, 15, 1, 504, 5, 119, 48, 44, 37, 263, 137, 4737, 159, 2320, 9, 1, 365, 254, 38, 20, 1, 79, 524, 232, 3, 364, 2343, 37, 29, 986, 83, 77, 50, 33, 89, 118, 48, 5, 77, 16, 65, 290, 273, 33, 142, 197, 9, 5, 1, 4339, 298, 4, 783, 9, 37, 290, 7, 7, 38, 273, 11, 19, 80, 5541, 22, 5, 343, 400]),\n",
       "       list([513, 121, 113, 31, 2137, 4636, 116, 967, 824, 10, 25, 123, 107, 2, 112, 134, 8, 1688, 7418, 23, 336, 5, 619, 1, 5398, 20, 391, 6, 3, 360, 14, 49, 14, 231, 8, 8161, 1, 187, 20, 9106, 6, 81, 916, 100, 109, 3478, 4, 109, 3, 3157, 41, 24, 1488, 2, 109, 1, 2792, 4, 145, 3, 2792, 28, 546, 277, 152, 675, 4423, 3, 521, 36, 1, 305, 2354, 6639, 119, 6, 799, 133, 96, 14, 3, 1121, 6056, 35, 487, 5, 4709, 1, 6769, 24, 108, 6, 51, 71, 667, 1, 1447, 129, 2, 1, 129, 117, 1, 4339, 3, 2060, 23, 29, 55, 2112, 163, 15, 1, 3127, 129, 2, 1, 105, 191, 1000, 27, 11, 17, 217, 126, 252, 55, 10, 63, 9, 60, 6, 179, 403]),\n",
       "       ...,\n",
       "       list([10, 210, 238, 316, 30, 1, 19, 1398, 2, 9, 13, 27, 643, 1415, 1415, 84, 1, 773, 13, 4346, 962, 1, 9432, 4, 314, 6094, 8, 3, 4180, 4508, 17, 13, 1141, 2, 109, 3, 326, 934, 145, 21, 4, 1473, 4, 1, 1733, 10, 13, 3132, 5, 131, 52, 2120, 5, 808, 11, 17, 41, 56, 1237, 912, 1240, 5, 1, 2112, 422, 1, 773, 45, 1050, 18, 1, 3312, 23, 1054, 1, 150, 2760, 57, 6094, 6, 1957, 47, 58, 131, 12, 44, 3, 203, 2708, 4, 1, 1123, 181, 8, 1, 176, 12, 1, 9961, 4, 1, 102, 2755, 3, 1272, 2, 29, 12, 3708, 18, 9, 39, 162, 1, 223, 17, 39, 37, 1, 129, 117, 6094, 217, 3433, 9, 612, 1514, 3061, 10, 292, 1012, 231, 396, 18, 130, 709, 73, 1343, 5, 229, 5174, 15, 40, 10, 154, 21, 15, 3, 780, 59, 13, 34, 868, 29, 1, 95, 2, 10, 339, 12, 1, 1304, 13, 1186, 69, 9, 6, 49, 864, 18, 161, 1502, 2171, 346, 10, 206, 987, 12, 1, 2701, 30, 1, 1398, 292, 34, 49, 34, 273, 10, 1055, 41, 137, 133, 18, 30, 1, 127, 4, 1, 17, 10, 413, 343, 533, 7210, 229, 37, 147, 9324, 172, 3910, 51, 612, 1, 86, 129, 8, 1, 1393, 6, 1278, 319, 2, 29, 18, 10, 66, 1, 3037, 7561, 4, 147, 3, 1150, 2, 51, 348, 318, 4, 1, 129, 8, 1, 6738, 1484, 734, 1, 283, 2368, 4109, 8, 7358, 9071, 957, 1194, 16, 1, 2850, 830, 2, 1205, 1509, 1, 1269, 207, 1, 825, 851, 54, 10, 39, 413, 76, 82, 11, 17, 96, 74]),\n",
       "       list([47, 104, 12, 22, 1185, 53, 15, 3, 6677, 477, 41, 5, 26, 248, 49, 1064, 104, 613, 4245, 4, 3805, 1049, 2, 283, 17, 12, 70, 78, 18, 259, 613, 47, 3418, 104, 2829, 400, 1, 6627, 4, 65, 9097, 6, 590, 37, 1, 1025, 6712, 7, 7, 1, 61, 1163, 148, 10, 67, 131, 42, 11, 19, 6, 12, 44, 1273, 734, 14, 637, 14, 1, 7943, 4, 2094, 79, 71, 12, 92, 3, 52, 4046, 7710, 19, 16, 1, 1890, 4, 3, 322, 2146, 31, 3, 7, 7, 44, 75, 3145, 3320, 18, 22, 62, 77, 25, 5, 3100, 42, 3, 19, 12, 5568, 4, 1, 205, 82, 92, 1098, 708, 34, 31, 1, 55, 9, 217, 5, 1, 1025, 2904, 841, 107, 9, 29, 472, 7, 7, 74, 17, 455, 80, 25, 3, 1964, 147, 1, 3079, 1726, 2448, 2649, 2, 1, 9769, 113, 4, 1, 174, 261, 1, 35, 13, 34, 74, 28, 115, 94, 68, 423, 3, 226, 2, 3495, 5, 15, 385, 8, 5, 1, 1174, 4, 177, 31, 7327, 3, 2549, 34, 74, 9, 436, 37, 10, 13, 147, 47, 1487, 53, 344, 373, 4, 3, 7333, 1231, 7, 7, 372, 372, 517, 2019, 29, 90, 517, 37, 11, 50, 2019, 1944, 3, 1075, 4, 3, 355, 18, 1114, 282, 2, 132, 104, 8, 260, 1219, 793]),\n",
       "       list([11, 6, 27, 4, 1, 7047, 104, 198, 123, 107, 9, 7576, 122, 801, 123, 543, 4, 704, 2, 1019, 5, 94, 3, 954, 4, 93, 29, 7, 7, 222, 21, 3, 689, 49, 347, 38, 108, 8, 1, 223, 954, 43, 46, 13, 3, 111, 9, 13, 32, 2, 14, 225, 14, 113, 269, 222, 161, 49, 5, 131, 34, 1811, 131, 161, 10, 1230, 2192, 386, 85, 11, 543, 4, 1832, 217, 1034, 2, 160, 613, 124, 1851, 1219, 21, 30, 47, 927, 101, 56, 545, 11, 62, 6, 3, 3609, 4, 2, 656, 9, 3, 254, 92, 590, 37, 11, 12, 45, 83, 1, 1519, 285, 37, 3, 335, 279, 19, 30, 224, 43, 22, 25, 9, 22, 755, 1002, 125, 55, 38, 290, 89, 451, 125, 55, 11, 6, 1377])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，单词homelessness变成了数字299，单词or变成了数字6，依此类推。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们还需要转换文本的其余部分，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_tokens = tokenizer_obj.texts_to_sequences(input_text_test)  #将文本转换为数字列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （4）数字列表截长补短"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        现在有另一个问题，因为tokens序列的长度取决于原始文本的长度，即使循环单元可以处理任意长度的序列。但是TensorFlow的工作方式是，批处理中的所有数据都需要具有相同的长度。\n",
    "        因此，我们需要确保整个数据集中的所有序列都具有相同的长度，或者编写一个自定义数据生成器，以确保单个批处理中的序列具有相同的长度。现在，要确保数据集中的所有序列都具有相同的长度也比较简单，但问题是存在一些异常值。假定我们认为超过2200个单词的句子太长，如果我们有超过2200个单词的句字，那么我们的记忆就会受到很大的伤害。因此，我们必须要做出妥协。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要计算每个输入序列中的所有单词或tokens。从下列结果我们可以看到，一个序列中的平均单词数大约是221个："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.27716"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_tokens = [len(tokens) for tokens in input_train_tokens + input_test_tokens]\n",
    "total_num_tokens = np.array(total_num_tokens)\n",
    "\n",
    "np.mean(total_num_tokens)     #计算所有数字序列的平均单词数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从下列结果，我们可以看到这些序列中最大的单词数超过2200："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(total_num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        平均值221和最大值2209之间有巨大的差别，如果我们只是在数据集中填充所有的句子，以便它们会有2209个tokens，那么我们就会浪费大量的内存。如果说我们有一个包含数百万个文本序列的数据集，这将会是一个很大的问题。\n",
    "        所以我们要做出一个妥协。我们将填充所有序列，并截断那些太长的序列，这样它们就有544个单词了。我们的计算方法是：取数据集中所有序列的平均单词数，并添加两个标准差，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_tokens = np.mean(total_num_tokens) + 2 * np.std(total_num_tokens)   #均值加两个标准差\n",
    "max_num_tokens = int(max_num_tokens)\n",
    "max_num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加标准差后，我们每一个序列的单词数将保留为544个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(total_num_tokens < max_num_tokens)/len(total_num_tokens)  #小于544个单词的序列个数占所有序列个数的比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这里我们可以看到，大约有95%的文本长度均为544，只有5%的文本比544个单词长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们知道，在Keras中称这些为函数。它们要么填充太短的序列(所以它们只添加零)，要么截断太长的序列(如果文本太长，基本上只需要切断一些单词)。 \n",
    "然而，需要注意的是：我们到底是在序列前还是在序列后模式下进行填充和截断呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，假设我们有一个整数tokens序列，因为它太短了，我们想要填充它。我们可以：要么在开头放置这些零，以便在结尾处有实际的整数tokens。或者用相反的方式来做，这样我们所有的数据都在开始，所有的零在结尾。但是，如果我们回到前面的RNN流程图，我们知道它是一步步地处理序列，所以如果我们开始处理零，它可能没有任何意义，内部状态可能只是保持为零。因此，每当它看到一个特定单词的整数token时，它就会知道，好的，现在我们开始处理数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，如果所有的零都在末尾，我们就会开始处理所有的数据；那么我们就会在循环单元中有一些内部状态。现在，我们看到了大量的零，这可能会破坏我们刚刚计算出来的内部状态。这就是为什么在开始时填充零可能是个好主意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个问题是关于截断文本。如果文本很长，我们将截断它，以使它适合于文字，或任何数字。现在，想象一下，我们在中间的某个地方抓住了一个句子，它写的是this very good movie，或者this is not。当然，我们只在很长的序列中这样做，但是我们有可能失去正确分类这篇文章所必需的信息。因此，这是我们在截断输入文本时需要做出妥协。一个比较好的方法是创建一个批处理并在批处理中填充文本。因此，当我们看到一个很长的序列时，我们会把其他序列放置在相同的长度上。但我们不需要将所有这些数据存储在内存中，因为大部分数据都是浪费的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来让我们返回并转换整个数据集，使其被截断和填充；它是一个大的数据矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pad = 'pre'        #pre表示从起始填充或截断\n",
    "\n",
    "input_train_pad = pad_sequences(input_train_tokens, maxlen=max_num_tokens,\n",
    "                            padding=seq_pad, truncating=seq_pad)           #padding表示填充，truncating表示截断\n",
    "\n",
    "input_test_pad = pad_sequences(input_test_tokens, maxlen=max_num_tokens,\n",
    "                           padding=seq_pad, truncating=seq_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们检查这个矩阵的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，让我们看看填充前后的特定示例tokens："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充前的数字矩阵如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 填充之后，这个示例如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train_pad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解了文本转换为数字列表之后，接下来，我们来看一个向后映射的功能，即从整数tokens映射回文本单词。我们只需要用一个非常简单的助手函数即可，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tokenizer_obj.word_index      #数字列表\n",
    "index_inverse_map = dict(zip(index.values(), index.keys()))    #zip函数将键和值反过来\n",
    "\n",
    "\n",
    "def convert_tokens_to_string(input_tokens):          \n",
    "    # Convert the tokens back to words\n",
    "    input_words = [index_inverse_map[token] for token in input_tokens if token != 0]   #将token整数转换为单词\n",
    "\n",
    "    # join them all words.\n",
    "    combined_text = \" \".join(input_words)\n",
    "\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，数据集中的原始文本如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们使用一个帮助函数将tokens转换回文本单词，我们将得到以下文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"or as george stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq kids to succeed technology the or worrying if they'll be next to end up on the streets br br but what if you were given a bet to live on the streets for a month without the you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what it's like to be homeless that is lesson br br mel brooks who directs who stars as plays a rich man who has everything in the world until deciding to make a bet with a sissy rival to see if he can live in the streets for thirty days without the if succeeds he can do what he wants with a future project of making more buildings the on where is thrown on the street with a on his leg to his every move where he can't step off the sidewalk he's given the by a after it's written on his forehead where meets other characters including a woman by the name of molly ann warren an ex dancer who got divorce before losing her home and her pals sailor howard morris and teddy wilson who are already used to the streets they're survivors isn't he's not used to reaching mutual like he once did when being rich where it's fight or flight kill or be killed br br while the love connection between molly and wasn't necessary to plot i found life stinks to be one of mel films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing young frankenstein or for the matter to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money maybe they should give it to the homeless instead of using it like money br br or maybe this film will inspire you to help others\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_tokens_to_string(input_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，除了标点符号和其他符号，其他基本一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        现在，我们需要创建RNN，我们将在Keras中用所谓的sequential模型来实现。\n",
    "        这个体系结构的第一层是所谓的嵌入层。如果我们回顾一下图1中的流程图，我们刚才所做的就是将原始输入文本转换为整数tokens。但是我们仍然不能将它输入到RNN，因此我们必须将其转换为嵌入向量，即介于-1和1之间的值。它们可以在一定程度上超过这个范围，但通常在-1到1之间，这是我们可以在神经网络中处理的数据。\n",
    "        我们需要决定每个向量的长度，例如，token11被转换成一个实值向量，我们可以将长度设置为10（这个长度实际上是非常短的，通常，它在100到300之间)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）加入嵌入层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 这里，我们将嵌入大小设置为8，然后使用Keras将该嵌入层添加到RNN中。这必须是网络的第一层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_size = 8 #typical value for this should be between 200 and 300\n",
    "\n",
    "rnn_type_model.add(Embedding(input_dim=num_top_words,\n",
    "                    output_dim=embedding_layer_size,\n",
    "                    input_length=max_num_tokens,\n",
    "                    name='embedding_layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）建立RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以添加第一个循环层，我们将使用所谓的gated recurrent unit(GRU)。通常情况下，我们看到人们会使用所谓的LSTM，但其他人似乎认为GRU更好，因为LSTM内部有多余的gates。实际上，更简单的代码在更少的gates上工作更好。因此，我们这里采用GRU，让我们定义我们的GRU架构，我们希望输出维数为16，我们需要返回序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type_model.add(GRU(units=16, return_sequences=True))\n",
    "\n",
    "rnn_type_model.add(GRU(units=8, return_sequences=True))\n",
    "\n",
    "rnn_type_model.add(GRU(units=4))\n",
    "\n",
    "rnn_type_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_optimizer = Adam(lr=1e-3)\n",
    "\n",
    "rnn_type_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们添加了三个循环层，最后一个dense层只给出GRU的最终输出，而不是一个完整的输出序列。这里的输出将被输入到一个完全连接或dense层中，该层应该为每个输入序列输出一个值。因为使用Sigmoid激活函数处理，所以它会输出一个介于0到1之间的值。我们在这里使用的是ADAM优化器，并且损失函数是RNN的输出和训练集的实际类值之间的二进制交叉熵，这个值要么是0，要么是1："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们查看模型的外观，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_type_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从该模型我们可以知道，我们有一个嵌入层，三个循环单元和一个dense层。注意，这没有太多的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、模型训练和结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们开始对模型进行训练，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programing software\\Anaconda\\envs\\tensorFlow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 194s 8ms/step - loss: 0.5021 - acc: 0.7364 - val_loss: 0.5293 - val_acc: 0.7648\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 188s 8ms/step - loss: 0.2781 - acc: 0.8945 - val_loss: 0.2628 - val_acc: 0.8880\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 186s 8ms/step - loss: 0.2111 - acc: 0.9265 - val_loss: 0.4714 - val_acc: 0.8032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19c4d1a17b8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_type_model.fit(input_train_pad, target_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这里我们可以看到，共执行了3个训练周期，其误差越来越小，准确率越来越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）评估模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 71s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "model_result=rnn_type_model.evaluate(input_test_pad,target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy:85.26%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （3）进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们看一个错误分类文本的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 首先，我们计算测试集中前1000个序列的预测类，然后取实际的类值。我们将它们进行比较，并得到存在这种不匹配的索引列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = rnn_type_model.predict(x=input_test_pad[0:1000])\n",
    "target_predicted = target_predicted.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用截止阈值表示上述所有值都为正值，其他值将被认为负值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted = np.array([1.0 if prob>0.5 else 0.0 for prob in target_predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们得到这1000条序列的实际类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_actual = np.array(target_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来让我们从输出中获取不正确的样本，代码及结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_samples = np.where(class_predicted != class_actual)\n",
    "incorrect_samples = incorrect_samples[0]\n",
    "len(incorrect_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们发现这些文本中有82篇被错误地分类；这是我们在这里计算的1000个文本的8.2%。让我们看一下第一个错误分类的文本，代码及结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = incorrect_samples[0]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BEING Warner Brothers\\' second historical drama featuring Civil War and Battle of the Little Big Horn, General George Armstrong Custer, THEY DIED WITH THEIR BOOTS ON (Warner Brothers, 1941) was the far more accurate of the two; especially when contrasted with SANTA FE TRAIL (Warner Brothers, 1940), which really didn\\'t set the bar very high.<br /><br />ALTHOUGH both pictures were starring vehicles for Errol Flynn, there was a change in the casting the part of General Custer. Whereas it was \"Dutch\", himself, Ronald Reagan portraying the flamboyant, egomaniacal Cavalryman in the earlier picture, with Mr. Flynn playing Virginian and later Confederate Hero General, J.E.B. (or Jeb) Stuart; Errol took on the Custer part for THEY DIED WITH THEIR BOOTS ON.<br /><br />ONCE again, the Warner Brothers\\' propensity for using a large number of reliable character actors from the \"Warner\\'s Repertory Company\" are employed in giving the film a sort of authenticity, and all is really happening right before our very own eyes. Major roles are taken by some better known actors and actresses, such as: Elizabeth Bacon/Mrs. Custer (co-star Olivia de Havilland), Ned Sharpe (Arthur Kennedy), Samuel Bacon (Gene Lockhart), Chief Crazy Horse (Anthony Quinn), \"Californy\" (Charlie Grapwin), Major Taipe (Stanley Ridges), General Phillip Sheridan (John Litel), Callie (the Bacon\\'s Maid, Hattie McDaniel). <br /><br />THE rest of the cast is just chock full of uncredited, though skilled players such as: Joe Sawyer, Eleanor Parker, Minor Watson, Tod Andrews, Irving Bacon, Roy Barcroft, Lane Chandler, Spencer Charters, Frank Ferguson, Francis Ford, William Forrest, George Eldridge, Russell Hicks, William Hopper, Hoppity Hooper, Eddie Keane, Fred Kelsey, Sam McDaniel, Patrick McVey, Frank Orth, Eddie Parker, Addison Richards, Ray Teal, Jim Thorpe (All-American, himself), Minerva Urecal, Dick Wessel, Gig Young and many, many more.<br /><br />THE film moves very quickly, particularly in the early goings; then sort of slows down out of necessity as the story moves along to the Post Civil War years, the assignment of Custer as a Colonel in the 7th Cavalry and the ultimate destiny at the Little Big Horn, in Montana. Under the guidance of Director, Griffith Veteran, Raoul Walsh, the film hits a greatly varied array of emotions; from the very serious, exciting battle scenes and convincing historical scenes; looking as if they were Matthew Brady Civil War Photos. As with most any of Mr. Walsh\\'s films, he punctuates and expedites the end of many a scene with a little humor; but not going overboard and thus risking the chance of turning the film into a comedy (farce, actually).<br /><br />AS previously mentioned, this is much more factual than its predecessor, SANTA FE TRAIL (last time we\\'ll mention it, honest Schultz, Scout\\'s Honor!). However, that is not to say that it wasn\\'t without a few little bits of \"Artistic and Literary License; as indeed, just about any Biopic will have. It would be impossible to make any similar type of film if indeed every fact and incident were to be tried to be included in the screenplay. Perhaps the most erroneous inclusion as well as the most obvious invocation of Literary License is that business about Custer\\'s being accidentally promoted to the rank of Brigadier General. It just didn\\'t happen that way, yet the \"gag\" both helped the film to move along; while it underscored the whole light, carefree feeling that permeated the early part of the film.<br /><br />DIRECTOR Walsh and Mr. Flynn collaborated in giving us what would seem to be a characterization of this legendary Civil War Hero that was very close to the real life man. And they did this on top of the recreation of an incident, being the Massacre by the Lakota Sioux, the Cheyenne and the Fukowi of Custer and his 7th Cavalry at the Little Big Horn. At the time of its occurrence, June 25, 1876, \"Custer\\'s Last Stand\" was as big an incident and shock to the Americans\\' National Psyche as were the Japanese Attack on Pearl Harbor (December 7, 1941) or the Atrocities perpetrated by the Islamic Fascists to New York\\'s Twin Trade Towers and the United States\\' Armed Forces\\' Headquarters in the Pentagon, Arlington, Virginia on September 11, 2002.<br /><br />JUST as so many films of that period of WORLD WAR II (and the years immediately before), there were so many incidents in it that were, if not intentionally done, were demonstrations of virtues that would be needed in time of another Global Conflict, such as we were in by the time of THEY DIED WITH THEIR BOOTS ON was finishing up its original Theatrical release period.<br /><br />POODLE SCHNITZ!!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_predicted_text = input_text_test[index]\n",
    "incorrect_predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看这个示例的模型输出以及实际的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11293286"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_predicted[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_actual[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这个结果我们可以看出，预测出的情感值与实际的情感值是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们根据一组新的数据样本测试我们训练了的模型，并查看其结果，这里共有8个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "test_sample_2 = \"Good movie!\"\n",
    "test_sample_3 = \"Maybe I like this movie\"\n",
    "test_sample_4 = \"Meh ...\"\n",
    "test_sample_5 = \"If I were a drunk teenager then this movie \"\n",
    "test_sample_6 = \"Bad movie\"\n",
    "test_sample_7 = \"Not a good movie\"\n",
    "test_sample_8 = \"This movie really sucks! Can I get my money back please？\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们将它们转换为整数tokens，并对这些数字序列截长补短："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen = max_num_tokens, padding = seq_pad, truncating = seq_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后填充它们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 544)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples_tokens_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，针对它们运行模型，得到如下结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96684575],\n",
       "       [0.96288127],\n",
       "       [0.9389076 ],\n",
       "       [0.96079415],\n",
       "       [0.927937  ],\n",
       "       [0.88653195],\n",
       "       [0.95413595],\n",
       "       [0.8341638 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_type_model.predict(test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接近0的值表示消极情感，接近1的值表示积极情感。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
